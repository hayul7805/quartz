---
title: "Abstract"
---

## AI 텍스트 말뭉치 기반 비윤리적 어휘 연구
> This study linguistically examined the characteristics of unethical words using corpus. The unethical words extracted from the unethical text corpus of AI learning data was used as the subject of this study. Firstly, the formal characteristics of unethical words were divided into abbreviations, variations, and similar derivation. As a formal characteristic, the unethical words represented the speaker’s negative emotional expression with abbreviations and initial consonants. Next, the semantic functional characteristics of unethical words were classified avoidance of prohibited words, encoding of situations, belittlement of specific objects, contextual metaphor, and complex semantic functions characteristics. Unethical words complexly represented the speaker's intention, language playfulness, and efficiency of expression. This study is meaningful in the analysis of unethical words based on the unethical text corpus actually built. Based on the linguistic characteristics of these unethical words, it is expected to quantitatively and qualitatively help research related to unethical data.
> 본 연구는 말뭉치를 이용하여 비윤리적인 단어의 특징을 언어학적으로 고찰하였다. 본 연구는 AI 학습 데이터의 비윤리적인 텍스트 말뭉치에서 추출한 비윤리적인 단어를 연구 대상으로 삼았다. 첫째, 비윤리적인 단어의 형식적 특성을 축약어, 변형어, 유사유도어로 구분하였다. 형식적인 특성으로 비윤리적인 단어들은 약어와 초성으로 화자의 부정적인 감정 표현을 나타낸다. 다음으로 비윤리적인 단어의 의미적 기능적 특성은 금지어 회피, 상황 부호화, 특정 사물의 경시, 문맥적 은유, 복잡한 의미적 기능적 특성으로 분류되었다. 비윤리적인 단어들은 화자의 의도, 언어유희성, 표현의 효율성을 복합적으로 대변했다. 본 연구는 실제 구축된 비윤리적인 텍스트 말뭉치를 기반으로 한 비윤리적인 단어 분석에 의의가 있다. 이러한 비윤리적인 단어들의 언어적 특성을 바탕으로 비윤리적인 데이터와 관련된 연구에 양적, 질적으로 도움이 될 것으로 기대된다.


강건한(robust) 혐오표현 탐지 모델은 이전에 학습한 데이터와 다른 특징을 가진 혐오표현 데이터에도 일정한 분류 성능을 내는, 일반화가 잘 되는 모델이다. 그러나 현행의 모델 학습에 사용되는 데이터셋은 생성 방식, 주석 지침, 수집 장소가 모두 달라서 어느 한 데이터셋을 잘 학습한 모델이더라도 비슷하지만 다른 데이터셋에서는 성능이 상당히 달라지고는 한다. 이와 관련하여 외국에서는 몇몇 연구들이 선행적으로 논의를 시작하고 있으나, 아직 한국어 혐오표현 데이터셋과 이를 학습한 모델을 대상으로는 연구가 진행되지 않았다. 한국에서도 현재 온라인 혐오표현으로 많은 피해자가 생기고 있는 만큼, 강건하고 일반화가 잘 되는 혐오표현 탐지 모델이 시급하다. 따라서 본 연구는 한국어 혐오표현 데이터셋과 이를 학습한 모델의 일반화 가능성(generalizability)을 데이터셋 교차 검증으로 확인하였다. 본 연구에서 사용한 데이터셋은 현재 공개되어 있는 데이터셋 중에 구축 지침이 명확한 4개의 데이터셋으로 한정하였다. 결과는 정확도와 F1 점수라는 정량적 지표와 함께, 각각에 대한 오분류 사례를 제시했다. 실험 결과의 결과로 Beep 데이터셋(Moon et al., 2021)으로 학습한 모델이 다른 데이터셋에서 비교적 적은 성능 변화가 있어, 가장 일반화가 잘 되었다. 또한 양성 샘플의 비율이 큰 데이터셋으로 학습한 모델이, 양성 샘플의 비율이 큰, 다른 데이터셋에 테스트할 때 성능의 변화가 적었다. 즉, 양성 샘플의 비율이 일반화 가능성과 관련이 있었다. 본 연구는 한국어 혐오표현 탐지 연구에 있어서 데이터셋 교차 검증을 통해 일반화에 대한 논의를 시작하였다는 의의가 있다. 본 연구를 바탕으로 혐오표현 탐지 연구의 강건성 및 일반화 가능성과 관련된 연구에 도움이 될 것으로 기대한다. 

Machine learning models are robust when they generalize well to unseen data. However, hate speech detection models usually fail to generalize to similar but different hate speech, such as other hate speech datasets, as they differ in construction methods, annotation guidelines, and data sources from the data the model learns. A few have begun to discuss the generalizability of hate speech datasets, but it is only limited to the datasets written in English. So the generalizability of the Korean hate speech datasets has not yet been shed light. However, as victims of online hate speech are growing in Korea, a robust hate speech detection model is urgently needed. Therefore, this study examined the generalizability of the Korean hate speech datasets through cross-dataset testing. We chose four publicly available datasets with clear construction guidelines and KcELECTRA model. We used them alternately in model training and testing and showed accuracy and F1 scores along with error analysis for each pair. As a result, we found that the performance of the model learned with Beep datasets (Moon et al., 2021) changed relatively few on other datasets, meaning the model generalizes well to new data. In addition, we found that the model learned with a dataset with a large proportion of positive samples showed little change in performance when tested on other datasets with a large proportion of positive samples, meaning that the proportion of positive samples is important for generalizability. This study is meaningful as the starting point of the discussion on the generalization of Korean hate speech datasets. We expect this research to be helpful in the robustness and generalizability of hate speech detection research.

## Beep!
> Toxic comments in online platforms are an unavoidable social issue under the cloak of anonymity. Hate speech detection has been actively done for languages such as English, German, or Italian, where manually labeled corpus has been released. In this work, we first present 9.4K manually labeled entertain- ment news comments for identifying Korean toxic speech, collected from a widely used on- line news platform in Korea. The comments are annotated regarding social bias and hate speech since both aspects are correlated. The inter-annotator agreement Krippendorffs alpha score is 0.492 and 0.496, respectively. We pro- vide benchmarks using CharCNN, BiLSTM, and BERT, where BERT achieves the highest score on all tasks. The models generally display better performance on bias identification, since the hate speech detection is a more subjective issue. Additionally, when BERT is trained with bias label for hate speech detec- tion, the prediction score increases, implying that bias and hate are intertwined. We make our dataset publicly available and open compe- titions with the corpus and benchmarks.
> 온라인 플랫폼의 독성 댓글은 익명성을 빙자한 피할 수 없는 사회적 이슈다. 혐오 음성 감지는 수동으로 레이블이 지정된 말뭉치가 출시된 영어, 독일어 또는 이탈리아어와 같은 언어에 대해 활발히 수행되었다. 본 연구에서는 먼저 한국에서 널리 사용되는 온라인 뉴스 플랫폼에서 수집된 한국어 독성 언어를 식별하기 위한 9.4K개의 수동으로 레이블링된 엔터테인먼트 뉴스 코멘트를 제시한다. 댓글에는 사회적 편견과 혐오 발언에 대한 주석이 달려 있는데, 두 가지 측면이 상관되기 때문이다. 주석자 간 합의 Krippendorffs 알파 점수는 각각 0.492와 0.496입니다. 우리는 CharCNN, BiLSTM 및 BERT를 사용하여 벤치마크를 제공하며, BERT는 모든 작업에서 가장 높은 점수를 달성한다. 혐오 발언 감지가 더 주관적인 문제이기 때문에 모델은 일반적으로 편향 식별에서 더 나은 성능을 보여준다. 또한, BERT가 혐오 음성 감지를 위한 편향 레이블로 훈련되면 예측 점수가 증가하여 편견과 혐오가 얽혀 있음을 의미한다. 우리는 데이터 세트를 공개적으로 사용할 수 있게 하고 말뭉치 및 벤치마크와 공개 경쟁을 한다.

## Cross-Domain Detection of Abusive Language Online
>We investigate to what extent the models trained to detect general abusive language gen- eralize between different datasets labeled with different abusive language types. To this end, we compare the cross-domain performance of simple classification models on nine different datasets, finding that the models fail to gen- eralize to out-domain datasets and that hav- ing at least some in-domain data is impor- tant. We also show that using the frustratingly simple domain adaptation (Daume III, 2007) in most cases improves the results over in- domain training, especially when used to aug- ment a smaller dataset with a larger one.
>일반적인 욕설을 감지하도록 훈련된 모델이 서로 다른 욕설 유형으로 레이블이 지정된 서로 다른 데이터 세트 사이에서 어느 정도 일반화되는지 조사한다. 이를 위해, 우리는 9개의 다른 데이터 세트에서 단순 분류 모델의 교차 도메인 성능을 비교하여 모델이 외부 도메인 데이터 세트로 일반화하지 못하고 적어도 일부 도메인 내 데이터를 갖는 것이 중요하다는 것을 발견했다. 또한 대부분의 경우 실망스러울 정도로 간단한 도메인 적응(Daume III, 2007)을 사용하면 특히 더 큰 데이터 세트를 사용하여 더 작은 데이터 세트를 보강하는 데 사용될 때 도메인 내 교육보다 결과가 향상된다는 것을 보여준다.

## On Cross-Dataset Generalization in Automatic Detection of Online Abuse
> NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, training and test datasets are usually obtained from similar data samples, in practice systems are often applied on data that are different from the training set in topic and class distributions. Also, the ambiguity in class definitions inher- ited in this task aggravates the discrepancies between source and target datasets. We ex- plore the topic bias and the task formulation bias in cross-dataset generalization. We show that the benign examples in the Wikipedia Detox dataset are biased towards platform- specific topics. We identify these examples using unsupervised topic modeling and man- ual inspection of topics’ keywords. Remov- ing these topics increases cross-dataset gener- alization, without reducing in-domain classifi- cation performance. For a robust dataset de- sign, we suggest applying inexpensive unsu- pervised methods to inspect the collected data and downsize the non-generalizable content before manually annotating for class labels.
> NLP 연구는 지도 분류 과제로서 욕설 탐지 분야에서 높은 성과를 거두었다. 연구 환경에서 훈련 및 테스트 데이터 세트는 일반적으로 유사한 데이터 샘플에서 얻지만, 실제로 시스템은 주제 및 클래스 분포의 훈련 세트와 다른 데이터에 적용되는 경우가 많다. 또한 이 작업에서 상속된 클래스 정의의 모호성은 소스 데이터 세트와 대상 데이터 세트 간의 불일치를 악화시킨다. 우리는 교차 데이터 세트 일반화에서 주제 편향과 작업 공식 편향에 대해 탐구한다. 우리는 위키피디아 디톡스 데이터 세트의 양성 예가 플랫폼별 주제에 편향되어 있음을 보여준다. 우리는 감독되지 않은 주제 모델링과 주제 키워드의 수동 검사를 사용하여 이러한 예를 식별한다. 이러한 항목을 제거하면 도메인 내 분류 성능을 저하시키지 않고 데이터 세트 간 일반화가 향상됩니다. 강력한 데이터 세트 설계를 위해 클래스 레이블에 수동으로 주석을 달기 전에 저렴한 비지도 방법을 적용하여 수집된 데이터를 검사하고 일반화할 수 없는 내용의 크기를 줄일 것을 제안한다.

## 혐오표현의 수행성과 그것에 대한 저항
>최근 우리학계에서는 혐오표현에 대한 논의가 활발하게 이루어지고 있는데 그 주된 논점은 혐오표현의 규제의 타당성 여부이다. 이 글에서는 혐오표현의 문제를 언어철학, 법경제학, 사회심리학 등의 성과를 힘에 빌려 논의를 전개하고자 한다. 제1장에서는 혐오표현의 의미를 밝혀보고자 한다. 여기에서는 용어의 의미를 설명 하는 방식에 관한 일반적인 차원에서의 고찰을 바탕으로 혐오표현의 의미를 설명해 보고자 한다. 제2장에서는 혐오표현의 수행성을 살펴보고자 한다. 옥스퍼드 일상언 어학파의 학자 오스틴(J. L. Austin)의 고찰을 혐오표현에 적용하여 혐오표현을 분석 해보고자 한다. 이러한 분석에 따르면, 혐오표현 자체는 발화행위와 그것을 통해 화자가 수행하는 발화수반행위로 구성된다. 그러나 혐오표현이 가져오는 효과, 즉 상대방을 상처주고 모욕하는 결과는 혐오표현 자체가 아니고 그것의 발화효과일 뿐이다. 따라서 혐오표현은 그 결과를 완전하게 결정하지는 못한다. 끝으로 제3장에 서는 혐오표현의 규제에 관한 문제를 다룬다. 혐오표현의 수행성에 관한 이러한 고찰은 혐오표현을 극복할 가능성을 제시해준다. 즉, 국가는 혐오표현에 대항하고자 하는 자들에게 혐오표현을 전유하고, 전유하고, 재맥락화하여 혐오표현과 느슨하게 연결되어 있는 결과 사이를 단절시킬 수 있도록 도와주어야 한다. 국가는 혐오표현을 규제하는 것이 아니라 반대자들에게 이에 대항할 수 있는 힘을 길러줘야 하는 것이다.

## 한국어 혐오 표현 코퍼스 구축 방법론 연구
>온라인 공간에서 특정인, 혹은 특정 집단의 사람들을 대상으로 한 혐오 표현은 당사자에게 정신적 고통을 미칠 뿐 아니라 이를 보는 이에게도 간접적인 불쾌함을 유발한다. 이에 관한 문제의식은 사회적으로 공감대가 형성된 바 있지만, 아직 한국어에서는 많은 연구들이 혐오 표현 자체의 논의에 집중하고 있으며, 이는 실제로 관찰되는 혐오 표현들의 자동 탐지 및 예방에는 효과적인 정보를 제공하지 못하는 것이 사실이다. 이에 우리는 실제 온라인 댓글들을 탐구하여 혐오, 모욕 및 사회적 편견을 탐지할 수 있는 모델 학습에 필요한 코퍼스 구축 가이드라인을 제작하였다. 구체적인 사례를 동반한 가이드라인과 크라우드소싱을 바탕으로 약 9천 3백 문장 가량의 코퍼스를 구축하였으며, 해당 데이터에 관한 개요와 함께 우리의 접근 방식이 어떤 점에서 기존의 담론과 연관되어 있는지에 대한 분석을 제시한다.

## 비윤리적 한국어 발언 검출을 위한 새 데이터 세트
>최근 한국에서도 이루다의 윤리 이슈를 기점으로 딥러닝 모델의 윤리적 언어학습 필요성이 대 두되었다. 그럼에도 불구하고 영어 데이터에 비해 한국어 데이터는 Korean Hate Speech Detection Dataset 이 유일하다. 이번 연구에서는 기존 데이터 세트의 유연성이 떨어지고 세부 라벨이 제한적 이라는 문제를 개선한 새로운 데이터 세트를 제안하고, 해당 데이터 세트에 대하여 다양한 신경망 분류 모델을 적용한 벤치마크 결과를 공개한다.