<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLG on</title><link>https://hayul7805.github.io/quartz/tags/NLG/</link><description>Recent content in NLG on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 15 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://hayul7805.github.io/quartz/tags/NLG/index.xml" rel="self" type="application/rss+xml"/><item><title>Evaluating NLG systems</title><link>https://hayul7805.github.io/quartz/notes/lectures/stanford-CS224n/Neural-Language-Generation/Evaluating-NLG-systems/</link><pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate><guid>https://hayul7805.github.io/quartz/notes/lectures/stanford-CS224n/Neural-Language-Generation/Evaluating-NLG-systems/</guid><description>Types of evaluation methods for text generation Content overlap metrics Model-based Metrics Human Evaluations ==Content overlap metrics== Compute a score that indicates the similarity between generated and gold-standard (human-written) text Fast and efficient and widely used Two broad categories: N-gram overlap metrics (e.</description></item><item><title>Exposure bias</title><link>https://hayul7805.github.io/quartz/notes/lectures/stanford-CS224n/Neural-Language-Generation/Exposure-bias/</link><pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate><guid>https://hayul7805.github.io/quartz/notes/lectures/stanford-CS224n/Neural-Language-Generation/Exposure-bias/</guid><description>Training with Teacher forcing leads to exposure bias at generation time During training, our modelâ€™s inputs are gold context tokens from real, human-generated texts.</description></item><item><title>Unlikelihood training</title><link>https://hayul7805.github.io/quartz/notes/lectures/stanford-CS224n/Neural-Language-Generation/Unlikelihood-training/</link><pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate><guid>https://hayul7805.github.io/quartz/notes/lectures/stanford-CS224n/Neural-Language-Generation/Unlikelihood-training/</guid><description>Given a set of undesired tokens $C$, lower their likelihood in context. $$L_{UL}^t = -\sum_{y_{neg} \in C} log(1-P(y_{neg} | {y^*}_{&amp;lt;t}))$$ Keep teacher forcing objective and combine them for final loss function.</description></item></channel></rss>