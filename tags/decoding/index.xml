<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>decoding on</title><link>https://hayul7805.github.io/quartz/tags/decoding/</link><description>Recent content in decoding on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 15 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://hayul7805.github.io/quartz/tags/decoding/index.xml" rel="self" type="application/rss+xml"/><item><title>Top-k sampling</title><link>https://hayul7805.github.io/quartz/notes/lectures/stanford-CS224n/Neural-Language-Generation/Top-k-sampling/</link><pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate><guid>https://hayul7805.github.io/quartz/notes/lectures/stanford-CS224n/Neural-Language-Generation/Top-k-sampling/</guid><description>Problem: Vanilla sampling makes every token in the vocabulary an option Even if most of the probability mass in the distribution is over a limited set of options, the tail of the distribution could be very long Many tokens are probably irrelevant in the current context Why are we giving them individually a tiny chance to be selected?</description></item><item><title>Top-p sampling</title><link>https://hayul7805.github.io/quartz/notes/lectures/stanford-CS224n/Neural-Language-Generation/Top-p-sampling/</link><pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate><guid>https://hayul7805.github.io/quartz/notes/lectures/stanford-CS224n/Neural-Language-Generation/Top-p-sampling/</guid><description>등장 배경: The probability distributions we sample from are dynamic. (참고: Top-k sampling) When the distribution $P_t$ is flatter, a limited $k$ removes many viable options.</description></item></channel></rss>