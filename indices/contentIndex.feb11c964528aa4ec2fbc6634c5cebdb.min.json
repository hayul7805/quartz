{"/":{"title":"🌱 Hayul's digital garden","content":"\n안녕하세요?  \n저의 개발 블로그, **'Hayul's digital garden'** 입니다.  \n아래 **Contents**를 통해 포스팅을 탐색할 수 있습니다. \n\n## Contents\n\n###  [👩‍💻 Coding Test](notes/coding%20test.md)\n### [📚 Lectures](notes/Lectures.md)\n###  [📑 Paper Review](notes/paper-review.md)\n### [🗣 Talks](notes/Talks.md)\n### [⚙️ Algorithms](notes/Algorithms.md)\n### [🧭 Linear Algebra](notes/linear%20algebra/Linear%20Algebra.md)\n### [🦾 Machine Learning](notes/Machine%20Learning.md)\n### [🤖 Deep Learning](notes/Deep%20Learning.md)\n### [🐍 Python](notes/Python.md)","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/Algorithms":{"title":"⚙️ Algorithms","content":"- [⚙️ Two Pointers](notes/algorithms/Two%20Pointers.md)\n- [⚙️ Dynamic Programming](notes/algorithms/Dynamic%20Programming.md)\n- [⚙️ DFS, BFS](notes/algorithms/DFS,%20BFS.md)\n- [⚙️ 이진탐색](notes/algorithms/이진탐색.md)\n\t- [파라메트릭 서치(Parametric Search)](notes/algorithms/이진탐색.md#파라메트릭%20서치(Parametric%20Search))","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/Deep-Learning":{"title":"Deep Learning","content":"\n- [Gradient descent](notes/dl/Gradient%20descent.md)\n- [Activation function](notes/dl/Activation%20function.md)\n- [Gradient Descent Methods](notes/dl/Gradient%20Descent%20Methods.md)","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/Lectures":{"title":"📚 Lectures","content":"\n### [🌲 Stanford CS224n](notes/lectures/stanford%20CS224n/CS224n%20main.md)","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/Machine-Learning":{"title":"Machine Learning","content":"\n## ML 대표 알고리즘\n\n-   Logistic Regression\n-   Decision Tree\n-   Naïve Bayes\n-   Support Vector Machine\n-   K Nearest Neighbors\n-   Decision Tree 기반 Ensemble 모형\n\t- Random Forest\n\t- Extra Trees Boosting\n- Decision Tree 기반 Gradient Boosting 모형\n\t- [Extreme Gradient Boosting](notes/ml/XGB%20Modeling.md)  \n\t- Light Gradient Boosting  \n\t- Categorical Gradient Boosting\n- Natural Gradient Boosting\n\n\u003e [!note] Note  \n\u003e   \n\u003e 일반적으로 **Decision Tree \u0026 Logistic Regression** 이 설명력이 좋아서 자주 쓰인다. 그러나 최근에는 **Gradient Boosting** 계열도 자주 쓰이고 있다.\n\n## ML system 종류의 대표 구분 3가지\n\n**1. 사람의 감독하에 프로그램이 훈련하는 것인지 여부** \n- [지도학습 (Supervised Learning)](notes/ml/지도학습%20(Supervised%20Learning).md)\n\t- 분류 (Classification)\n\t- 회귀 (Regression)\n- [비지도학습 (Un-supervised Learning)](notes/ml/비지도학습%20(Un-supervised%20Learning).md)\n- 준지도학습 (Semi-supervised Learning)\n- 강화학습 (Reinforcement Learning)\n\n**2. 실시간으로 점진적인 학습을 하는지 여부**\n- 온라인 학습 (Online Learning)  \n- 배치 학습 (Batch Learning)\n\n**3. 기존의 데이터와 새로운 데이터를 간단히 비교분석하는 것인지 혹은 훈련 데이터 셋으로부터 패턴을 발견하여 예측 모델을 만드는지 여부** \n- 사례 기반 학습 (Instance-based Learning)\n- 모델 기반 학습 (Model-based Learning)\n\n## ML 문제 해결 순서\n\n\n```mermaid\nstateDiagram-v2\n\tData_distribution --\u003e Models\n\tModels --\u003e Hyper_parameter_tuning\n\tModels --\u003e Cross_validation\n\tHyper_parameter_tuning --\u003e Training\n\tCross_validation --\u003e Training\n\tTraining --\u003e Testing\n```\n\n1. [EDA \u0026 Visualization](notes/ml/EDA%20\u0026%20Visualization.md)\n\t1. 학습 데이터와 테스트 데이터의 분포가 동일한지 여부를 파악한다\n2. 활용할 모델 알고리즘들을 정한다\n3. [Hyper parameter tuning](notes/ml/Hyper%20parameter%20tuning.md)을 수행한다\n4. `Cross Validation` 기반 학습을 진행한다. \n5. 유의미한 결과가 나오는 알고리즘 및 Hyper parameter Case를 기반으로 학습 데이터 전체를 학습한다.\n6. 테스트 결과를 확인한다.\n\n## Techniques\n\n- [SMOTE](notes/ml/SMOTE.md)\n- [Recall and Precision](notes/ml/Recall%20and%20Precision.md)\n- [MAE and MSE](notes/ml/MAE%20and%20MSE.md)\n- [Sensitivity and Specificity](notes/ml/Sensitivity%20and%20Specificity.md)","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/Python":{"title":"💾 Python","content":"","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/Talks":{"title":"Talks","content":"\n- [ChatGPT Is a Blurry JPEG of the Web](notes/talks/ChatGPT%20Is%20a%20Blurry%20JPEG%20of%20the%20Web.md)\n- [What Is ChatGPT Doing … and Why Does It Work?](notes/talks/What%20Is%20ChatGPT%20Doing%20…%20and%20Why%20Does%20It%20Work?.md)","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/algorithms/%EC%9D%B4%EC%A7%84%ED%83%90%EC%83%89":{"title":"이진탐색","content":"\u003e [!note] Note  \n\u003e   \n\u003e 이진탐색은 **정렬되어 있는 리스트**에서 탐색 범위를 절반씩 좁혀가며 데이터를 탐색하는 방법이다. 이때, log N의 시간복잡도를 가진다. \n\u003e 시작점, 끝점, 중간점을 이용해서 탐색 범위를 설정한다. \n\n```python\ndef binary_search(array, target, start, end):\n\tif start \u003e end:\n\t\treturn None\n\tmid = (start + end) // 2\n\n\t#찾은 경우 중간점 인덱스 반환\n\tif array[mid] == target:\n\t\treturn mid\n\n\t#중간점의 값보다 찾고자 하는 값이 작은 경우 왼쪽 확인\n\telif array[mid] \u003e target:\n\t\treturn binary_search(array, target,\n\t\t\t\t\t\t\tstart, mid - 1)\n\n\t#중간점의 값보다 찾고자 하는 값이 크다면 오른쪽 확인\n\telse:\n\t\treturn binary_search(array, target,\n\t\t\t\t\t\t\tmid + 1, end)\n\nresult = binary_search(array, target, 0, n - 1)\n\nif result == None:\n\tprint(\"원소가 존재하지 않습니다.\")\nelse:\n\tprint(result + 1)\n\n```\n\n그리고 알아두면 좋은 라이브러리로 `bisect` 이 있다. \n\n- `bisect_left(a,x)`: 정렬된 순서를 유지하면서 배열 a에 x를 삽입할 가장 왼쪽 인덱스를 반환.\n- `bisect_right(a,x)`: 정렬된 순서를 유지하면서 배열 a에 x를 삽입할 가장 오른쪽 인덱스를 반환.\n\n```python\nfrom bisect import bisect_left, bisect_right\n\na = [1,2,4,4,8]\nx = 4\n\nprint(bisect_left(a,x))\n\u003e\u003e\u003e 2\n\nprint(bisect_right(a,x))\n\u003e\u003e\u003e 4\n```\n\n## 파라메트릭 서치(Parametric Search)\n\n- 이진탐색 문제는 **파라메트릭 서치 문제**를 풀기 위해서 자주 사용된다. \n- 파라메트릭 서치란 **최적화 문제를 결정 문제('예' 혹은 '아니오')로 바꾸어 해결**하는 기법이다.\n\t- 예시: 특정한 조건을 만족하는 가장 알맞은 값을 빠르게 찾는 최적화 문제\n\n","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/algorithms/DFS-BFS":{"title":"DFS, BFS","content":"\n\u003e [!note] Note  \n\u003e   \n\u003e DFS와 BFS는 대표적인 그래프 탐색 알고리즘이다.\n\n## 깊이 우선 탐색(DFS)\n- DFS는 그래프에서 **깊은 부분을 우선적으로 탐색하는 알고리즘**이다.\n- DFS는 **스택 자료구조(혹은 재귀 함수)** 를 이용하며, 구체적인 동작 과정은 다음과 같다.\n\t1. 탐색 시작 노드를 스택에 삽입하고 방문 처리한다.\n\t2. 스택의 최상단 노드에 방문하지 않은 인접한 노드가 하나라도 있으면 그 노드를 스택에 넣고 방문 처리한다. 방문하지 않은 인접 노드가 없으면 스택에서 최상단 노드를 꺼낸다.\n\t3. 더 이상의 2번의 과정을 수행할 수 없을 때까지 반복한다.\n\n```python\n#각 노드가 연결된 정보를 표현(2차원 리스트)\ngraph = [\n\t\t [],      #편의를 위해 하나 더 큰 크기로 만든다. \n\t\t [2,3,8], #첫번째 노드와 연결된 노드들 정보\n\t\t [1,7],\n\t\t [1,4,5],\n\t\t [3,5],\n\t\t [3,4],\n\t\t [7],\n\t\t [2,6,8],\n\t\t [1,7]\n]\n\n#각 노드가 방문된 정보를 표현(1차원 리스트)\nvisited = [False] * 9\n\n#DFS 메서드 정의\ndef dfs(graph, v, visited):\n\t\n\t#현재 노드를 방문 처리\n\tvisited[v] = True\n\t\n\t#현재 노드와 연결된 다른 노드를 재귀적으로 방문\n\tfor i in graph[v]:\n\t\tif not visited[i]:\n\t\t\tdfs(graph, i, visited)\n\n#정의된 DFS 함수 호출\ndfs(graph, 1, visited)\n```\n\n## 너비 우선 탐색(BFS)\n- BFS는 그래프에서 **가까운 노드부터 우선적으로 탐색**하는 알고리즘이다. \n- BFS는 **큐 자료구조(queue)** 를 이용하며, 구체적인 동작 과정은 다음과 같다.\n\t1. 탐색 시작 노드를 큐에 삽입하고 방문 처리를 한다.\n\t2. 큐에서 노드를 꺼낸 뒤에 해당 노드의 인접 노드 중에서 방문하지 않은 노드를 모두 큐에 삽입하고 방문 처리한다. **(한번에! 삽입한다는 것이 특징이다.)**\n\t3. 더 이상 2번의 과정을 수행할 수 없을 때까지 반복한다. \n- 방문 기준은 '번호가 낮은 인접 노드'부터 간다. \n- **'최단 거리 문제'** 를 해결하기 위한 방법으로 많이 쓰인다.\n\n```python\nfrom collections import deque\n\n#각 노드가 연결된 정보를 표현(2차원 리스트)\ngraph = [\n\t\t [],      #편의를 위해 하나 더 큰 크기로 만든다. \n\t\t [2,3,8], #첫번째 노드와 연결된 노드들 정보\n\t\t [1,7],\n\t\t [1,4,5],\n\t\t [3,5],\n\t\t [3,4],\n\t\t [7],\n\t\t [2,6,8],\n\t\t [1,7]\n]\n\n#각 노드가 방문된 정보를 표현(1차원 리스트)\nvisited = [False] * 9\n\ndef bfs(graph, start, visited):\n\tqueue = deque([start])\n\t\n\t#현재 노드를 방문처리\n\tvisited[start] = True\n\n\t#큐가 빌 때까지 반복\n\twhile queue:\n\t\t\n\t\t#큐에서 하나의 원소를 뽑아내기\n\t\tv = queue.popleft()\n\t\t\n\t\t#아직 방문하지 않은 인접한 원소들을 큐에 삽입\n\t\tfor i in graph[v]:\n\t\t\tif not visited[i]:\n\t\t\t\tqueue.append(i)\n\t\t\t\tvisited[i] = True\n\nbfs(graph, 1, visited)\n\n```","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/algorithms/Dynamic-Programming":{"title":"⚙️ Dynamic Programming","content":"\n\u003e 이번 포스팅에서는 _\"한 번 계산한 문제는 다시 계산하지 않도록 한다!\"_ 는 **다이나믹 프로그래밍(Dynamic Programming, 동적 계획법이라고도 함)**에 대해서 소개해보고 이를 Python으로 구현하는 방법에 대해 알아보자.\n\n\n다이나믹 프로그래밍은 **메모리 공간을 약간 더 사용**해서 연산 속도를 비약적으로 증가시키는 방법이다. 우선 다음과 같은 2가지 조건을 만족할 때 다이나믹 프로그래밍을 사용할 수 있다.\n\n1.  **큰 문제를 작은 문제로 나눌 수 있다.**\n2.  **작은 문제에서 구한 정답은 그것을 포함하는 큰 문제에서도 동일**하다.\n\n위 조건을 만족하는 대표적인 문제가 **피보나치 수열** 문제이다. 피보나치 수열은 다음과 같은 점화식을 만족하는 수열이다.\n\n$$\na_n=a_{n−1} + a_{n−2},a_1=1 ,a_2=1\n$$\n\n다이나믹 프로그래밍의 포인트는 바로 한 번 결과를 수행한 것을 메모리에 저장해 놓고 다음에 똑같은 결과가 필요하면 그 때 다시 연산하지 않고 메모리에 저장된 그 값을 가져와 쓰는 것이다.\n\n이러한 것을 **메모제이션(캐싱) 기법**이라고도 한다. 다음은 **재귀함수**를 사용한 다이나믹 프로그래밍으로 피보나치 수열을 구현한 코드이다.\n\n```python\nimport time\n\ndp = [0] * 50\n\ndef fibo(x):\n    if x == 1 or x == 2:\n        return 1\n    if dp[x] != 0:\n        return dp[x]\n    dp[x] = fibo(x-1) + fibo(x-2)\n    return dp[x]\n\nfor num in range(5, 40, 10):\n    start = time.time()\n    res = fibo(num)\n\n```\n\n이렇게 재귀함수를 사용해 구현하는 다이나믹 프로그래밍 방법은 메모제이션 기법을 활용한 `Top-Down` 방식이라고 한다.\n\n즉, 큰 문제를 해결하기 위해 작은 문제를 호출하는 것이다. \n\n반면에 재귀함수를 사용하지 않고 **단순 반복문**을 사용해 다이나믹 프로그래밍을 구현할 수 있다. 하단의 코드를 살펴보자.\n\n```python\ndp = [0] * 100\n\ndp[1] = 1 # 첫 번째 항\ndp[2] = 1 # 두 번째 항\nN = 99   # 피보나치 수열의 99번째 숫자는?\n\nfor i in range(3, N+1):\n    dp[i] = dp[i-1] + dp[i-2]\n\nprint(dp[N])\n```\n\n위와 같은 방식은 작은 문제부터 차근차근 답을 도출해서 큰 문제를 해결한다고 하여 `Bottom-Up` 방식이라고 한다. 참고로 Top-Down 방식에서는 이미 수행한 결과를 저장하는 것을 `메모제이션`, Bottom-Up 방식에서는 `DP 테이블`이라고 한다.\n\n일반적으론 단순 반복문을 활용하는 **Bottom-Up 방식으로 다이나믹 프로그래밍 방법을 해결**하라고 권장한다. 만약 재귀함수를 사용하는 Top-Down 방식을 사용하다 보면 재귀 횟수 제한 오류가 걸릴 수도 있기 때문이다.","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/algorithms/Two-Pointers":{"title":"⚙️ Two Pointers","content":"\n**투포인터 알고리즘(Two Pointers Algorithm)** 또는 **슬라이딩 윈도우(Sliding Window)** 라고 부른다.\n\n알고리즘 문제를 풀다 완전탐색으로 해결하면 시간 초과가 나는 문제가 종종 있는데, 이때 사용하면 빠르게 해결할 수 있다. 기본적인 원리는 다음과 같다.\n\n\u003e 1차원 배열이 있고, 이 배열에서 각자 다른 원소를 가리키고 있는 2개의 포인터를 조작해가면서 원하는 것을 얻기\n\nN칸의 1차원 배열이 있을 때, 부분 배열 중 그 원소의 합이 M이 되는 경우의 수를 구한다고 생각해보자. 모든 경우의 수를 다 테스트 해보면 구간 합을 구간 배열로 `O(1)`만에 구한다고 해도 경우의 수는 `O(N^2)`이 된다. 따라서 문제를 풀 수 없다. N의 최대 범위가 너무 크기 때문이다. 그러나 이 경우 각 원소는 자연수이고 M 또한 자연수인데, 따라서 다음과 같이 풀 수 있다.\n\n- 포인터 2개를 준비한다. 시작과 끝을 알 수 있도록 start, end 라고 한다.\n- 맨 처음에는 start = end = 0이며, 항상 start\u003c=end을 만족해야 한다.\n- 2개의 포인터는 현재 부분 배열의 시작과 끝을 가리키는 역할을 한다.\n\ns=e일 경우 그건 크기가 0인, 아무것도 포함하지 않는 부분 배열을 뜻한다. 다음의 과정을 s \u003c N인 동안 반복한다.\n\n1. 만약 현재 부분합이 M 이상이거나, 이미 e = N이면 s++\n2. 그렇지 않다면 e++\n3. 현재 부분합이 M과 같으면 결과 ++ \n\n쉽게 이해하자면, start와 end 를 무조건 증가시키는 방향으로만 변화시켜가면서 도중에 부분 배열의 합이 정확히 M이 되는 경우를 세는 것이다. \n\nEx) M = 5인 경우를 살펴보자.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_56/kks227_1471976777631dvMpe_PNG/1.png?type=w2)\n\n초기 상태이며, 빨간색 포인터 : start, 파란색 포인터 : end이다. S : 합.\n\n**end**가 뒤로 움직일 때는 새로 포함한 원소를 S에 더하고, **start**가 뒤로 움직일 때는 새로 넘긴 원소를 S에서 빼는 식으로 현재 [start, end)의 합 S를 매번 쉽게 구할 수 있다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_196/kks227_1471976777962Qks67_PNG/2.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_274/kks227_1471976778508STsIS_PNG/3.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_118/kks227_1471976778842HkF4H_PNG/4.png?type=w2)\n\n처음에는 이렇게 end만 증가하게 된다. S가 계속 M보다 작기 때문! 마지막엔 S\u003e=M이 되었으므로 아래와 같다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_101/kks227_1471976779156aosTT_PNG/5.png?type=w2)\n\nstart를 한 칸 옮겼는데, 동시에 S = 5인 경우를 만났다. 이때 결과를 1 증가시켜 준다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_186/kks227_1471976779456z8WVP_PNG/6.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_1/kks227_1471976779887ko5yw_PNG/7.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_265/kks227_1471976780291PDw0Y_PNG/8.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_139/kks227_1471976780603hkxD5_PNG/9.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_284/kks227_1471976780877YjQiA_PNG/10.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_101/kks227_1471976781212P3Li0_PNG/11.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_188/kks227_14719767815252r1eQ_PNG/12.png?type=w2)\n\n이런 식으로 포인터들이 움직이게 된다. 여기서 2번째로 S = 5인 지점을 만났으므로 결과를 1 증가시켜 준다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_80/kks227_14719767817475h0eo_PNG/13.png?type=w2)\n\n그 직후, start가 1 증가하면서 start = end인 경우가 나온다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_240/kks227_1471976782107sRHbv_PNG/14.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_240/kks227_14719767826459iErQ_PNG/15.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_192/kks227_1471976782977RS8E6_PNG/16.png?type=w2)\n\n계속 가다 보면 세 번째로 S = 5인 지점을 만난다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_147/kks227_1471976783270H1Bah_PNG/17.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_44/kks227_1471976783607C4F3g_PNG/18.png?type=w2)\n\n그 이후 조건에 맞춰 포인터를 증가시키다 보면, end가 배열 끝을 가리키게 되어 더이상 증가할 수 없는 상태가 된다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_197/kks227_1471976784071FLqRR_PNG/19.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_87/kks227_14719767845214em80_PNG/20.png?type=w2)\n\n그렇게 되면 그냥 start만 증가시켜 가다가 start 역시 배열 끝에 다다르면 종료해도 되고, 그냥 그 자리에서 루프를 끝내버려도 된다. 이렇게 해서 S = 5인 경우는 3개 발견되었다.\n\n- 시간 복잡도 \n  - 이 알고리즘은 매 루프마다 항상 두 포인터 중 하나는 1씩 증가하고 있고, 각 포인터가 N번 누적 증가해야 알고리즘이 끝난다. 따라서 각각 배열 끝에 다다르는데 `O(N)`이라서 합쳐도 `O(N)`이 된다.\n\n- **추천 문제(백준 기준)**\n  - 2003 : 수들의 합\n  - 1644 : 소수의 연속합\n  - 1806 : 부분합\n  - 2230 : 수 고르기\n  - 1484 : 다이어트\n  - 2038 : 골룽 수열\n  - 2531 : 회전 초밥\n  - 2096 : 내려가기\n  - 2293 : 동전1\n\n---\n### Reference\n- https://github.com/WooVictory/Ready-For-Tech-Interview/blob/master/Algorithm/투포인터%20알고리즘.md\n","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test":{"title":"👩‍💻 Coding Test","content":"\n## 프로그래머스 문제 모음\n  \n### Level 1\n- [숫자 문자열과 영단어](notes/coding%20test/숫자%20문자열과%20영단어.md)\n- [키패드 누르기](notes/coding%20test/키패드%20누르기.md)\n  \n### Level 2\n- [기능개발](notes/coding%20test/기능개발.md)\n- [올바른 괄호](notes/coding%20test/올바른%20괄호.md)\n- [다음 큰 숫자](notes/coding%20test/다음%20큰%20숫자.md)\n- [영어 끝말잇기](notes/coding%20test/영어%20끝말잇기.md)\n- [구명보트](notes/coding%20test/구명보트.md)\n- [멀리뛰기](notes/coding%20test/멀리뛰기.md)\n- [두 큐 합 같게 만들기](notes/coding%20test/두%20큐%20합%20같게%20만들기.md) \n- [H-index](notes/coding%20test/H-index.md)\n- [숫자 카드 나누기](notes/coding%20test/숫자%20카드%20나누기.md)\n- [스킬트리](notes/coding%20test/스킬트리.md)\n- [타겟 넘버](notes/coding%20test/타겟%20넘버.md)\n- [게임 맵 최단거리](notes/coding%20test/게임%20맵%20최단거리.md) #BFS \n- [짝지어 제거하기](notes/coding%20test/짝지어%20제거하기.md) #stack \n- [큰 수 만들기](notes/coding%20test/큰%20수%20만들기.md) #stack \n- [전화번호 목록](notes/coding%20test/전화번호%20목록.md) #hash\n- [주차 요금 계산](notes/coding%20test/주차%20요금%20계산.md) #hash \n- [전력망 둘로 나누기](notes/coding%20test/전력망%20둘로%20나누기.md) #BFS \n- [줄 서는 방법](notes/coding%20test/줄%20서는%20방법.md)\n- [베스트앨범](notes/coding%20test/베스트앨범.md)\n- [캐시](notes/coding%20test/캐시.md) #Kakao \n- [압축](notes/coding%20test/압축.md) #Kakao \n  \n### Level 3\n- [정수 삼각형](notes/coding%20test/정수%20삼각형.md)\n- [네트워크](notes/coding%20test/네트워크.md)\n  \n\n## HackerRank\n  \n### Basic\n- [Picking Numbers](notes/coding%20test/Picking%20Numbers.md)\n\n## Goorm\n\n### Level 2\n- [단어장 만들기](notes/coding%20test/단어장%20만들기.md)","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%E1%84%87%E1%85%A6%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%E1%84%8B%E1%85%A2%E1%86%AF%E1%84%87%E1%85%A5%E1%86%B7":{"title":"베스트앨범","content":"\n## 문제 설명\n\n스트리밍 사이트에서 장르 별로 가장 많이 재생된 노래를 두 개씩 모아 베스트 앨범을 출시하려 합니다. 노래는 고유 번호로 구분하며, 노래를 수록하는 기준은 다음과 같습니다.\n\n1.  속한 노래가 많이 재생된 장르를 먼저 수록합니다.\n2.  장르 내에서 많이 재생된 노래를 먼저 수록합니다.\n3.  장르 내에서 재생 횟수가 같은 노래 중에서는 고유 번호가 낮은 노래를 먼저 수록합니다.\n\n노래의 장르를 나타내는 문자열 배열 genres와 노래별 재생 횟수를 나타내는 정수 배열 plays가 주어질 때, 베스트 앨범에 들어갈 노래의 고유 번호를 순서대로 return 하도록 solution 함수를 완성하세요.\n\n### 제한사항\n\n-   genres[i]는 고유번호가 i인 노래의 장르입니다.\n-   plays[i]는 고유번호가 i인 노래가 재생된 횟수입니다.\n-   genres와 plays의 길이는 같으며, 이는 1 이상 10,000 이하입니다.\n-   장르 종류는 100개 미만입니다.\n-   장르에 속한 곡이 하나라면, 하나의 곡만 선택합니다.\n-   모든 장르는 재생된 횟수가 다릅니다.\n\n### 입출력 예\n\n| genres | plays | return |\n| ------ | ----- | ------ |\n| [\"classic\", \"pop\", \"classic\", \"classic\", \"pop\"]       | [500, 600, 150, 800, 2500]      |      [4, 1, 3, 0]  |\n\n### 입출력 예 설명\n\nclassic 장르는 1,450회 재생되었으며, classic 노래는 다음과 같습니다.\n\n-   고유 번호 3: 800회 재생\n-   고유 번호 0: 500회 재생\n-   고유 번호 2: 150회 재생\n\npop 장르는 3,100회 재생되었으며, pop 노래는 다음과 같습니다.\n\n-   고유 번호 4: 2,500회 재생\n-   고유 번호 1: 600회 재생\n\n따라서 pop 장르의 [4, 1]번 노래를 먼저, classic 장르의 [3, 0]번 노래를 그다음에 수록합니다.\n\n-   장르 별로 가장 많이 재생된 노래를 최대 두 개까지 모아 베스트 앨범을 출시하므로 2번 노래는 수록되지 않습니다.\n\n## 문제 풀이\n\n```python\ndef solution(genres, plays):\n    answer = []\n    albums = {} #set의 효과가 있다. \n\n    for g in range(len(genres)):\n        i,j = genres[g], plays[g]\n        if i not in albums:\n            albums[i] = []\n        albums[i].append([j, g])\n\n    ls = []\n    for genre in albums:\n        temp = 0\n        for num, _ in albums[genre]:\n            temp += num\n        ls.append([temp, genre])\n    ls.sort(reverse = True)\n\n    for _, genre in ls:\n        albums[genre].sort(key = lambda x: (-x[0], x[1]))\n        if len(albums[genre]) == 1:\n            answer.append(albums[genre][0][1])\n            continue\n        answer.append(albums[genre][0][1])\n        answer.append(albums[genre][1][1])\n\n    return answer\n```","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%85%E1%85%A7%E1%86%A8%E1%84%86%E1%85%A1%E1%86%BC-%E1%84%83%E1%85%AE%E1%86%AF%E1%84%85%E1%85%A9-%E1%84%82%E1%85%A1%E1%84%82%E1%85%AE%E1%84%80%E1%85%B5":{"title":"전력망 둘로 나누기","content":"## 문제 설명\n\nn개의 송전탑이 전선을 통해 하나의 [트리](https://en.wikipedia.org/wiki/Tree_(data_structure)) 형태로 연결되어 있습니다. 당신은 이 전선들 중 하나를 끊어서 현재의 전력망 네트워크를 2개로 분할하려고 합니다. 이때, 두 전력망이 갖게 되는 송전탑의 개수를 최대한 비슷하게 맞추고자 합니다.\n\n송전탑의 개수 n, 그리고 전선 정보 wires가 매개변수로 주어집니다. 전선들 중 하나를 끊어서 송전탑 개수가 가능한 비슷하도록 두 전력망으로 나누었을 때, 두 전력망이 가지고 있는 송전탑 개수의 차이(절대값)를 return 하도록 solution 함수를 완성해주세요.\n\n---\n\n### 제한사항\n\n-   n은 2 이상 100 이하인 자연수입니다.\n-   wires는 길이가 `n-1`인 정수형 2차원 배열입니다.\n    -   wires의 각 원소는 [v1, v2] 2개의 자연수로 이루어져 있으며, 이는 전력망의 v1번 송전탑과 v2번 송전탑이 전선으로 연결되어 있다는 것을 의미합니다.\n    -   1 ≤ v1 \u003c v2 ≤ n 입니다.\n    -   전력망 네트워크가 하나의 트리 형태가 아닌 경우는 입력으로 주어지지 않습니다.\n\n---\n\n### 입출력 예\n\n| n   | wires                                               | result |\n| --- | --------------------------------------------------- | ------ |\n| 9   | `[[1,3],[2,3],[3,4],[4,5],[4,6],[4,7],[7,8],[7,9]]` | 3      |\n| 4   | `[[1,2],[2,3],[3,4]]`                               | 0      |\n| 7   | `[[1,2],[2,7],[3,7],[3,4],[4,5],[6,7]]`             | 1      |\n\n---\n\n### 입출력 예 설명\n\n입출력 예 #1\n\n-   다음 그림은 주어진 입력을 해결하는 방법 중 하나를 나타낸 것입니다.\n-   ![ex1.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/5b8a0dcd-cba0-47ca-b5e3-d3bafc81f9d6/ex1.png)\n-   4번과 7번을 연결하는 전선을 끊으면 두 전력망은 각 6개와 3개의 송전탑을 가지며, 이보다 더 비슷한 개수로 전력망을 나눌 수 없습니다.\n-   또 다른 방법으로는 3번과 4번을 연결하는 전선을 끊어도 최선의 정답을 도출할 수 있습니다.\n\n입출력 예 #2\n\n-   다음 그림은 주어진 입력을 해결하는 방법을 나타낸 것입니다.\n-   ![ex2.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/b28865e1-a18e-429d-ae7a-14e77e801539/ex2.png)\n-   2번과 3번을 연결하는 전선을 끊으면 두 전력망이 모두 2개의 송전탑을 가지게 되며, 이 방법이 최선입니다.\n\n입출력 예 #3\n\n-   다음 그림은 주어진 입력을 해결하는 방법을 나타낸 것입니다.\n-   ![ex3.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/0a7f21af-1e07-4015-8ad3-c06155c613b3/ex3.png)\n-   3번과 7번을 연결하는 전선을 끊으면 두 전력망이 각각 4개와 3개의 송전탑을 가지게 되며, 이 방법이 최선입니다.\n\n\n## 나의 풀이\n\n```python\nfrom collections import deque\n\ndef bfs(start, visited, graph):\n    q = deque([start])\n    n_connected = 1\n    visited[start] = True\n    \n    while q:\n        curr = q.popleft()\n        \n        for neighbor in graph[curr]:\n            if not visited[neighbor]:\n                n_connected += 1\n                q.append(neighbor)\n                visited[neighbor] = True\n                \n    return n_connected\n\ndef solution(n, wires):\n    answer = n\n    graph = [[] for _ in range(n+1)]\n    \n    for v1,v2 in wires:\n        graph[v1].append(v2)\n        graph[v2].append(v1)\n\n    for start, end in wires:\n        visited = [False]*(n+1)\n        visited[end] = True\n        result = bfs(start,visited,graph)\n        if abs(result - (n-result)) \u003c answer:\n            answer = abs(result - (n-result))\n\n    \n    return answer\n```","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%E1%84%8C%E1%85%AE%E1%86%AF-%E1%84%89%E1%85%A5%E1%84%82%E1%85%B3%E1%86%AB-%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8":{"title":"줄 서는 방법","content":"## 문제 설명\n\nn명의 사람이 일렬로 줄을 서고 있습니다. n명의 사람들에게는 각각 1번부터 n번까지 번호가 매겨져 있습니다. n명이 사람을 줄을 서는 방법은 여러가지 방법이 있습니다. 예를 들어서 3명의 사람이 있다면 다음과 같이 6개의 방법이 있습니다.\n\n- `[1, 2, 3]`\n- `[1, 3, 2]`\n- `[2, 1, 3]`\n- `[2, 3, 1]`\n- `[3, 1, 2]`\n- `[3, 2, 1]`\n\n사람의 수 n과, 자연수 k가 주어질 때, 사람을 나열 하는 방법을 사전 순으로 나열 했을 때, k번째 방법을 return하는 solution 함수를 완성해주세요.\n\n###### 제한사항\n\n-   n은 20이하의 자연수 입니다.\n-   k는 n! 이하의 자연수 입니다.\n\n---\n\n### 입출력 예\n\n| n   | k   | result |\n| --- | --- | ------ |\n| 3    |6     |  `[3,1,2]`      |\n\n### 입출력 예시 설명\n\n입출력 예 #1  \n문제의 예시와 같습니다.\n\n## 문제 풀이 \n\n```python\nimport math\n\ndef solution(n, k):\n\tanswer = []\n\tk -= 1\n    ls = [x for x in range(1, n+1)]\n\n    for i in range(n, 0, -1):\n        max_num = math.factorial(n)\n        split_num = max_num // n\n        answer.append(ls[k//split_num])\n        ls.pop(k//split_num)\n        k %= split_num\n        n -= 1\n    return answer\n```\n\n이 문제는 모든 경우를 구하기보다, 숫자가 등장하는 패턴을 파악하는 것이 중요하다. ","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EA%B2%8C%EC%9E%84-%EB%A7%B5-%EC%B5%9C%EB%8B%A8%EA%B1%B0%EB%A6%AC":{"title":"게임 맵 최단거리","content":"ROR 게임은 두 팀으로 나누어서 진행하며, 상대 팀 진영을 먼저 파괴하면 이기는 게임입니다. 따라서, 각 팀은 상대 팀 진영에 최대한 빨리 도착하는 것이 유리합니다. \n\n지금부터 당신은 한 팀의 팀원이 되어 게임을 진행하려고 합니다. 다음은 5 x 5 크기의 맵에, 당신의 캐릭터가 (행: 1, 열: 1) 위치에 있고, 상대 팀 진영은 (행: 5, 열: 5) 위치에 있는 경우의 예시입니다.\n\n![최단거리1_sxuruo.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/dc3a1b49-13d3-4047-b6f8-6cc40b2702a7/%E1%84%8E%E1%85%AC%E1%84%83%E1%85%A1%E1%86%AB%E1%84%80%E1%85%A5%E1%84%85%E1%85%B51_sxuruo.png)\n\n위 그림에서 검은색 부분은 벽으로 막혀있어 갈 수 없는 길이며, 흰색 부분은 갈 수 있는 길입니다. 캐릭터가 움직일 때는 동, 서, 남, 북 방향으로 한 칸씩 이동하며, 게임 맵을 벗어난 길은 갈 수 없습니다.  \n아래 예시는 캐릭터가 상대 팀 진영으로 가는 두 가지 방법을 나타내고 있습니다.\n\n-   첫 번째 방법은 11개의 칸을 지나서 상대 팀 진영에 도착했습니다.\n\n![최단거리2_hnjd3b.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/9d909e5a-ca95-4088-9df9-d84cb804b2b0/%E1%84%8E%E1%85%AC%E1%84%83%E1%85%A1%E1%86%AB%E1%84%80%E1%85%A5%E1%84%85%E1%85%B52_hnjd3b.png)\n\n-   두 번째 방법은 15개의 칸을 지나서 상대팀 진영에 도착했습니다.\n\n![최단거리3_ntxygd.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/4b7cd629-a3c2-4e02-b748-a707211131de/%E1%84%8E%E1%85%AC%E1%84%83%E1%85%A1%E1%86%AB%E1%84%80%E1%85%A5%E1%84%85%E1%85%B53_ntxygd.png)\n\n위 예시에서는 첫 번째 방법보다 더 빠르게 상대팀 진영에 도착하는 방법은 없으므로, 이 방법이 상대 팀 진영으로 가는 가장 빠른 방법입니다.\n\n만약, 상대 팀이 자신의 팀 진영 주위에 벽을 세워두었다면 상대 팀 진영에 도착하지 못할 수도 있습니다. 예를 들어, 다음과 같은 경우에 당신의 캐릭터는 상대 팀 진영에 도착할 수 없습니다.\n\n![최단거리4_of9xfg.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/d963b4bd-12e5-45da-9ca7-549e453d58a9/%E1%84%8E%E1%85%AC%E1%84%83%E1%85%A1%E1%86%AB%E1%84%80%E1%85%A5%E1%84%85%E1%85%B54_of9xfg.png)\n\n게임 맵의 상태 maps가 매개변수로 주어질 때, 캐릭터가 상대 팀 진영에 도착하기 위해서 지나가야 하는 칸의 개수의 **최솟값**을 return 하도록 solution 함수를 완성해주세요. 단, 상대 팀 진영에 도착할 수 없을 때는 -1을 return 해주세요.\n\n### 제한사항\n\n-   maps는 n x m 크기의 게임 맵의 상태가 들어있는 2차원 배열로, n과 m은 각각 1 이상 100 이하의 자연수입니다.\n    -   n과 m은 서로 같을 수도, 다를 수도 있지만, n과 m이 모두 1인 경우는 입력으로 주어지지 않습니다.\n-   maps는 0과 1로만 이루어져 있으며, 0은 벽이 있는 자리, 1은 벽이 없는 자리를 나타냅니다.\n-   처음에 캐릭터는 게임 맵의 좌측 상단인 (1, 1) 위치에 있으며, 상대방 진영은 게임 맵의 우측 하단인 (n, m) 위치에 있습니다.\n\n---\n\n### 입출력 예\n\n| maps                                                          | answer |\n| ------------------------------------------------------------- | ------ |\n| [\\[1,0,1,1,1],[1,0,1,0,1],[1,0,1,1,1],[1,1,1,0,1],[0,0,0,0,1]] | 11     |\n| [\\[1,0,1,1,1],[1,0,1,0,1],[1,0,1,1,1],[1,1,1,0,0],[0,0,0,0,1]]                                            |      -1  |\n\n### 입출력 예 설명\n\n**입출력 예 #1**  \n주어진 데이터는 다음과 같습니다.\n\n![최단거리6_lgjvrb.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/6db71f7f-58d3-4623-9fab-7cd99fa863a5/%E1%84%8E%E1%85%AC%E1%84%83%E1%85%A1%E1%86%AB%E1%84%80%E1%85%A5%E1%84%85%E1%85%B56_lgjvrb.png)\n\n캐릭터가 적 팀의 진영까지 이동하는 가장 빠른 길은 다음 그림과 같습니다.\n\n![최단거리2_hnjd3b (1).png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/d223d017-b3e2-4772-9045-a565133d45ff/%E1%84%8E%E1%85%AC%E1%84%83%E1%85%A1%E1%86%AB%E1%84%80%E1%85%A5%E1%84%85%E1%85%B52_hnjd3b%20%281%29.png)\n\n따라서 총 11칸을 캐릭터가 지나갔으므로 11을 return 하면 됩니다.\n\n**입출력 예 #2**  \n문제의 예시와 같으며, 상대 팀 진영에 도달할 방법이 없습니다. 따라서 -1을 return 합니다.\n\n## 나의 풀이\n\n```python\nfrom collections import deque\n\ndef bfs(graph, x, y):\n    \n    # 이동할 네 가지 방향 정의 (상, 하, 좌, 우)\n    dx = [-1, 1, 0, 0]\n    dy = [0, 0, -1, 1]\n\n    n = len(graph)\n    m = len(graph[0])\n\n    visited = [[False]*m for _ in range(n)]\n        \n    queue = deque()\n    queue.append((x, y))\n    visited[0][0]=True\n    \n    # 큐가 빌 때까지 반복하기\n    while queue:\n        x, y = queue.popleft()\n        # 현재 위치에서 4가지 방향으로의 위치 확인\n        for i in range(4):\n            nx = x + dx[i]\n            ny = y + dy[i]\n            # 주어진 공간을 벗어난 경우 무시\n            if nx \u003c 0 or nx \u003e= n or ny \u003c 0 or ny \u003e= m:\n                continue\n            # 벽인 경우 무시\n            if graph[nx][ny] == 0:\n                continue\n            # 해당 노드를 처음 방문하는 경우에만 최단 거리 기록\n            if graph[nx][ny] == 1:\n                if not visited[nx][ny]:\n                        visited[nx][ny] = True\n                        queue.append((nx, ny))\n                        graph[nx][ny] = graph[x][y] + 1\n    \n    # 큐가 비었는데도 도착지가 1이라면 벽으로 막혀있는 것.\n    if graph[n-1][m-1]==1:\n        return -1\n    \n    # 가장 오른쪽 아래까지의 최단 거리 반환\n    return graph[n - 1][m - 1]\n\ndef solution(maps):\n    return bfs(maps, 0, 0)\n```\n\n이 문제는 최단거리 문제로 [BFS](notes/algorithms/DFS,%20BFS.md) 알고리즘을 이용하는 문제이다. 특이할 만한 점은, 도착지가 벽으로 막혀 있는 상황을 가정해야 한다는 것이다. 처음에는 일일이 조건을 붙이다가, 결국에는 이러한 상황이란 **'큐가 비었는데도 여전히 도착지의 값이 1인 상황'** 이라 판단했다. 그러고나니 코드가 훨씬 간소해졌다. ","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EA%B5%AC%EB%AA%85%EB%B3%B4%ED%8A%B8":{"title":"구명보트","content":"무인도에 갇힌 사람들을 구명보트를 이용하여 구출하려고 합니다. 구명보트는 작아서 한 번에 최대 **2명**씩 밖에 탈 수 없고, 무게 제한도 있습니다.\n\n예를 들어, 사람들의 몸무게가 [70kg, 50kg, 80kg, 50kg]이고 구명보트의 무게 제한이 100kg이라면 2번째 사람과 4번째 사람은 같이 탈 수 있지만 1번째 사람과 3번째 사람의 무게의 합은 150kg이므로 구명보트의 무게 제한을 초과하여 같이 탈 수 없습니다.\n\n구명보트를 최대한 적게 사용하여 모든 사람을 구출하려고 합니다.\n\n사람들의 몸무게를 담은 배열 people과 구명보트의 무게 제한 limit가 매개변수로 주어질 때, 모든 사람을 구출하기 위해 필요한 구명보트 개수의 최솟값을 return 하도록 solution 함수를 작성해주세요.\n\n## 제한사항\n\n-   무인도에 갇힌 사람은 1명 이상 50,000명 이하입니다.\n-   각 사람의 몸무게는 40kg 이상 240kg 이하입니다.\n-   구명보트의 무게 제한은 40kg 이상 240kg 이하입니다.\n-   구명보트의 무게 제한은 항상 사람들의 몸무게 중 최댓값보다 크게 주어지므로 사람들을 구출할 수 없는 경우는 없습니다.\n\n## 입출력 예\n\n| people           | limit | return |\n| ---------------- | ----- | ------ |\n| [70, 50, 80, 50] | 100   | 3      |\n| [70, 80, 50]     | 100   | 3      |\n\n\n## 나의 풀이\n\n```python\ndef solution(people, limit) :\n    gone = 0\n    people.sort()\n\n    a = 0\n    b = len(people) - 1\n    \n    while a \u003c b :\n        if people[b] + people[a] \u003c= limit :\n            a += 1\n            gone += 1\n        b -= 1\n    \n    answer = len(people) - gone\n    \n    return answer\n```\n\n이 문제에서 사용된  [Two Pointers](notes/algorithms/Two%20Pointers.md) 알고리즘은 1차원 배열에서 두 개의 포인터를 조작하여 기존의 방식보다 빠르게 원하는 결과를 얻어내는 알고리즘이다.\n","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EA%B8%B0%EB%8A%A5%EA%B0%9C%EB%B0%9C":{"title":"기능개발","content":"\n-  프로그래머스 팀에서는 기능 개선 작업을 수행 중입니다. 각 기능은 진도가 100%일 때 서비스에 반영할 수 있습니다.  \n- 또, 각 기능의 개발속도는 모두 다르기 때문에 뒤에 있는 기능이 앞에 있는 기능보다 먼저 개발될 수 있고, 이때 뒤에 있는 기능은 앞에 있는 기능이 배포될 때 함께 배포됩니다.  \n- 먼저 배포되어야 하는 순서대로 작업의 진도가 적힌 정수 배열 progresses와 각 작업의 개발 속도가 적힌 정수 배열 speeds가 주어질 때 각 배포마다 몇 개의 기능이 배포되는지를 `return` 하도록 `solution` 함수를 완성하세요.  \n\n## 제한 사항\n\n-   작업의 개수(progresses, speeds배열의 길이)는 100개 이하입니다.\n-   작업 진도는 100 미만의 자연수입니다.\n-   작업 속도는 100 이하의 자연수입니다.\n-   배포는 하루에 한 번만 할 수 있으며, 하루의 끝에 이루어진다고 가정합니다. 예를 들어 진도율이 95%인 작업의 개발 속도가 하루에 4%라면 배포는 2일 뒤에 이루어집니다.\n\n## 입출력 예\n\n**입출력 예 #1**\n첫 번째 기능은 93% 완료되어 있고 하루에 1%씩 작업이 가능하므로 7일간 작업 후 배포가 가능합니다.두 번째 기능은 30%가 완료되어 있고 하루에 30%씩 작업이 가능하므로 3일간 작업 후 배포가 가능합니다. 하지만 이전 첫 번째 기능이 아직 완성된 상태가 아니기 때문에 첫 번째 기능이 배포되는 7일째 배포됩니다.세 번째 기능은 55%가 완료되어 있고 하루에 5%씩 작업이 가능하므로 9일간 작업 후 배포가 가능합니다.\n\n따라서 7일째에 2개의 기능, 9일째에 1개의 기능이 배포됩니다.\n\n**입출력 예 #2**\n모든 기능이 하루에 1%씩 작업이 가능하므로, 작업이 끝나기까지 남은 일수는 각각 5일, 10일, 1일, 1일, 20일, 1일입니다. 어떤 기능이 먼저 완성되었더라도 앞에 있는 모든 기능이 완성되지 않으면 배포가 불가능합니다.\n\n따라서 5일째에 1개의 기능, 10일째에 3개의 기능, 20일째에 2개의 기능이 배포됩니다.\n\n## 풀이\n\n```python\ndef solution(progresses, speeds):\n    Q=[]\n    for p, s in zip(progresses, speeds):\n        if len(Q)==0 or Q[-1][0]\u003c-((p-100)//s):\n            Q.append([-((p-100)//s),1])\n        else:\n            Q[-1][1]+=1\n    return [q[1] for q in Q]\n```\n","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC":{"title":"네트워크","content":"\n네트워크란 컴퓨터 상호 간에 정보를 교환할 수 있도록 연결된 형태를 의미합니다. 예를 들어, 컴퓨터 A와 컴퓨터 B가 직접적으로 연결되어있고, 컴퓨터 B와 컴퓨터 C가 직접적으로 연결되어 있을 때 컴퓨터 A와 컴퓨터 C도 간접적으로 연결되어 정보를 교환할 수 있습니다. 따라서 컴퓨터 A, B, C는 모두 같은 네트워크 상에 있다고 할 수 있습니다.\n\n컴퓨터의 개수 n, 연결에 대한 정보가 담긴 2차원 배열 computers가 매개변수로 주어질 때, 네트워크의 개수를 return 하도록 solution 함수를 작성하시오.\n\n### 제한사항\n\n-   컴퓨터의 개수 n은 1 이상 200 이하인 자연수입니다.\n-   각 컴퓨터는 0부터 `n-1`인 정수로 표현합니다.\n-   i번 컴퓨터와 j번 컴퓨터가 연결되어 있으면 computers\\[i]\\[j]를 1로 표현합니다.\n-   computer\\[i]\\[i]는 항상 1입니다.\n\n### 입출력 예\n\n| n   | computers                         | return |\n| --- | --------------------------------- | ------ |\n| 3   | \\[[1, 1, 0], [1, 1, 0], [0, 0, 1]] | 2      |\n| 3   | \\[[1, 1, 0], [1, 1, 1], [0, 1, 1]] | 1      |\n\n### 입출력 예 설명\n\n**예제 #1**  \n아래와 같이 2개의 네트워크가 있습니다.  \n![image0.png](https://grepp-programmers.s3.amazonaws.com/files/ybm/5b61d6ca97/cc1e7816-b6d7-4649-98e0-e95ea2007fd7.png)\n\n**예제 #2**  \n아래와 같이 1개의 네트워크가 있습니다.  \n![image1.png](https://grepp-programmers.s3.amazonaws.com/files/ybm/7554746da2/edb61632-59f4-4799-9154-de9ca98c9e55.png)\n\n## 나의 풀이\n\n```python\ndef dfs(node, computers, visited):\n    visited[node] = True \n    \n    for i, neighbor in enumerate(computers[node]):\n        # 인접노드 중 자기 자신이 아니고 아직 방문하지 않은 노드에 대해 \n        if neighbor \u003e 0 \n        and i != node \n        and visited[i] == False:\n            dfs(i, computers, visited)\n    return visited\n    \n\ndef solution(n, computers):\n    answer = 1\n    visited = [False] * n\n    start_node = 0\n    \n    while True:\n        visited = dfs(start_node, computers, visited)\n        \n        if False in visited:\n            start_node = visited.index(False)\n            answer += 1\n        else:\n            break\n    \n    return answer\n```\n\n이 문제는 깊이 우선 탐색, [DFS](notes/algorithms/DFS,%20BFS.md) 알고리즘을 이용해 푸는 문제다. 이번에는 연결된 노드가 인덱스가 아니라 연결됨(1)과 연결되지 않음(0)으로 주어져서, 따로 `enumerate` 함수로 인덱스를 가져와서 풀어야 했다. \n우선 0번째 컴퓨터부터 시작해서 이와 직/간접적으로 연결되어 있는 모든 컴퓨터들 방문한다. 이후에도 아직 방문하지 않은 컴퓨터가 있으면 `answer`를 하나씩 증가시키고, 방문하지 않은 컴퓨터 중 번호가 가장 작은 것에서부터 시작해 다시 `dfs`를 호출한다. 마지막으로  모든 컴퓨터를 방문하면 반복을 종료하고 `answer`를 반환합니다.","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EB%8B%A4%EC%9D%8C-%ED%81%B0-%EC%88%AB%EC%9E%90":{"title":"다음 큰 숫자","content":"\n자연수 n이 주어졌을 때, n의 다음 큰 숫자는 다음과 같이 정의 합니다.\n\n-   조건 1. n의 다음 큰 숫자는 n보다 큰 자연수 입니다.\n-   조건 2. n의 다음 큰 숫자와 n은 2진수로 변환했을 때 1의 갯수가 같습니다.\n-   조건 3. n의 다음 큰 숫자는 조건 1, 2를 만족하는 수 중 가장 작은 수 입니다.\n\n예를 들어서 78(1001110)의 다음 큰 숫자는 83(1010011)입니다.\n\n**자연수 n이 매개변수로 주어질 때, n의 다음 큰 숫자를 return 하는 solution 함수를 완성해주세요.**\n\n### 제한 사항\n-   n은 1,000,000 이하의 자연수 입니다.\n\n### 입출력 예\n| n   | result |\n| --- | ------ |\n| 78  | 83     |\n| 15    |      23  |\n\n### 입출력 예 설명\n- 입출력 예#1  \n\t- 문제 예시와 같습니다.  \n- 입출력 예#2  \n\t- 15(1111)의 다음 큰 숫자는 23(10111)입니다.\n--- \n\n## 좋은 풀이\n  \n```python\ndef solution(n):\n    num1 = bin(n).count('1')\n    while True:\n        n = n + 1\n        if num1 == bin(n).count('1'):\n            break\n    return n\n```\n  \n## 나의 풀이\n  \n```python\nfrom collections import Counter\n\ndef find_one(k):\n    return Counter(list(format(k, 'b')))['1']\n\ndef solution(n):\n    answer = n\n    while True:\n        answer += 1\n        if find_one(n) == find_one(answer):\n            break\n    return answer\n```\n   \n## 둘의 차이점\n우선 `Counter()`를 쓸 이유가 없었다. 왜냐하면 문자열 뒤에는` .count()`를 사용할 수 있기 때문이다. 굳이 `Counter()`를 쓰기 위해 리스트로 변환하는 작업도 필요가 없었던 것이다. 즉, 연산 낭비이다. 기억하자, 문자열에서 특정한 문자의 갯수를 알고 싶다면 `.count()`를 쓰면 된다는 것. ","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EB%8B%A8%EC%96%B4%EC%9E%A5-%EB%A7%8C%EB%93%A4%EA%B8%B0":{"title":"단어장 만들기","content":"\n## 입력\n```\n7 3 \naaaa \naaa \naa \na \nb \nbb \nbbb\n```\n\n## 조건\n\n- 단어들을 정렬하고 $k$ 번째에 있는 단어 출력하기\n- 정렬은 길이 순서\n- 단, 길이가 같은 단어의 경우 사전 순서\n\n## 나의 풀이\n```python\nuser_input = input()\nn, k = map(int, user_input.split())\nls = []\nfor _ in range(n):\n\tls.append(input())\n\nls.sort(key=lambda x: (len(x), x))\nprint(ls[k-1])\n```\n\n\u003e [!note] Note  \n\u003e   \n\u003e 조건이 많이 나오는 유형이라 기록한다. 이 문제는 리스트에 존재하는 문자들을 **길이와 사전 순으로 정렬**하는 문제이다. 문자열 구현 유형에서 많이 나오는 스킬인 만큼 `ls.sort(key=lambda x: (len(x), x))` 테크닉은 익혀두는 것이 좋다. 그냥 `key=len`으로 하면 길이순으로만 요소를 정렬하지만 `key=lambda x: (len(x), x)`으로 하면 길이와 사전 순으로 정렬시킨다.\n","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EB%91%90-%ED%81%90-%ED%95%A9-%EA%B0%99%EA%B2%8C-%EB%A7%8C%EB%93%A4%EA%B8%B0":{"title":"두 큐 합 같게 만들기","content":"\n길이가 같은 두 개의 큐가 주어집니다. 하나의 큐를 골라 원소를 추출(pop)하고, 추출된 원소를 **다른 큐**에 집어넣는(insert) 작업을 통해 각 큐의 원소 합이 같도록 만들려고 합니다. 이때 필요한 작업의 최소 횟수를 구하고자 합니다. 한 번의 pop과 한 번의 insert를 합쳐서 작업을 1회 수행한 것으로 간주합니다.\n\n큐는 먼저 집어넣은 원소가 먼저 나오는 구조입니다. 이 문제에서는 큐를 배열로 표현하며, 원소가 배열 앞쪽에 있을수록 먼저 집어넣은 원소임을 의미합니다. 즉, pop을 하면 배열의 첫 번째 원소가 추출되며, insert를 하면 배열의 끝에 원소가 추가됩니다. 예를 들어 큐 `[1, 2, 3, 4]`가 주어졌을 때, pop을 하면 맨 앞에 있는 원소 1이 추출되어 `[2, 3, 4]`가 되며, 이어서 5를 insert하면 `[2, 3, 4, 5]`가 됩니다.\n\n다음은 두 큐를 나타내는 예시입니다.\n\n```\nqueue1 = [3, 2, 7, 2]\nqueue2 = [4, 6, 5, 1]\n```\n\n두 큐에 담긴 모든 원소의 합은 30입니다. 따라서, 각 큐의 합을 15로 만들어야 합니다. 예를 들어, 다음과 같이 2가지 방법이 있습니다.\n\n1.  queue2의 4, 6, 5를 순서대로 추출하여 queue1에 추가한 뒤, queue1의 3, 2, 7, 2를 순서대로 추출하여 queue2에 추가합니다. 그 결과 queue1은 [4, 6, 5], queue2는 [1, 3, 2, 7, 2]가 되며, 각 큐의 원소 합은 15로 같습니다. 이 방법은 작업을 7번 수행합니다.\n2.  queue1에서 3을 추출하여 queue2에 추가합니다. 그리고 queue2에서 4를 추출하여 queue1에 추가합니다. 그 결과 queue1은 [2, 7, 2, 4], queue2는 [6, 5, 1, 3]가 되며, 각 큐의 원소 합은 15로 같습니다. 이 방법은 작업을 2번만 수행하며, 이보다 적은 횟수로 목표를 달성할 수 없습니다.\n\n따라서 각 큐의 원소 합을 같게 만들기 위해 필요한 작업의 최소 횟수는 2입니다.\n\n길이가 같은 두 개의 큐를 나타내는 정수 배열 `queue1`, `queue2`가 매개변수로 주어집니다. 각 큐의 원소 합을 같게 만들기 위해 필요한 작업의 최소 횟수를 return 하도록 solution 함수를 완성해주세요. 단, 어떤 방법으로도 각 큐의 원소 합을 같게 만들 수 없는 경우, -1을 return 해주세요.\n\n---\n\n### 제한사항\n\n-   1 ≤ `queue1`의 길이 = `queue2`의 길이 ≤ 300,000\n-   1 ≤ `queue1`의 원소, `queue2`의 원소 ≤ 10^9\n-   주의: 언어에 따라 합 계산 과정 중 산술 오버플로우 발생 가능성이 있으므로 long type 고려가 필요합니다.\n\n---\n\n### 입출력 예\n| queue1       | queue2        | result |\n| ------------ | ------------- | ------ |\n| [3, 2, 7, 2] | [4, 6, 5, 1]  | 2      |\n| [1, 2, 1, 2] | [1, 10, 1, 2] | 7      |\n| [1, 1]             |  [1, 5]             |   -1     |\n\n---\n\n### 입출력 예 설명\n\n**입출력 예 #1**\n\n문제 예시와 같습니다.\n\n**입출력 예 #2**\n\n두 큐에 담긴 모든 원소의 합은 20입니다. 따라서, 각 큐의 합을 10으로 만들어야 합니다. queue2에서 1, 10을 순서대로 추출하여 queue1에 추가하고, queue1에서 1, 2, 1, 2와 1(queue2으로부터 받은 원소)을 순서대로 추출하여 queue2에 추가합니다. 그 결과 queue1은 [10], queue2는 [1, 2, 1, 2, 1, 2, 1]가 되며, 각 큐의 원소 합은 10으로 같습니다. 이때 작업 횟수는 7회이며, 이보다 적은 횟수로 목표를 달성하는 방법은 없습니다. 따라서 7를 return 합니다.\n\n**입출력 예 #3**\n\n어떤 방법을 쓰더라도 각 큐의 원소 합을 같게 만들 수 없습니다. 따라서 -1을 return 합니다.\n\n---\n\n### 나의 풀이\n\n```python\nfrom collections import deque\n\ndef act(queue1, queue2, reverse = None):\n    if reverse == None:\n        gone = queue1[0]\n        queue2.append(queue1.popleft())\n    else:\n        gone = queue2[0]\n        queue1.append(queue2.popleft())\n\n    return queue1, queue2, gone\n\ndef solution(queue1, queue2):\n    answer = 0\n\n    queue1 = deque(queue1)\n    queue2 = deque(queue2)\n\n    #먼저 기준을 잡아두고 시작하자.\n    flag =  len(queue1)\n    left = sum(queue1) \n    right = sum(queue2)\n\n    while left != right:\n        if left \u003e right:\n            queue1, queue2, gone = act(queue1, queue2)\n            answer += 1\n            left -= gone\n            right += gone\n        elif left \u003c right:\n            queue1, queue2, gone = act(queue1, queue2,\n\t\t\t\t\t\t\t        reverse = True)\n            answer += 1\n            right -= gone\n            left += gone\n        else:\n            break\n        if answer \u003e flag * 3:\n            return -1\n\n    return answer\n```\n\n역시 `deque`가 빠르다. 괜히 리스트로 풀려고 했다가 시간초과가 자꾸 떠서 시간만 날려먹었네...이렇게 명시적으로 `FIFO` 놀이를 시킨다면 고민하지말고  `deque`를 불러오자. \n\n그것과는 별개로, 마지막 `-1` 리턴하는 코드가 미완성이다. 이건 소위 야매로 풀어버린거라...이 점은 더 생각해봐야겠다. 그래도 `act()` 함수를 새로 만들어서 푼 것은 나쁘지 않은 선택이었던 것 같다. ","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EB%A9%80%EB%A6%AC%EB%9B%B0%EA%B8%B0":{"title":"멀리뛰기","content":"\n효진이는 멀리 뛰기를 연습하고 있습니다. 효진이는 한번에 1칸, 또는 2칸을 뛸 수 있습니다. 칸이 총 4개 있을 때, 효진이는\n\n```\n(1칸, 1칸, 1칸, 1칸)  \n(1칸, 2칸, 1칸)  \n(1칸, 1칸, 2칸)  \n(2칸, 1칸, 1칸)  \n(2칸, 2칸)  \n```\n\n의 5가지 방법으로 맨 끝 칸에 도달할 수 있습니다. \n\n**멀리뛰기에 사용될 칸의 수 n이 주어질 때, 효진이가 끝에 도달하는 방법이 몇 가지인지 알아내, 여기에 1234567를 나눈 나머지를 리턴하는 함수, solution을 완성하세요.** \n\n예를 들어 4가 입력된다면, 5를 return하면 됩니다.\n\n**제한 사항**\n\n-   n은 1 이상, 2000 이하인 정수입니다.\n\n**입출력 예**\n\n| n   | result |\n| --- | ------ |\n| 4   | 5      |\n| 3   | 3      |\n\n**입출력 예 설명**\n\n입출력 예 #1  \n위에서 설명한 내용과 같습니다.\n\n입출력 예 #2\n\n(2칸, 1칸)  \n(1칸, 2칸)  \n(1칸, 1칸, 1칸)  \n\n총 3가지 방법으로 멀리 뛸 수 있습니다.\n\n## 문제 풀이\n\n이 문제는 [Dynamic Programming](notes/algorithms/Dynamic%20Programming.md)을 이용하는 문제이다. `dp[n]` 은 `n`칸을 갈 수 있는 방법의 수이다. 그래서 `dp[1]`부터 `dp[n]`까지 경우의 수를 찾아 더해준다. 두 번째 for문에서는 1칸만으로 가는 경우의 수를 더해주고, 2칸도 사용해서 가는 경우의 수를 또 더해준다. \n\n결과적으로, 갈 수 있는 칸이 `[1,2]` 칸이기 때문에 `can-step`이 항상 `can-1` 또는 `can-2`가 되어 피보나치와 같아지게 된다. 이 코드는 만약 `[1,2]` 칸이 아니라 더 여러 개의 칸을 갈 수 있는 문제였어도 그대로 적용할 수 있다. \n\n피보나치로 푸는 코드는 다음과 같다.\n\n```python\ndef solution(n):\n    dp = [1] + [0] * n\n    dp[0], dp[1] = 1, 1\n    for i in range(2, n+1):\n        dp[i] = (dp[i-1] + dp[i-2]) % 1234567\n    return dp[n]\n```","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EC%88%AB%EC%9E%90-%EB%AC%B8%EC%9E%90%EC%97%B4%EA%B3%BC-%EC%98%81%EB%8B%A8%EC%96%B4":{"title":"숫자 문자열과 영단어","content":"\n![img1.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/d31cb063-4025-4412-8cbc-6ac6909cf93e/img1.png)\n\n네오와 프로도가 숫자놀이를 하고 있습니다. 네오가 프로도에게 숫자를 건넬 때 일부 자릿수를 영단어로 바꾼 카드를 건네주면 프로도는 원래 숫자를 찾는 게임입니다.  \n  \n다음은 숫자의 일부 자릿수를 영단어로 바꾸는 예시입니다.\n\n-   1478 → \"one4seveneight\"\n-   234567 → \"23four5six7\"\n-   10203 → \"1zerotwozero3\"\n\n이렇게 숫자의 일부 자릿수가 영단어로 바뀌어졌거나, 혹은 바뀌지 않고 그대로인 문자열 `s`가 매개변수로 주어집니다. `s`가 의미하는 원래 숫자를 return 하도록 solution 함수를 완성해주세요.\n\n---\n### 제한사항\n\n-   1 ≤ `s`의 길이 ≤ 50\n-   `s`가 \"zero\" 또는 \"0\"으로 시작하는 경우는 주어지지 않습니다.\n-   return 값이 1 이상 2,000,000,000 이하의 정수가 되는 올바른 입력만 `s`로 주어집니다.\n\n---\n### 입출력 예\n\n| s                    | result      |\n| -------------------- | ----------- |\n| `\"one4seveneight\"`   | 1478        |\n| `\"23four5six7\"`      | 234567 |\n| `\"2three45sixseven\"` | 234567      |\n|        `\"123\"`              |     123        |\n\n---\n### 입출력 예 설명\n\n**입출력 예 #1**\n\n-   문제 예시와 같습니다.\n\n**입출력 예 #2**\n\n-   문제 예시와 같습니다.\n\n**입출력 예 #3**\n\n-   \"three\"는 3, \"six\"는 6, \"seven\"은 7에 대응되기 때문에 정답은 입출력 예 2와 같은 234567이 됩니다.\n-   입출력 예 2와 3과 같이 같은 정답을 가리키는 문자열이 여러 가지가 나올 수 있습니다.\n\n**입출력 예 #4**\n\n-   `s`에는 영단어로 바뀐 부분이 없습니다.\n---\n### 제한시간 안내\n-   정확성 테스트 : 10초\n***\n\n## 내 풀이\n\n```python\ndef solution(s):\n    result = ''\n    eng = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n    for idx, num in enumerate(eng):\n        if num in s:\n            s = s.replace(num, str(idx))\n        result = s\n            \n    return int(result)\n```\n\n`enumerate` 가 반환하는 인덱스와 영문 숫자가 일치해서 가능한 방법이었다.\n주어진 문자열에 해당 영문 숫자가 있는지 확인하고, 있다면 `replace`로 대체했다.\n이후 result를 계속 업데이트하고 최종적으로 `int`를 씌워서 반환하면 끝이다.","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EC%88%AB%EC%9E%90-%EC%B9%B4%EB%93%9C-%EB%82%98%EB%88%84%EA%B8%B0":{"title":"숫자 카드 나누기","content":"\n철수와 영희는 선생님으로부터 숫자가 하나씩 적힌 카드들을 절반씩 나눠서 가진 후, 다음 두 조건 중 하나를 만족하는 가장 큰 양의 정수 `a`의 값을 구하려고 합니다.\n\n1.  철수가 가진 카드들에 적힌 모든 숫자를 나눌 수 있고 영희가 가진 카드들에 적힌 모든 숫자들 중 하나도 나눌 수 없는 양의 정수 `a`\n2.  영희가 가진 카드들에 적힌 모든 숫자를 나눌 수 있고, 철수가 가진 카드들에 적힌 모든 숫자들 중 하나도 나눌 수 없는 양의 정수 `a`\n\n예를 들어, 카드들에 10, 5, 20, 17이 적혀 있는 경우에 대해 생각해 봅시다. 만약, 철수가 [10, 17]이 적힌 카드를 갖고, 영희가 [5, 20]이 적힌 카드를 갖는다면 두 조건 중 하나를 만족하는 양의 정수 a는 존재하지 않습니다. 하지만, 철수가 [10, 20]이 적힌 카드를 갖고, 영희가 [5, 17]이 적힌 카드를 갖는다면, 철수가 가진 카드들의 숫자는 모두 10으로 나눌 수 있고, 영희가 가진 카드들의 숫자는 모두 10으로 나눌 수 없습니다. 따라서 철수와 영희는 각각 [10, 20]이 적힌 카드, [5, 17]이 적힌 카드로 나눠 가졌다면 조건에 해당하는 양의 정수 a는 10이 됩니다.\n\n철수가 가진 카드에 적힌 숫자들을 나타내는 정수 배열 `arrayA`와 영희가 가진 카드에 적힌 숫자들을 나타내는 정수 배열 `arrayB`가 주어졌을 때, 주어진 조건을 만족하는 가장 큰 양의 정수 a를 return하도록 solution 함수를 완성해 주세요. 만약, 조건을 만족하는 a가 없다면, 0을 return 해 주세요.\n\n---\n\n### 제한사항\n\n-   1 ≤ `arrayA`의 길이 = `arrayB`의 길이 ≤ 500,000\n-   1 ≤ `arrayA`의 원소, `arrayB`의 원소 ≤ 100,000,000\n-   `arrayA`와 `arrayB`에는 중복된 원소가 있을 수 있습니다.\n\n---\n\n### 입출력 예\n\n| arrayA   | arrayB  | result |\n| -------- | ------- | ------ |\n| [10, 17] | [5, 20] | 0      |\n| [10, 20] | [5, 17] | 10     |\n| [14, 35, 119]         |     [18, 30, 102]    |     7   |\n\n---\n\n### 입출력 예 설명\n\n**입출력 예 #1**\n\n-   문제 예시와 같습니다.\n\n**입출력 예 #2**\n\n-   문제 예시와 같습니다.\n\n**입출력 예 #3**\n\n-   철수가 가진 카드에 적힌 숫자들은 모두 3으로 나눌 수 없고, 영희가 가진 카드에 적힌 숫자는 모두 3으로 나눌 수 있습니다. 따라서 3은 조건에 해당하는 양의 정수입니다. 하지만, 철수가 가진 카드들에 적힌 숫자들은 모두 7로 나눌 수 있고, 영희가 가진 카드들에 적힌 숫자는 모두 7로 나눌 수 없습니다. 따라서 최대값인 7을 return 합니다.\n\n---\n## 나의 풀이\n\n```python\nfrom math import gcd\n\ndef solution(arrayA, arrayB):\n    gcd1, gcd2 = arrayA[0], arrayB[0]\n    for each1, each2 in zip(arrayA[1:], arrayB[1:]):\n        gcd1, gcd2 = gcd(each1, gcd1), gcd(each2, gcd2)\n    answer = []\n    for each1 in arrayA:\n        if each1 % gcd2 == 0:\n            break\n    else:\n        answer.append(gcd2)\n    for each2 in arrayB:\n        if each2 % gcd1 == 0:\n            break\n    else:\n        answer.append(gcd1)\n    return max(answer) if answer else 0\n```\n\n최대공약수(GCD)를 구해서 조건을 적용해보고, 조건이 모두 충족되면 answer 리스트에 저장한다. 그리고 answer에서 최대값을 꺼내서 return하면 끝.","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EC%8A%A4%ED%82%AC%ED%8A%B8%EB%A6%AC":{"title":"스킬트리","content":"선행 스킬이란 어떤 스킬을 배우기 전에 먼저 배워야 하는 스킬을 뜻합니다.\n\n예를 들어 선행 스킬 순서가 `스파크 → 라이트닝 볼트 → 썬더`일때, 썬더를 배우려면 먼저 라이트닝 볼트를 배워야 하고, 라이트닝 볼트를 배우려면 먼저 스파크를 배워야 합니다.\n\n위 순서에 없는 다른 스킬(힐링 등)은 순서에 상관없이 배울 수 있습니다. 따라서 `스파크 → 힐링 → 라이트닝 볼트 → 썬더`와 같은 스킬트리는 가능하지만, `썬더 → 스파크`나 `라이트닝 볼트 → 스파크 → 힐링 → 썬더`와 같은 스킬트리는 불가능합니다.\n\n선행 스킬 순서 `skill`과 유저들이 만든 스킬트리를 담은 배열 `skill_trees`가 매개변수로 주어질 때, 가능한 스킬트리 개수를 return 하는 solution 함수를 작성해주세요.\n\n### 제한 조건\n\n-   스킬은 알파벳 대문자로 표기하며, 모든 문자열은 알파벳 대문자로만 이루어져 있습니다.\n-   스킬 순서와 스킬트리는 문자열로 표기합니다.\n    -   예를 들어, `C → B → D` 라면 \"CBD\"로 표기합니다\n-   선행 스킬 순서 skill의 길이는 1 이상 26 이하이며, 스킬은 중복해 주어지지 않습니다.\n-   skill_trees는 길이 1 이상 20 이하인 배열입니다.\n-   skill_trees의 원소는 스킬을 나타내는 문자열입니다.\n    -   skill_trees의 원소는 길이가 2 이상 26 이하인 문자열이며, 스킬이 중복해 주어지지 않습니다.\n\n### 입출력 예\n\n| skill | skill_trees | return |\n| ----- | ----------- | ------ |\n| `\"CBD\"`      |  `[\"BACDE\", \"CBADF\", \"AECB\", \"BDA\"]`           |      2  |\n\n### 입출력 예 설명\n\n-   \"BACDE\": B 스킬을 배우기 전에 C 스킬을 먼저 배워야 합니다. 불가능한 스킬트립니다.\n-   \"CBADF\": 가능한 스킬트리입니다.\n-   \"AECB\": 가능한 스킬트리입니다.\n-   \"BDA\": B 스킬을 배우기 전에 C 스킬을 먼저 배워야 합니다. 불가능한 스킬트리입니다.\n\n## 나의 풀이\n\n```python\nfrom collections import deque\n\ndef solution(skill, skill_trees):\n    answer = 0\n\n    for skills in skill_trees:\n        skill_list = deque(list(skill))\n\n        for s in skills:\n            if s in skill:\n                if s != skill_list.popleft():\n                    break\n        else:\n            answer += 1\n\n    return answer\n```\n\n1. 먼저 각 스킬트리를 deque에 태운다. 이 deque는 왼쪽부터 제거될 예정.\n2. 그래서 만약 한 스킬트리의 한 글자가 skill에 포함되어 있다면, \n\t1. deque에서 가장 왼쪽의 글자를 빼고 둘을 비교한다.\n\t2. 둘이 같다면 계속하고, 다르다면 중지한다.\n\t3. 이때, deque의 길이는 하나 줄어들게 된다.\n3. 계속 비교해서 내려가고, 조건을 충족시키면 answer에 계속 더해 return 하면 끝. ","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EC%95%95%EC%B6%95":{"title":"압축","content":"신입사원 어피치는 카카오톡으로 전송되는 메시지를 압축하여 전송 효율을 높이는 업무를 맡게 되었다. 메시지를 압축하더라도 전달되는 정보가 바뀌어서는 안 되므로, 압축 전의 정보를 완벽하게 복원 가능한 무손실 압축 알고리즘을 구현하기로 했다.\n\n어피치는 여러 압축 알고리즘 중에서 성능이 좋고 구현이 간단한 **LZW**(Lempel–Ziv–Welch) 압축을 구현하기로 했다. LZW 압축은 1983년 발표된 알고리즘으로, 이미지 파일 포맷인 GIF 등 다양한 응용에서 사용되었다.\n\nLZW 압축은 다음 과정을 거친다.\n\n1.  길이가 1인 모든 단어를 포함하도록 사전을 초기화한다.\n2.  사전에서 현재 입력과 일치하는 가장 긴 문자열 `w`를 찾는다.\n3.  `w`에 해당하는 사전의 색인 번호를 출력하고, 입력에서 `w`를 제거한다.\n4.  입력에서 처리되지 않은 다음 글자가 남아있다면(`c`), `w+c`에 해당하는 단어를 사전에 등록한다.\n5.  단계 2로 돌아간다.\n\n압축 알고리즘이 영문 대문자만 처리한다고 할 때, 사전은 다음과 같이 초기화된다. 사전의 색인 번호는 정수값으로 주어지며, 1부터 시작한다고 하자.\n\n| 색인 번호 | 1   | 2   | 3   | ... | 24  | 25  | 26  |\n| --------- | --- | --- | --- | --- | --- | --- | --- |\n| 단어          |A     | B    | C    | ...    |X     |Y     |Z     |\n\n예를 들어 입력으로 `KAKAO`가 들어온다고 하자.\n\n1.  현재 사전에는 `KAKAO`의 첫 글자 `K`는 등록되어 있으나, 두 번째 글자까지인 `KA`는 없으므로, 첫 글자 `K`에 해당하는 색인 번호 11을 출력하고, 다음 글자인 `A`를 포함한 `KA`를 사전에 27 번째로 등록한다.\n2.  두 번째 글자 `A`는 사전에 있으나, 세 번째 글자까지인 `AK`는 사전에 없으므로, `A`의 색인 번호 1을 출력하고, `AK`를 사전에 28 번째로 등록한다.\n3.  세 번째 글자에서 시작하는 `KA`가 사전에 있으므로, `KA`에 해당하는 색인 번호 27을 출력하고, 다음 글자 `O`를 포함한 `KAO`를 29 번째로 등록한다.\n4.  마지막으로 처리되지 않은 글자 `O`에 해당하는 색인 번호 15를 출력한다.\n\n| 현재 입력(w) | 다음 글자(c) | 출력 | 사전 추가(w+c) |\n| ------------ | ------------ | ---- | -------------- |\n| K            | A            | 11   | 27: KA         |\n| A            | K            | 1    | 28: AK         |\n| KA           | O            | 27   | 29: KAO        |\n|  O            |              | 15     |                |\n\n이 과정을 거쳐 다섯 글자의 문장 `KAKAO`가 4개의 색인 번호 [11, 1, 27, 15]로 압축된다.\n\n### 입력 형식\n입력으로 영문 대문자로만 이뤄진 문자열 `msg`가 주어진다. `msg`의 길이는 1 글자 이상, 1000 글자 이하이다.\n\n### 출력 형식\n주어진 문자열을 압축한 후의 사전 색인 번호를 배열로 출력하라.\n\n\n## 문제 풀이\n\n```python\nfrom string import ascii_uppercase\n\ndef solution(msg):\n\n    dictionary = {}\n    answer = []\n\n    for index, char in enumerate(list(ascii_uppercase), start = 1):\n        dictionary[char] = index\n\n    start_idx, end_idx = 0, 0\n\n    while True:\n        end_idx += 1\n        if end_idx == len(msg):\n            answer.append(dictionary[msg[start_idx:end_idx]])\n            break\n\n        if msg[start_idx:end_idx + 1] not in dictionary:\n            dictionary[msg[start_idx:end_idx + 1]] = len(dictionary) + 1\n            answer.append(dictionary[msg[start_idx:end_idx]])\n            start_idx = end_idx\n\n    return answer\n```","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EC%98%81%EC%96%B4-%EB%81%9D%EB%A7%90%EC%9E%87%EA%B8%B0":{"title":"영어 끝말잇기","content":"\n1부터 n까지 번호가 붙어있는 n명의 사람이 영어 끝말잇기를 하고 있습니다. 영어 끝말잇기는 다음과 같은 규칙으로 진행됩니다.\n\n1. 1번부터 번호 순서대로 한 사람씩 차례대로 단어를 말합니다.\n2. 마지막 사람이 단어를 말한 다음에는 다시 1번부터 시작합니다.\n3. 앞사람이 말한 단어의 마지막 문자로 시작하는 단어를 말해야 합니다.\n4. 이전에 등장했던 단어는 사용할 수 없습니다.\n5. 한 글자인 단어는 인정되지 않습니다.\n\n다음은 3명이 끝말잇기를 하는 상황을 나타냅니다.\n\ntank → kick → know → wheel → land → dream → mother → robot → tank\n\n위 끝말잇기는 다음과 같이 진행됩니다.\n\n- 1번 사람이 자신의 첫 번째 차례에 tank를 말합니다.\n- 2번 사람이 자신의 첫 번째 차례에 kick을 말합니다.\n- 3번 사람이 자신의 첫 번째 차례에 know를 말합니다.\n- 1번 사람이 자신의 두 번째 차례에 wheel을 말합니다.\n- (계속 진행)\n\n끝말잇기를 계속 진행해 나가다 보면, 3번 사람이 자신의 세 번째 차례에 말한 tank 라는 단어는 이전에 등장했던 단어이므로 탈락하게 됩니다.\n\n사람의 수 n과 사람들이 순서대로 말한 단어 words 가 매개변수로 주어질 때, 가장 먼저 탈락하는 사람의 번호와 그 사람이 자신의 몇 번째 차례에 탈락하는지를 구해서 return 하도록 solution 함수를 완성해주세요.\n\n#### 제한 사항\n\n- 끝말잇기에 참여하는 사람의 수 n은 2 이상 10 이하의 자연수입니다.\n- words는 끝말잇기에 사용한 단어들이 순서대로 들어있는 배열이며, 길이는 n 이상 100 이하입니다.\n- 단어의 길이는 2 이상 50 이하입니다.\n- 모든 단어는 알파벳 소문자로만 이루어져 있습니다.\n- 끝말잇기에 사용되는 단어의 뜻(의미)은 신경 쓰지 않으셔도 됩니다.\n- 정답은 [ 번호, 차례 ] 형태로 return 해주세요.\n- 만약 주어진 단어들로 탈락자가 생기지 않는다면, [0, 0]을 return 해주세요.\n\n----\n\n#### 입출력 예\n\n| **n** | **words**                                                                                                                                                          | **result** |\n| ----- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------- |\n| 3     | [\"tank\", \"kick\", \"know\", \"wheel\", \"land\", \"dream\", \"mother\", \"robot\", \"tank\"]                                                                                      | [3,3]      |\n| 5     | [\"hello\", \"observe\", \"effect\", \"take\", \"either\", \"recognize\", \"encourage\", \"ensure\", \"establish\", \"hang\", \"gather\", \"refer\", \"reference\", \"estimate\", \"executive\"] | [0,0]      |\n| 2     | [\"hello\", \"one\", \"even\", \"never\", \"now\", \"world\", \"draw\"]                                                                                                          | [1,3]      |\n\n#### 입출력 예 설명\n\n입출력 예 #1\n\n3명의 사람이 끝말잇기에 참여하고 있습니다.\n\n- 1번 사람 : tank, wheel, mother\n- 2번 사람 : kick, land, robot\n- 3번 사람 : know, dream, `tank`\n\n와 같은 순서로 말을 하게 되며, 3번 사람이 자신의 세 번째 차례에 말한 `tank`라는 단어가 1번 사람이 자신의 첫 번째 차례에 말한 `tank`와 같으므로 3번 사람이 자신의 세 번째 차례로 말을 할 때 처음 탈락자가 나오게 됩니다.\n\n입출력 예 #2\n\n5명의 사람이 끝말잇기에 참여하고 있습니다.\n\n- 1번 사람 : hello, recognize, gather\n- 2번 사람 : observe, encourage, refer\n- 3번 사람 : effect, ensure, reference\n- 4번 사람 : take, establish, estimate\n- 5번 사람 : either, hang, executive\n\n와 같은 순서로 말을 하게 되며, 이 경우는 주어진 단어로만으로는 탈락자가 발생하지 않습니다. 따라서 [0, 0]을 return하면 됩니다.\n\n입출력 예 #3\n\n2명의 사람이 끝말잇기에 참여하고 있습니다.\n\n- 1번 사람 : hello, even, `now`, draw\n- 2번 사람 : one, never, world\n\n와 같은 순서로 말을 하게 되며, 1번 사람이 자신의 세 번째 차례에 'r'로 시작하는 단어 대신, n으로 시작하는 `now`를 말했기 때문에 이때 처음 탈락자가 나오게 됩니다.\n\n---\n\n## 나의 풀이\n\n```python\ndef solution(n, words):\n    for p in range(1, len(words)):\n        if words[p-1][-1] != words[p][0] or words[p] in words[:p]:\n            return [(p%n) + 1, (p//n) + 1]\n    else:\n        return([0,0])\n```\n\n### 해석\n처음에는 단어와 사람 번호를 같이 딕셔너리에 저장하려고 했다. 그러나 계속 코드가 복잡해지자 생각을 바꾸고 위와 같이 코드를 새로 작성했다. 이 문제의 핵심은 두 가지이다.\n\n- **이전 단어의 마지막 글자와 현재 단어의 첫 글자가 일치하는가**\n- **이전에 나왔던 단어가 다시 나오는가**\n\n처음 것은 인덱스로 간단하게 확인하면 되는데, 두번째 것은 슬라이싱으로 확인하는 방법이 신선한 점이다. 결국 단어 리스트가 주어졌을 때, 현재 단어 이전에 똑같은 단어가 있는지만 확인하면 되는 것이므로. `LV-2` 이지만 짧게 해결할 수 있는 문제였다.","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EC%98%AC%EB%B0%94%EB%A5%B8-%EA%B4%84%ED%98%B8":{"title":"올바른 괄호","content":"\n괄호가 바르게 짝지어졌다는 것은 '(' 문자로 열렸으면 반드시 짝지어서 ')' 문자로 닫혀야 한다는 뜻입니다. 예를 들어\n\n-   \"()()\" 또는 \"(())()\" 는 올바른 괄호입니다.\n-   \")()(\" 또는 \"(()(\" 는 올바르지 않은 괄호입니다.\n\n'(' 또는 ')' 로만 이루어진 문자열 `s`가 주어졌을 때, 문자열 `s`가 올바른 괄호이면 `true`를 `return` 하고, 올바르지 않은 괄호이면 `false`를 `return` 하는 `solution` 함수를 완성해 주세요.\n\n## 제한사항\n\n-   문자열 `s`의 길이 : 100,000 이하의 자연수\n-   문자열 `s`는 '(' 또는 ')' 로만 이루어져 있습니다.\n\n\n## 입출력 예\n| s        | answer |\n| -------- | ------ |\n| \"()()\"   | true   |\n| \"(())()\" | true   |\n| \")()(\"   | false  |\n| \"(()(\"   | false  |\n\n\n## 나의 풀이\n\n```python\ndef solution(s):\n    answer = True\n    flag = 0\n    for n in s :\n        if flag \u003c 0 :\n            return False\n        if n == '(' :\n            flag += 1\n        else :\n            flag -= 1\n    return True if flag == 0 else False\n```\n\n이 문제는 스택을 사용하는 문제이다. `flag` 용도로 쓸 변수를 선언하고 해당 변수를 이용하여 괄호가 올바른지를 체크했다. 만약에 `flag`가 음수가 되는 경우는 `)`가 먼저 나오는 경우라서 바로 올바르지 않기 때문에 `False`를 리턴했고, `(`이 나오는 경우는 `flag`를 증가시키고 `)`이 나오는 경우 `flag`를 감소시켜서 만약에 `0`이 되면 괄호가 열린만큼 닫힌거라 `True`를 리턴했고 `0`보다 크면 `(`가 더 많이 떴다는 뜻으로 `False`를 리턴했다.\n","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EC%A0%84%ED%99%94%EB%B2%88%ED%98%B8-%EB%AA%A9%EB%A1%9D":{"title":"전화번호 목록","content":"전화번호부에 적힌 전화번호 중, 한 번호가 다른 번호의 접두어인 경우가 있는지 확인하려 합니다.  \n전화번호가 다음과 같을 경우, 구조대 전화번호는 영석이의 전화번호의 접두사입니다.\n\n-   구조대 : 119\n-   박준영 : 97 674 223\n-   지영석 : 11 9552 4421\n\n전화번호부에 적힌 전화번호를 담은 배열 `phone_book` 이 solution 함수의 매개변수로 주어질 때, 어떤 번호가 다른 번호의 접두어인 경우가 있으면 false를 그렇지 않으면 true를 return 하도록 solution 함수를 작성해주세요.\n\n### 제한 사항\n\n-   `phone_book의` 길이는 1 이상 1,000,000 이하입니다.\n    -   각 전화번호의 길이는 1 이상 20 이하입니다.\n    -   같은 전화번호가 중복해서 들어있지 않습니다.\n\n### 입출력 예제\n\n| phone_book                        | return |\n| --------------------------------- | ------ |\n| [\"119\", \"97674223\", \"1195524421\"] | false  |\n| [\"123\",\"456\",\"789\"]               | true   |\n| [\"12\",\"123\",\"1235\",\"567\",\"88\"]    | false  |\n\n### 입출력 예 설명\n\n**입출력 예 #1**  \n앞에서 설명한 예와 같습니다.\n\n**입출력 예 #2**  \n한 번호가 다른 번호의 접두사인 경우가 없으므로, 답은 true입니다.\n\n**입출력 예 #3**  \n첫 번째 전화번호, “12”가 두 번째 전화번호 “123”의 접두사입니다. 따라서 답은 false입니다.\n\n## 나의 풀이\n\n```python\ndef solution(phone_book):\n    dic = {}\n\n    for num in phone_book:\n        if num[0] not in dic:\n            dic[num[0]] = [num]\n        else:\n            dic[num[0]].append(num)\n\n    for i in dic:\n        dic[i].sort()\n\n    for i in dic:\n        if len(dic[i]) == 1:\n            continue\n        else:\n            start = dic[i][0]\n            area = len(start)\n            for j in range(1, len(dic[i])):\n                if start in dic[i][j][:area]:\n                    return False\n                else:\n                    start = dic[i][j]\n                    area = len(start)\n    return True\n```\n\n\u003e[!note] Note  \n\u003e  \n\u003e이 문제는 `hash`자료구조, 즉 파이썬 dictionary 자료구조를 이용해 풀 수 있다. 주어진 phone_book에서 각 번호의 첫째 글자를 hash에 key로 저장하고, 그 첫째 글자로 시작하는 모든 숫자들을 value에 리스트로 저장한다. \n\u003e그리고 각 리스트를 정렬한 후, 인접한 것들끼리 비교하는 방식으로 문제를 해결했다. 이럴 경우 시간 복잡도는 $O(n * log(n))$가 된다.\n\n## 좋은 풀이\n\n```python\ndef solution(phoneBook):\n    phoneBook = sorted(phoneBook)\n\n    for p1, p2 in zip(phoneBook, phoneBook[1:]):\n        if p2.startswith(p1):\n            return False\n    return True\n```\n\u003e[!note] Note  \n\u003e  \n\u003e이 코드는 이전 코드의 보다 간결하고 효율적인 구현이다. 먼저 내장된 정렬 기능을 사용하여 전화 번호의 입력 목록을 정렬한다. 그런 다음 `zip` 을 사용하여 각 전화 번호를 목록의 다음 전화 번호와 페어링하고 for 루프를 사용하여 이러한 쌍을 확인하는 작업을 반복한다. \n\u003e각 전화 번호 쌍에 대해 이 코드는 `starts with` 메서드를 사용하여 두 번째 전화 번호가 첫 번째 전화 번호의 접두사인지 확인한다. 한 전화 번호가 다른 전화 번호의 접두사임을 발견하면 즉시 False를 반환한다. 접두사를 찾지 않고 모든 쌍을 반복하는 것을 마치면 True를 반환한다.\n\u003e이 코드는 이전 코드의 해당 루프보다 효율적인 내장 정렬(`sorted`) 및 `zip` 함수를 사용하기 때문에 이전 구현보다 효율적입니다. 또한, 이전 코드에는 세 개의 별도 루프가 있는 반면, 여기서는 전화 번호 목록을 한 번만 루프하면 됩니다. 따라서 이 코드의 시간 복잡도는 $O(n * log(n))$이며, 여기서 n은 입력 목록에 있는 전화 번호의 수이고, 이는 이전 코드와 동일하지만 상수 계수가 더 낮다. ","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EC%A0%95%EC%88%98-%EC%82%BC%EA%B0%81%ED%98%95":{"title":"정수 삼각형","content":"\n![스크린샷 2018-09-14 오후 5.44.19.png](https://grepp-programmers.s3.amazonaws.com/files/production/97ec02cc39/296a0863-a418-431d-9e8c-e57f7a9722ac.png)\n\n위와 같은 삼각형의 꼭대기에서 바닥까지 이어지는 경로 중, 거쳐간 숫자의 합이 가장 큰 경우를 찾아보려고 합니다. 아래 칸으로 이동할 때는 대각선 방향으로 한 칸 오른쪽 또는 왼쪽으로만 이동 가능합니다. 예를 들어 3에서는 그 아래칸의 8 또는 1로만 이동이 가능합니다.\n\n삼각형의 정보가 담긴 배열 `triangle`이 매개변수로 주어질 때, 거쳐간 숫자의 최댓값을 `return` 하도록 `solution` 함수를 완성하세요.\n\n### 제한사항\n\n-   삼각형의 높이는 1 이상 500 이하입니다.\n-   삼각형을 이루고 있는 숫자는 0 이상 9,999 이하의 정수입니다.\n\n### 입출력 예\n\n| triangle                                                  | result |\n| --------------------------------------------------------- | ------ |\n| [\\[7], [3, 8], [8, 1, 0], [2, 7, 4, 4], [4, 5, 2, 6, 5]\\] | 30     |\n\n## 나의 오답\n\n```python\ndef solution(triangle):\n    curr_answer = 0\n    k = 0\n    for i in range(len(triangle)-1, 1, -1):\n        if k not in [0, len(triangle[i])-1]:\n            triangle[i-1][k-1] += triangle[i][k]\n            triangle[i-1][k] += triangle[i][k]\n\n        elif k == 0:\n            triangle[i-1][k] += triangle[i][k]\n        else:\n            triangle[i-1][k-1] += triangle[i][k]\n        k += 1\n        if k == len(triangle[i])-1:\n            k = 0\n    left = triangle[0][0] + triangle[1][0]\n    right = triangle[0][0] + triangle[1][1]\n    prev_answer = max(left, right)\n    if prev_answer \u003e curr_answer:\n        curr_answer = prev_answer\n    \n    return curr_answer\n```\n\n이건 오답이다. \n아래의 정답과 비교하자. \n\n## 다른 사람의 풀이\n\n```python\ndef solution(triangle):\n\n    height = len(triangle)\n\n    while height \u003e 1:\n        for i in range(height - 1):\n            triangle[height-2][i] += max([triangle[height-1][i], triangle[height-1][i+1]])\n        height -= 1\n\n    answer = triangle[0][0]\n    return answer\n```\n\n나도 이렇게 **삼각형을 거꾸로 시작하는 방법**을 생각했는데...\n`for i in range(len(triangle)-1, 1, -1):` 이런 걸로 하나하나 내려갈 필요 없이, `while height \u003e 1:`으로 한 다음, `height -= 1`으로 내려가면 되었구나. 괜히 복잡하게 생각했다. \n\n그리고 따로 [`DP 테이블`](notes/algorithms/Dynamic%20Programming.md) 을 사용할 필요 없이, 아래 칸의 값을 현재 칸의 좌 우의 값의 합으로 저장해버리면 된다. 나는 여기에서 뺄셈을 사용하는 바람에 조건문이 더 추가되어 버렸는데, 그냥 `i + 1`로 해버려도 된다. \n","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EC%A3%BC%EC%B0%A8-%EC%9A%94%EA%B8%88-%EA%B3%84%EC%82%B0":{"title":"주차 요금 계산","content":"\n주차장의 요금표와 차량이 들어오고(입차) 나간(출차) 기록이 주어졌을 때, 차량별로 주차 요금을 계산하려고 합니다. 아래는 하나의 예시를 나타냅니다.\n\n-   **요금표**\n\n| 기본 시간(분) | 기본 요금(원) | 단위 시간(분) | 단위 요금(원) |\n| ------------- | ------------- | ------------- | ------------- |\n| 180              |         5000      |10               |            600   |\n\n-   **입/출차 기록**\n\n| 시각(시:분) | 차량 번호 | 내역 |\n| ----------- | --------- | ---- |\n| 05:34       | 5961      | 입차 |\n| 06:00       | 0000      | 입차 |\n| 06:34       | 0000      | 출차 |\n| 07:59       | 5961      | 출차 |\n| 07:59       | 0148      | 입차 |\n| 18:59       | 0000      | 입차 |\n| 19:09       | 0148      | 출차 |\n| 22:59       | 5961      | 입차 |\n| 23:00            |        5961   |    출차  |\n\n-   **자동차별 주차 요금**\n\n| 차량 번호 | 누적 주차 시간(분) | 주차 요금(원)                               |\n| --------- | ------------------ | ------------------------------------------- |\n| 0000      | 34 + 300 = 334     | 5000 + `⌈`(334 - 180) / 10`⌉` x 600 = 14600 |\n| 0148      | 670                | 5000 +`⌈`(670 - 180) / 10`⌉`x 600 = 34400   |\n| 5961      | 145 + 1 = 146      | 5000                                        |\n\n-   어떤 차량이 입차된 후에 출차된 내역이 없다면, 23:59에 출차된 것으로 간주합니다.\n    -   `0000`번 차량은 18:59에 입차된 이후, 출차된 내역이 없습니다. 따라서, 23:59에 출차된 것으로 간주합니다.\n-   00:00부터 23:59까지의 입/출차 내역을 바탕으로 차량별 누적 주차 시간을 계산하여 요금을 일괄로 정산합니다. \n-   누적 주차 시간이 `기본 시간`이하라면, `기본 요금`을 청구합니다.  \n-   누적 주차 시간이 `기본 시간`을 초과하면, `기본 요금`에 더해서, 초과한 시간에 대해서 `단위 시간` 마다 `단위 요금`을 청구합니다.\n    -   초과한 시간이 `단위 시간`으로 나누어 떨어지지 않으면, `올림`합니다.\n    -   `⌈`a`⌉` : a보다 작지 않은 최소의 정수를 의미합니다. 즉, `올림`을 의미합니다.\n\n주차 요금을 나타내는 정수 배열 `fees`, 자동차의 입/출차 내역을 나타내는 문자열 배열 `records`가 매개변수로 주어집니다. **차량 번호가 작은 자동차부터** 청구할 주차 요금을 차례대로 정수 배열에 담아서 return 하도록 solution 함수를 완성해주세요.\n\n### 제한사항\n\n-   `fees`의 길이 = 4\n    -   fees[0] = `기본 시간(분)`\n    -   1 ≤ fees[0] ≤ 1,439 \n    -   fees[1] = `기본 요금(원)`\n    -   0 ≤ fees[1] ≤ 100,000\n    -   fees[2] = `단위 시간(분)`\n    -   1 ≤ fees[2] ≤ 1,439\n    -   fees[3] = `단위 요금(원)`\n    -   1 ≤ fees[3] ≤ 10,000\n\n- 1 ≤ `records`의 길이 ≤ 1,000\n    -   `records`의 각 원소는 `\"시각 차량번호 내역\"` 형식의 문자열입니다.\n    -   `시각`, `차량번호`, `내역`은 하나의 공백으로 구분되어 있습니다.\n    -   `시각`은 차량이 입차되거나 출차된 시각을 나타내며, `HH:MM` 형식의 길이 5인 문자열입니다.\n        -   `HH:MM`은 00:00부터 23:59까지 주어집니다.\n        -   잘못된 시각(\"25:22\", \"09:65\" 등)은 입력으로 주어지지 않습니다.\n    -   `차량번호`는 자동차를 구분하기 위한, `0'~'9'로 구성된 길이 4인 문자열입니다.  \n    -   `내역`은 길이 2 또는 3인 문자열로, `IN` 또는 `OUT`입니다. `IN`은 입차를, `OUT`은 출차를 의미합니다. \n    -   `records`의 원소들은 시각을 기준으로 오름차순으로 정렬되어 주어집니다.\n    -   `records`는 하루 동안의 입/출차된 기록만 담고 있으며, 입차된 차량이 다음날 출차되는 경우는 입력으로 주어지지 않습니다.\n    -   같은 시각에, 같은 차량번호의 내역이 2번 이상 나타내지 않습니다.\n    -   마지막 시각(23:59)에 입차되는 경우는 입력으로 주어지지 않습니다.\n    -   아래의 예를 포함하여, 잘못된 입력은 주어지지 않습니다.\n        -   주차장에 없는 차량이 출차되는 경우\n        -   주차장에 이미 있는 차량(차량번호가 같은 차량)이 다시 입차되는 경우\n\n---\n\n### 입출력 예\n\n| fees                 | records                                                                                                                                                         | result               |\n| -------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------- |\n| [180, 5000, 10, 600] | `[\"05:34 5961 IN\", \"06:00 0000 IN\", \"06:34 0000 OUT\", \"07:59 5961 OUT\", \"07:59 0148 IN\", \"18:59 0000 IN\", \"19:09 0148 OUT\", \"22:59 5961 IN\", \"23:00 5961 OUT\"]` | [14600, 34400, 5000] |\n| [120, 0, 60, 591]    | `[\"16:00 3961 IN\",\"16:00 0202 IN\",\"18:00 3961 OUT\",\"18:00 0202 OUT\",\"23:58 3961 IN\"]`                                                                           | [0, 591]             |\n| [1, 461, 1, 10]      | `[\"00:00 1234 IN\"]`                                                                                                                                             | [14841]              |\n\n## 나의 풀이\n```python\nimport math\nimport datetime\n\ndef solution(fees, records):\n    answer = []\n    ht = {}\n    for i in records:\n        temp = i.split()\n        if temp[1] not in ht:\n            ht[temp[1]] = [temp[0]]\n        else:\n            ht[temp[1]].append(temp[0])\n\n    for i in ht:\n        if len(ht[i]) % 2 == 1:\n            ht[i].append(\"23:59\")\n\n    temp = sorted([i for i in ht])\n    new_ht = {}\n    for i in temp:\n        new_ht[i] = ht[i]\n\n    for i in new_ht:\n        total_fees = 0\n        total_minutes = 0\n\n        for j in range(len(new_ht[i])//2):\n\n            out_time = new_ht[i][2*j + 1].split(':')\n            out_time = datetime.timedelta(\n            hours = int(out_time[0]), \n            minutes = int(out_time[1]))\n\n            in_time = new_ht[i][2*j].split(':')\n            in_time = datetime.timedelta(\n            hours = int(in_time[0]), \n            minutes = int(in_time[1]))\n\n            minutes = ((out_time - in_time).seconds)/60\n            total_minutes += minutes\n\n        total_fees = fees[0] - total_minutes\n\n        if total_fees \u003e= 0:\n            answer.append(fees[1])\n        else:\n            answer.append(fees[1] \n            + math.ceil(\n            (abs(total_fees) / fees[2])\n            ) * fees[3])\n\n    return answer\n```\n\n\u003e[!note] Note  \n\u003e  \n\u003e이 문제는 재미있다. 풀이는 크게 세 부분으로 구성된다.\n\u003e1. `records` 리스트를 받아서 **차 번호를 key로, 입출 시간들을 value로  갖는 `ht` 딕셔너리**에 저장한다. \n\u003e2. (출제자가 일부로 꼰 것 같지만) 차 번호가 낮은 순서대로 출력하기 위해서 `ht`를 정렬한 `new_ht`를 만든다. 이후 new_ht에서 각 차 번호(`key`)를 순회하며 입출 시간을 분 단위로 계산하여 저장한다. \n\u003e3. 누적된 주차 시간(`total_minutes`)을 기본 시간과 비교하며(`total_fees`) 기본 시간보다 더 클 경우, 단위 시간마다 단위 요금을 더해  `answer`에 저장해준다. ","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/%EC%A7%9D%EC%A7%80%EC%96%B4-%EC%A0%9C%EA%B1%B0%ED%95%98%EA%B8%B0":{"title":"짝지어 제거하기","content":"\n짝지어 제거하기는, 알파벳 소문자로 이루어진 문자열을 가지고 시작합니다. 먼저 문자열에서 같은 알파벳이 2개 붙어 있는 짝을 찾습니다. 그다음, 그 둘을 제거한 뒤, 앞뒤로 문자열을 이어 붙입니다. 이 과정을 반복해서 문자열을 모두 제거한다면 짝지어 제거하기가 종료됩니다. 문자열 S가 주어졌을 때, 짝지어 제거하기를 성공적으로 수행할 수 있는지 반환하는 함수를 완성해 주세요. 성공적으로 수행할 수 있으면 1을, 아닐 경우 0을 리턴해주면 됩니다.\n\n예를 들어, 문자열 S = `baabaa` 라면\n\nb _aa_ baa → _bb_ aa → _aa_ →\n\n의 순서로 문자열을 모두 제거할 수 있으므로 1을 반환합니다.\n\n### 제한사항\n\n-   문자열의 길이 : 1,000,000이하의 자연수\n-   문자열은 모두 소문자로 이루어져 있습니다.\n\n---\n\n### 입출력 예\n\n| s      | result |\n| :------ | :------ |\n| baabaa | 1      |\n| cdcd       |    0    |\n\n### 입출력 예 설명\n\n**입출력 예 #1**  \n위의 예시와 같습니다.  \n\n**입출력 예 #2**  \n문자열이 남아있지만 짝지어 제거할 수 있는 문자열이 더 이상 존재하지 않기 때문에 0을 반환합니다.\n\n## 나의 풀이\n\n```python\ndef solution(s):\n    s = list(s)\n    stack = [s[0]]\n\n    for i in range(1, len(s)):\n        stack.append(s[i])\n        if len(stack) \u003e 1:\n            if stack[-1] == stack[-2]:\n                stack.pop()\n                stack.pop()\n\n    if stack:\n        return 0\n    else:\n        return 1\n```\n\n\u003e [!note] Note  \n\u003e  \n\u003e 이 문제는 스택 자료구조를 이용해서 쉽게 풀 수 있는 문제이다. 처음에는 재귀함수로 풀려고 했었지만, 문자열의 길이가 길어지면서 시간 복잡도도 같이 증가하는 문제가 있었다. 이에 스택에 하나하나 추가하고 터뜨리면서(?) 푸니 $O(N)$의 시간복잡도로 풀 수 있었다.\n","lastmodified":"2023-03-14T08:55:46.467505504Z","tags":null},"/notes/coding-test/%EC%BA%90%EC%8B%9C":{"title":"캐시","content":"\n지도개발팀에서 근무하는 제이지는 지도에서 도시 이름을 검색하면 해당 도시와 관련된 맛집 게시물들을 데이터베이스에서 읽어 보여주는 서비스를 개발하고 있다.  \n이 프로그램의 테스팅 업무를 담당하고 있는 어피치는 서비스를 오픈하기 전 각 로직에 대한 성능 측정을 수행하였는데, 제이지가 작성한 부분 중 데이터베이스에서 게시물을 가져오는 부분의 실행시간이 너무 오래 걸린다는 것을 알게 되었다.  \n어피치는 제이지에게 해당 로직을 개선하라고 닦달하기 시작하였고, 제이지는 DB 캐시를 적용하여 성능 개선을 시도하고 있지만 캐시 크기를 얼마로 해야 효율적인지 몰라 난감한 상황이다.\n\n어피치에게 시달리는 제이지를 도와, DB 캐시를 적용할 때 캐시 크기에 따른 실행시간 측정 프로그램을 작성하시오.\n\n### 입력 형식\n\n-   캐시 크기(`cacheSize`)와 도시이름 배열(`cities`)을 입력받는다.\n-   `cacheSize`는 정수이며, 범위는 0 ≦ `cacheSize` ≦ 30 이다.\n-   `cities`는 도시 이름으로 이뤄진 문자열 배열로, 최대 도시 수는 100,000개이다.\n-   각 도시 이름은 공백, 숫자, 특수문자 등이 없는 영문자로 구성되며, 대소문자 구분을 하지 않는다. 도시 이름은 최대 20자로 이루어져 있다.\n\n### 출력 형식\n\n-   입력된 도시이름 배열을 순서대로 처리할 때, \"총 실행시간\"을 출력한다.\n\n### 조건\n\n-   캐시 교체 알고리즘은 `LRU`(Least Recently Used)를 사용한다.\n-   `cache hit`일 경우 실행시간은 `1`이다.\n-   `cache miss`일 경우 실행시간은 `5`이다.\n\n### 입출력 예제\n\n| 캐시크기(cacheSize) | 도시이름(cities)                                                                                                  | 실행시간 |\n| ------------------- | ----------------------------------------------------------------------------------------------------------------- | -------- |\n| 3                   | [\"Jeju\", \"Pangyo\", \"Seoul\", \"NewYork\", \"LA\", \"Jeju\", \"Pangyo\", \"Seoul\", \"NewYork\", \"LA\"]                          | 50       |\n| 3                   | [\"Jeju\", \"Pangyo\", \"Seoul\", \"Jeju\", \"Pangyo\", \"Seoul\", \"Jeju\", \"Pangyo\", \"Seoul\"]                                 | 21       |\n| 2                   | [\"Jeju\", \"Pangyo\", \"Seoul\", \"NewYork\", \"LA\", \"SanFrancisco\", \"Seoul\", \"Rome\", \"Paris\", \"Jeju\", \"NewYork\", \"Rome\"] | 60       |\n| 5                   | [\"Jeju\", \"Pangyo\", \"Seoul\", \"NewYork\", \"LA\", \"SanFrancisco\", \"Seoul\", \"Rome\", \"Paris\", \"Jeju\", \"NewYork\", \"Rome\"] | 52       |\n| 2                   | [\"Jeju\", \"Pangyo\", \"NewYork\", \"newyork\"]                                                                          | 16       |\n| 0                   | [\"Jeju\", \"Pangyo\", \"Seoul\", \"NewYork\", \"LA\"]                                                                      | 25       |\n\n## 문제 풀이\n\n```python\nfrom collections import deque\n\ndef solution(cacheSize, cities):\n    answer = 0\n    lru = deque([])\n\n    for city in cities:\n\n        city = city.lower()\n\n        if city not in lru:\n            if cacheSize == 0:\n                answer += 5\n                continue\n            if len(lru) \u003c cacheSize:\n                lru.append(city)\n            else:\n                if len(lru) != 0:\n                    lru.popleft()\n                lru.append(city)\n            answer += 5\n        else:\n            lru.remove(city)\n            lru.append(city)\n            answer +=1\n\n    return answer\n```","lastmodified":"2023-03-14T08:55:46.467505504Z","tags":null},"/notes/coding-test/%ED%81%B0-%EC%88%98-%EB%A7%8C%EB%93%A4%EA%B8%B0":{"title":"큰 수 만들기","content":"어떤 숫자에서 k개의 수를 제거했을 때 얻을 수 있는 가장 큰 숫자를 구하려 합니다.\n\n예를 들어, 숫자 1924에서 수 두 개를 제거하면 [19, 12, 14, 92, 94, 24] 를 만들 수 있습니다. 이 중 가장 큰 숫자는 94 입니다.\n\n문자열 형식으로 숫자 number와 제거할 수의 개수 k가 solution 함수의 매개변수로 주어집니다. number에서 k 개의 수를 제거했을 때 만들 수 있는 수 중 가장 큰 숫자를 문자열 형태로 return 하도록 solution 함수를 완성하세요.\n\n### 제한 조건\n\n-   number는 2자리 이상, 1,000,000자리 이하인 숫자입니다.\n-   k는 1 이상 `number의 자릿수` 미만인 자연수입니다.\n\n### 입출력 예\n\n| number       | k   | return   |\n| ------------ | --- | -------- |\n| \"1924\"       | 2   | \"94\"     |\n| \"1231234\"    | 3   | \"3234\"   |\n| \"4177252841\" | 4   | \"775841\" |\n\n## 나의 풀이\n\n```python\ndef solution(number, k):\n    stack = []\n    for n in number:\n        while stack and stack[-1] \u003c n and k \u003e 0:\n            stack.pop()\n            k -= 1\n        stack.append(n)\n\n    if k \u003e 0:\n        stack = stack[:-k]\n\n    return ''.join(stack)\n```\n\n\u003e [!note] Note  \n\u003e  \n\u003e이 문제도 `stack` 자료형을 이용한 `greedy` 문제이다. 이 경우, 시간복잡도는 $O(N)$이 된다. ","lastmodified":"2023-03-14T08:55:46.467505504Z","tags":null},"/notes/coding-test/%ED%82%A4%ED%8C%A8%EB%93%9C-%EB%88%84%EB%A5%B4%EA%B8%B0":{"title":"키패드 누르기","content":"\n스마트폰 전화 키패드의 각 칸에 다음과 같이 숫자들이 적혀 있습니다.\n\n![kakao_phone1.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/4b69a271-5f4a-4bf4-9ebf-6ebed5a02d8d/kakao_phone1.png)\n\n이 전화 키패드에서 왼손과 오른손의 엄지손가락만을 이용해서 숫자만을 입력하려고 합니다.  \n맨 처음 왼손 엄지손가락은 `*` 키패드에 오른손 엄지손가락은 `#` 키패드 위치에서 시작하며, 엄지손가락을 사용하는 규칙은 다음과 같습니다.\n\n1.  엄지손가락은 상하좌우 4가지 방향으로만 이동할 수 있으며 키패드 이동 한 칸은 거리로 1에 해당합니다.\n2.  왼쪽 열의 3개의 숫자 `1`, `4`, `7`을 입력할 때는 왼손 엄지손가락을 사용합니다.\n3.  오른쪽 열의 3개의 숫자 `3`, `6`, `9`를 입력할 때는 오른손 엄지손가락을 사용합니다.\n4.  가운데 열의 4개의 숫자 `2`, `5`, `8`, `0`을 입력할 때는 두 엄지손가락의 현재 키패드의 위치에서 더 가까운 엄지손가락을 사용합니다.  \n    4-1. 만약 두 엄지손가락의 거리가 같다면, 오른손잡이는 오른손 엄지손가락, 왼손잡이는 왼손 엄지손가락을 사용합니다.\n\n순서대로 누를 번호가 담긴 배열 numbers, 왼손잡이인지 오른손잡이인 지를 나타내는 문자열 hand가 매개변수로 주어질 때, 각 번호를 누른 엄지손가락이 왼손인 지 오른손인 지를 나타내는 연속된 문자열 형태로 return 하도록 solution 함수를 완성해주세요.\n\n## 제한사항\n\n-   numbers 배열의 크기는 1 이상 1,000 이하입니다.\n-   numbers 배열 원소의 값은 0 이상 9 이하인 정수입니다.\n-   hand는 `\"left\"` 또는 `\"right\"` 입니다.\n    -   `\"left\"`는 왼손잡이, `\"right\"`는 오른손잡이를 의미합니다.\n-   왼손 엄지손가락을 사용한 경우는 `L`, 오른손 엄지손가락을 사용한 경우는 `R`을 순서대로 이어붙여 문자열 형태로 return 해주세요.\n\n## 입출력 예\n\n| numbers                           | hand      | result          |\n| --------------------------------- | --------- | --------------- |\n| [1, 3, 4, 5, 8, 2, 1, 4, 5, 9, 5] | `\"right\"` | `\"LRLLLRLLRRL\"` |\n| [7, 0, 8, 2, 8, 3, 1, 5, 7, 6, 2] | `\"left\"`  | `\"LRLLRRLLLRR\"` |\n| [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]    | `\"right\"` | `\"LLRLLRLLRL\"`  |\n\n## 나의 풀이\n\n```python\ndef solution(numbers, hand):\n    pad = {\n        1:(0,0), 2:(0,1), 3:(0,2),\n        4:(1,0), 5:(1,1), 6:(1,2),\n        7:(2,0), 8:(2,1), 9:(2,2),\n        '*':(3,0), 0:(3,1), '#':(3,2)\n    }\n    L_prev_key = pad['*']\n    R_prev_key = pad['#']\n    answer = ''\n\n    for key in numbers:\n        if key in [1,4,7]:\n            answer += 'L'\n            L_prev_key = pad[key]\n        elif key in [3,6,9]:\n            answer += 'R'\n            R_prev_key = pad[key]\n        else:\n            L_distance = abs(L_prev_key[0] - pad[key][0]) + abs(L_prev_key[1] - pad[key][1])\n            R_distance = abs(R_prev_key[0] - pad[key][0]) + abs(R_prev_key[1] - pad[key][1])\n            if L_distance \u003c R_distance:\n                answer += 'L'\n                L_prev_key = pad[key]\n            elif L_distance \u003e R_distance:\n                answer += 'R'\n                R_prev_key = pad[key]\n            else:\n                if hand == 'right':\n                    answer += 'R'\n                    R_prev_key = pad[key]\n                else:\n                    answer += 'L'\n                    L_prev_key = pad[key]\n    return answer\n```\n\n이 문제는 좌표가 필요한 문제이다. 좌표 튜플을 각 번호에 해당하는 딕셔너리에 저장하고 시작하면 된다. ","lastmodified":"2023-03-14T08:55:46.467505504Z","tags":null},"/notes/coding-test/%ED%83%80%EA%B2%9F-%EB%84%98%EB%B2%84":{"title":"타겟 넘버","content":"\nn개의 음이 아닌 정수들이 있습니다. 이 정수들을 순서를 바꾸지 않고 적절히 더하거나 빼서 타겟 넘버를 만들려고 합니다. 예를 들어 [1, 1, 1, 1, 1]로 숫자 3을 만들려면 다음 다섯 방법을 쓸 수 있습니다.\n\n```\n-1+1+1+1+1 = 3\n+1-1+1+1+1 = 3\n+1+1-1+1+1 = 3\n+1+1+1-1+1 = 3\n+1+1+1+1-1 = 3\n```\n\n사용할 수 있는 숫자가 담긴 배열 numbers, 타겟 넘버 target이 매개변수로 주어질 때 숫자를 적절히 더하고 빼서 타겟 넘버를 만드는 방법의 수를 return 하도록 solution 함수를 작성해주세요.\n\n### 제한사항\n\n-   주어지는 숫자의 개수는 2개 이상 20개 이하입니다.\n-   각 숫자는 1 이상 50 이하인 자연수입니다.\n-   타겟 넘버는 1 이상 1000 이하인 자연수입니다.\n\n### 입출력 예\n\n| numbers         | target | return |\n| --------------- | ------ | ------ |\n| [1, 1, 1, 1, 1] | 3      | 5      |\n| [4, 1, 2, 1]                |      4  |  2      |\n\n### 입출력 예 설명\n\n**입출력 예 #1**\n\n문제 예시와 같습니다.\n\n**입출력 예 #2**\n\n```\n+4+1-2+1 = 4\n+4-1+2-1 = 4\n```\n\n-   총 2가지 방법이 있으므로, 2를 return 합니다.\n\n---\n## 다른 사람 풀이\n\n```python\nfrom collections import deque\n\ndef solution(numbers, target):\n    answer = 0\n    queue = deque([(0, 0)])\n    while queue:\n        current_sum, num_idx = queue.popleft()\n\n        if num_idx == len(numbers):\n            if current_sum == target:\n                answer += 1\n        else:\n            number = numbers[num_idx]\n            queue.append((current_sum + number, \n            num_idx + 1))\n            queue.append((current_sum - number, \n            num_idx + 1))\n\n    return answer\n```\n\n이 풀이의 작동 원리는 [이 링크](https://pythontutor.com/render.html#code=from%20collections%20import%20deque%0A%0Adef%20solution%28numbers,%20target%29%3A%0A%20%20%20%20answer%20%3D%200%0A%20%20%20%20queue%20%3D%20deque%28%5B%280,%200%29%5D%29%0A%20%20%20%20while%20queue%3A%0A%20%20%20%20%20%20%20%20current_sum,%20num_idx%20%3D%20queue.popleft%28%29%0A%0A%20%20%20%20%20%20%20%20if%20num_idx%20%3D%3D%20len%28numbers%29%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20current_sum%20%3D%3D%20target%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20answer%20%2B%3D%201%0A%20%20%20%20%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20number%20%3D%20numbers%5Bnum_idx%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20queue.append%28%28current_sum%20%2B%20number,%20%0A%20%20%20%20%20%20%20%20%20%20%20%20num_idx%20%2B%201%29%29%0A%20%20%20%20%20%20%20%20%20%20%20%20queue.append%28%28current_sum%20-%20number,%20%0A%20%20%20%20%20%20%20%20%20%20%20%20num_idx%20%2B%201%29%29%0A%0A%20%20%20%20return%20answer%0A%0Anumbers%20%3D%20%5B4,1,2,1%5D%0Atarget%20%3D%204%0A%0Asolution%28numbers,%20target%29\u0026cumulative=false\u0026curInstr=196\u0026heapPrimitives=nevernest\u0026mode=display\u0026origin=opt-frontend.js\u0026py=3\u0026rawInputLstJSON=%5B%5D\u0026textReferences=false) 로 보면 편하게 이해할 수 있다. 이 코드는 breadth-first search([BFS](notes/algorithms/DFS,%20BFS.md)) 알고리즘을 사용한다. BFS는 그래프 수준별로 탐색하는 그래프 탐색 알고리즘의 일종으로, 루트 노드에서 시작하여 다음 수준으로 진행하기 전에 각 수준에서 모든 노드를 확장한다.\n\n이 코드의 시간 복잡도은 그래프의 노드 수와 그래프의 분기 계수에 따라 달라지게 된다. 이 경우 그래프는 숫자 시퀀스로 표시되며 분기 계수는 2가 된다(각 단계에서 대기열에 두 개의 새 노드를 추가하기 때문에). 따라서 이 코드의 시간 복잡도는 $O(2^n)$이며, 여기서 n은 숫자 배열의 길이이다.\n\n## 재귀함수를 써서 풀 수도 있다.\n\n```python\ndef target_number(numbers, target):\n    # Initialize a counter for the number of ways\n    ways = 0\n\n    # Recursively try all possible combinations of numbers\n    # and check if they add up to the target number\n    def find_ways(numbers, index, cur_sum, target):\n        nonlocal ways\n\n        # If we have reached the end of the array,\n        # check if the current sum is equal to the target\n        if index == len(numbers):\n            if cur_sum == target:\n                ways += 1\n            return\n\n        # Try adding the current number to the sum\n        find_ways(numbers, index + 1, cur_sum + numbers[index], target)\n\n        # Try subtracting the current number from the sum\n        find_ways(numbers, index + 1, cur_sum - numbers[index], target)\n\n    # Start the recursive search from the first number in the array\n    find_ways(numbers, 0, 0, target)\n\n    # Return the number of ways\n    return ways\n\n```\n\nThis solution uses a recursive function `find_ways` that tries all possible combinations of numbers in the array and checks if their sum is equal to the target number. At each step, the function has two options: either add the current number to the sum, or subtract it. When the end of the array is reached, the current sum is compared to the target number, and if they are equal, the counter `ways` is incremented.\n\n이 솔루션은 배열에서 가능한 모든 숫자 조합을 시도하고 합이 목표 숫자와 동일한지 확인하는 재귀 함수 `find_ways`를 사용합니다. 각 단계에서 함수에는 두 가지 옵션이 있습니다. 즉, 현재 숫자를 합계에 추가하거나 숫자를 빼거나 둘 중 하나입니다. 배열의 끝에 도달하면 현재 합계가 목표 숫자와 비교되고, 둘이 같으면 `ways` 가 증가합니다.\n\n```python\ndef solution(numbers, target): \n\tdef recursive_helper(current_sum, num_idx): \n\t\tif num_idx == len(numbers):\n\t\t\treturn 1 if current_sum == target else 0 \n\t\telse: \n\t\t\tnumber = numbers[num_idx] \n\t\t\treturn recursive_helper(current_sum + number, num_idx + 1) + recursive_helper(current_sum - number, num_idx + 1)\n\n\treturn recursive_helper(0, 0)\n```\n\n위 방법이 더 명료한 듯 싶다.","lastmodified":"2023-03-14T08:55:46.467505504Z","tags":null},"/notes/coding-test/H-index":{"title":"H-index","content":"H-Index는 과학자의 생산성과 영향력을 나타내는 지표입니다. 어느 과학자의 H-Index를 나타내는 값인 h를 구하려고 합니다. 위키백과[1](https://school.programmers.co.kr/learn/courses/30/lessons/42747/solution_groups?language=python3\u0026type=my#fn1)에 따르면, H-Index는 다음과 같이 구합니다.\n\n어떤 과학자가 발표한 논문 `n`편 중, `h`번 이상 인용된 논문이 `h`편 이상이고 나머지 논문이 h번 이하 인용되었다면 `h`의 최댓값이 이 과학자의 H-Index입니다.\n\n어떤 과학자가 발표한 논문의 인용 횟수를 담은 배열 `citations가` 매개변수로 주어질 때, 이 과학자의 H-Index를 `return` 하도록 `solution` 함수를 작성해주세요.\n\n### 제한사항\n\n-   과학자가 발표한 논문의 수는 1편 이상 1,000편 이하입니다.\n-   논문별 인용 횟수는 0회 이상 10,000회 이하입니다.\n\n### 입출력 예\n\n| citations | return |\n| --------- | ------ |\n| [3, 0, 6, 1, 5]          |     3   |\n\n### 입출력 예 설명\n\n이 과학자가 발표한 논문의 수는 5편이고, 그중 3편의 논문은 3회 이상 인용되었습니다. 그리고 나머지 2편의 논문은 3회 이하 인용되었기 때문에 이 과학자의 H-Index는 3입니다.\n\n## 나의 풀이\n\n```python\ndef solution(citations):\n\n    citations.sort()\n\n    h = [len(citations[x:])-1 for x in range(len(citations)) if len(citations[x:]) \u003e= citations[x]]\n    if len(h) != 0:\n       return h[-1]\n    else:\n        return len(citations)\n```\n\n\u003e[!warning] Warning  \n\u003e   \n\u003e이 문제에서 h는 꼭 citations 안에 있지 않다! \n\u003e예를 들어, citations = [6, 5, 5, 5, 3, 2, 1, 0] 이면, h = 4이다.\n\n문제 정독과 테스트 케이스의 중요함을 느끼십시오...\nh가 반드시 주어지는 줄 알고 한참을 헤맸다. 이러면 실전에서는 완전 나가리다. \n솔직히 문제가 헷갈리도록 나왔다고 생각하는데, \n그것과는 별개로 문제를 잘 읽자. \n그리고 반례를 항상 잘 떠올리자. \n\n## 다른 사람의 코드\n\n```python\ndef solution(citations):\n    citations = sorted(citations)\n    l = len(citations)\n    for i in range(l):\n        if citations[i] \u003e= l-i:\n            return l-i\n    return 0\n```\n\n이게 더 효율적인 코드로 보인다. 괜히 `h list`를 만들 필요가 없다. ","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/coding-test/Picking-Numbers":{"title":"Picking Numbers","content":"\n\u003e Given an array of integers, find the longest subarray where the absolute difference between any two elements is less than or equal to 1. \n\n**Example**\n\na = [1,1,2,2,4,4,5,5,5]\n\nThere are two subarrays meeting the criterion:  [1,1,2,2,] and [4,4,5,5,5]. The maximum length subarray has 5 elements.\n\n**Function Description**\n\nComplete the _pickingNumbers_ function in the editor below. \n\npickingNumbers has the following parameter(s): \n\n- int a[n] : an array of integers \n\n**Returns**\n\n- int : the length of the longest subarray that meets the criterion \n\n**Input Format**\n\nThe first line contains a single integer _n_, the size of the array _a_.   \nThe second line contains _n_ space-separated integers, each *a[i]* .\n\n**Constraints**\n- 2 \u003c= n \u003c= 100\n- 0 \u003c a[i] \u003c 100\n-   The answer will be =\u003e 2. \n\n**Sample Input 0**\n\n```\n6\n4 6 5 3 3 1\n```\n\n**Sample Output 0**\n\n3\n\n**Explanation 0**\n\nWe choose the following multiset of integers from the array: {4,3,3}. Each pair in the multiset has an absolute difference  \u003c= 1(i.e., |4-1| = 1 and |3-3| = 0), so we print the number of chosen integers, 3, as our answer.\n\n## 나의 풀이\n\n```python\nimport math\nimport os\nimport random\nimport re\nimport sys\nfrom collections import Counter\n\ndef pickingNumbers(a):\n   \n # imput is un array of numbers.\n    count_nums = Counter(a)\n    max_num = 0\n    \n    for i in range(1, 100):\n\n        max_num = max(max_num, \n\t\t\t\t  count_nums[i] + count_nums[i+1])\n    return max_num\n\nif __name__ == '__main__':\n    fptr = open(os.environ['OUTPUT_PATH'], 'w')\n    \n    n = int(input().strip())\n    a = list(map(int, input().rstrip().split()))\n    \n    result = pickingNumbers(a)\n    \n    fptr.write(str(result) + '\\n')\n    fptr.close()\n```\n\n\u003e [!note]  Note  \n\u003e   \n\u003e 이 문제는 `Counter()`로 일단 세고 시작하면 간단한다.\n\u003e 개수를 센 이후에는 양 옆의 크기를 더한 것이 가장 큰 경우가 답이므로.\n","lastmodified":"2023-03-14T08:55:46.463505497Z","tags":null},"/notes/dl/Activation-function":{"title":"Activation function","content":"## Activation functions\n\n![](notes/images/스크린샷%202023-01-10%20오후%2011.04.41.png)\n\n\u003e 입력 신호의 총합을 출력 신호로 변환하는 함수\n\n-   `Sigmoid`\n    -   • 출력 값을 0에서 1로 변경해줍니다(Squashes number to range [0, 1])\n        -   단점: `Saturation` 문제\n            -   Sigmoid 함수의 출력 그래프를 보면 입력 신호의 총합이 크거나 작을 때 기울기가 0에 가까워지는 것을 볼 수 있습니다. 이렇듯 Activation Function의 구간에서 **기울기(gradient)가 0에 가까워지는 현상을 Saturated**라고 합니다. 이는 `Vanishing Gradient`문제를 야기합니다.\n-   `tanh`\n    -   출력 값을 -1에서 1로 압축시켜줍니다.\n    -   단점: 여전히 `Saturation` 문제가 있음.\n-   `ReLU`\n    -   양의 값에서는 Saturated 되지 않습니다.\n    -   단점: 음수 영역에서 saturated 되는 문제가 다시 발생합니다.\n-   `LeakyReLU`\n    -   ReLU와 유사하지만 negative regime(음의 영역)에서 더 이상 0이 아닙니다.\n    -   saturated 되지 않습니다","lastmodified":"2023-03-14T08:55:46.467505504Z","tags":null},"/notes/dl/Gradient-Descent-Methods":{"title":"Gradient Descent Methods","content":"## Gradient Descent\n\n-   first-order(1차 미분한 값만 사용) interative(반복적으로)\n-   for finding a local minimum of a differentiable fuction.\n\n## 중요한 개념들\n\n-   **Generalization**\n    -   한 네트워크의 성능이 학습 데이터와 비슷하게 나올 것이라고 보장해줄 때.\n-   **Under-fitting vs. over-fitting**\n    -   학습 데이터에 잘 동작하지만, 테스트 데이터에서 잘 동작하지 않을 때 Over-fitting이라고 한다.\n    -   네트워크가 너무 간단하거나 해서 정답을 잘못 맞추는 것을 Under-fitting이라고 한다.\n-   **Cross validation**\n    -   학습에 사용되지 않은 validation 데이터에 대해 얼마나 잘 동작하는지 보는 것.\n    -   Validation을 어느 크기로 해야 하는가?에 대한 답이 되는 개념.\n    -   최적의 하이퍼 파라미터 셋을 찾고, 이후 제대로 학습할 때는 모든 데이터를 다 이용해서 학습시킨다.\n    -   어느 경우에도 테스트 데이터셋을 사용하지 않는다!\n-   **Bias-variance tradeoff**\n    ![](notes/images/스크린샷%202023-01-11%20오전%2012.29.10.png)\n    -   `Variance`: 출력이 얼마나 일관되는가? Variance가 클수록 출력이 분산되어 있다.\n    -   `Bias`: 평균적으로 봤을 때, True target에 가까이 있는가? Bias가 높을 수록 True target에서 떨어져 있는 것이다.\n\t![](notes/images/스크린샷%202023-01-11%20오전%2012.31.43.png)\n    -   이 둘은 Tradeoff 관계에 있다. 이 둘을 모두 동시에 줄이는 것은 어렵다.\n-   **Bootstrapping**\n    -   서브 샘플링을 통해 여러 데이터셋, 여러 모델을 만들고 하겠다는 방법.\n-   **Bagging and boosting**\n\t![](notes/images/스크린샷%202023-01-11%20오전%2012.35.40.png)\n    -   `Bagging` :부트스트래핑으로 여러 모델을 만들고 그들의 출력을 평균내겠다는 방법.\n        -   ex) ensemble 기법\n    -   `Boosting`: 모델을 만들고, 그 모델(weak learner)이 잘 못하는 것에 대해서만 모델을 만들고, 만들고… 쭉 해서 이들을 시퀀셜하게 합쳐서 하나의 strong learner를 만드는 것. 결과적으로 하나의 모델이 만들어진다.\n\n## Practical Gradient Descent Methods\n1.  **SGD**\n    1.  한 개(single sample)만 보고 업데이트, 한 개만 보고 업데이트 …\n2.  **Mini-batch gradient descent**\n    1.  배치 사이즈마다 업데이트 → 일반적인 방법임.\n3.  **Batch gradient descent**\n    1.  한 번에 다 보고 업데이트\n\n\u003e 배치 사이즈는 중요하다.\n\n-   큰 배치 사이즈를 사용하면 sharp minimizers에 도달한다.\n-   작은 배치 사이즈를 사용하면 flat minimizers에 도달한다.\n-   sharp 보다는 flat이 좋다. 왜?\n\t![](notes/images/스크린샷%202023-01-11%20오전%2012.41.06.png)\n-   Flat minimizers는 좀 떨어져도 어느 정도 작은 값을 보장한다.\n-   반면, Sharp minimizers는 조금만 떨어져도 높게 되어버려 일반화 성능이 떨어진다.\n-   작은 배치 사이즈가 일반화에 더 좋다.\n    \n\n## 자동 미분 방법(Gradient Descent Methods)\n\n-   Gradient Descent:\n\t![](notes/images/스크린샷%202023-01-11%20오전%2012.43.39.png)\n    -   러닝 레이트 잡기가 어렵다. 더 빨리 학습시키는 방법은 없을까?\n    -   → 여러 최적화 테크닉들이 출현.\n-   `Momentum`: 이전 배치에서 흐른 정보를 활용하자.\n\t  ![](notes/images/스크린샷%202023-01-11%20오전%2012.45.00.png)\n    -   한 번 흘러가기 시작한 흐름을 어느 정도 유지시켜준다. 그레디언트가 왔다갔다 해도 어느 정도는 잘 학습시켜준다는 장점.\n-   `NAG`: 모멘텀에다가 Lookahead gradient를 사용하여, 한 번 이동하게 된다.\n\t![](notes/images/스크린샷%202023-01-11%20오전%2012.48.06.png)\n    -   local minimum을 지나지 않고 그 아래로 빨리 컨버징한다는 장점.\n-   `Adagrad`: 지금까지 변한 그레디언트($G_t)$에 따라서 조정.\n    -   많이 변한 그레디언트는 적게 변화시키고, 적게 변화시킨 그레디언트는 크게 변화시킨다.\n    -   $G_t$가 무한대로 커지면, 즉, 뒤로 가면 갈 수록 학습이 멈춰지게 된다는 단점 존재.\n-   `Adadelta`: $G_t$가 계속 커지는 것을 어느 정도 막겠다. EMA 사용\n    -   $H_t$라는 새로운 파라미터가 추가됨.\n    -   러닝 레이트가 사용되지 않음.\n-   `RMSprop`: EMA 사용, 강의 때 교수가 제안\n-   `Adam` : 그레디언트 조정과 모멘텀을 둘 다 쓰겠다는 아이디어.\n    -   $\\beta_1, \\beta_2$ 파라미터를 사용한다.\n    -   0으로 나눠지는 것을 막기 위해 $\\epsilon$ (`epsilon`)도 사용하는데 이 파라미터도 아주 중요!","lastmodified":"2023-03-14T08:55:46.467505504Z","tags":null},"/notes/dl/Gradient-descent":{"title":"Gradient descent","content":"## 미분\n\n\u003e 미분은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구로, 최적화에 제일 많이 사용하는 기법입니다.\n\n-   미분은 변화율(’접선의 기울기’) 의 극한으로 정의합니다.\n\n$$ f'(x) = \\lim_{h \\to 0}{f(x+h) - f(x) \\over h} $$\n\n-   `sympy.diff` 로 할 수 있다.\n-   한 점에서 접선의 기울기를 알면, 어느 방향으로 점을 움직여야 함수값이 증가, 감소하는지 알 수 있다.\n-   **증가시키고 싶으면 미분값을 더하고**\n    -   미분값을 더하면 ‘경사상승법’이라 하며 함수의 극대값의 위치를 구할 때 사용한다.\n-   **감소시키고 싶으면 미분값을 뺸다.**\n    -   미분값을 빼면 ‘경사하강법’이라 하며 함수의 극소값의 위치를 구할 때 사용한다.\n    -   목적함수를 최소화할 때 사용한다.\n-   극값에 도달하면 미분값이 0이므로 더 이상 업데이트가 되지 않고 움직임이 멈춘다.\n\n## 편미분(Partial Differentiation)\n\n\u003e 벡터가 입력인 다변수 함수의 경우 편미분을 사용한다.\n\n$$ \\partial_{x_i}f({x}) = \\lim_{h \\to 0}{f({x} + h{e}_i) - f({x}) \\over h} $$\n\n-   ${e}_i$는 i번째 값만 1이고 나머지는 0인 단위벡터\n\n$$\nf(x,y) = x^2+2xy+3+\\cos(x+2y), \n\\\\ \\partial_xf(x,y) = 2xy+2y-\\sin(x+2y) \n$$\n\n-   x 방향 편미분을 하면 y를 상수취급하고 x에 대해 미분한 결과만 반환한다.\n-   각 변수 별로 편미분을 계산한 ‘그레디언트 벡터’를 이용하여 경사하강/경사상승법에 사용할 수 있다.\n-   즉, 앞서 사용한 미분값 $f'(x)$ 대신 벡터 $\\nabla f$를 사용하여 변수 $x = (x_1, x_2, \\cdots, x_d)$를 동시에 업데이트 가능하다. 이를 수식으로 나타내면 아래와 같다.\n\n$$ \\nabla f = (\\partial_{x_1}f, \\partial_{x_2}f, \\cdots, \\partial_{x_d}f) $$\n\n-   그레디언트 벡터 $\\nabla f(x,y)$는 각 점 $(x,y)$에서 가장 빨리 증가하는 방향으로 흐르게 된다.\n-   이와 반대로, 그레디언트 벡터에 마이너스를 붙이면 극소값으로 이동한다. 즉, $-\\nabla f$는 극소값으로 이동한다.\n\n$$ f(x,y) = x^2 + 2y^2 \\\\\n\n-   \\nabla f = -(2x, 4y) $$\n\n-   알고리즘 상에서. 벡터를 다룰 때는 노름(norm)을 계산하여 종료조건으로 설정해야 한다.","lastmodified":"2023-03-14T08:55:46.467505504Z","tags":null},"/notes/lectures/stanford-CS224n/CS224n-main":{"title":"🌲 Stanford CS224n","content":"\n12.  [Neural Language Generation](notes/lectures/stanford%20CS224n/Neural%20Language%20Generation.md)","lastmodified":"2023-03-14T08:55:46.503505566Z","tags":null},"/notes/lectures/stanford-CS224n/Neural-Language-Generation":{"title":"Neural Language Generation","content":"\n\u003e **What is natural language generation?**  \n\u003e Any task involving text production for human consumption requires natural language generation\n\n### Formalizing NLG: a simple model and training algorithm\n\n- Basics of natural language generation\n\t- In autoregressive text generation models, at each time step $t$, our model takes in a sequence of tokens of text as input $\\{y\\}_{\u003ct}$ and outputs a new token, ${\\hat{y}}_t$.\n\t- ![](notes/lectures/stanford%20CS224n/image/cs224n_1.png)\n\t- At each time step t, our model computes a vector of scores for each token in our vocabulary, $S \\in \\mathbb{R}^V$:\n\t  $$S = f(\\{y_{\u003ct}\\}, \\theta)$$\n\t- Then, we compute a probability distribution $𝑃$ over $w \\in V$ using these scores:\n\t  $$P(y_t = w | \\{y_{\u003ct}\\}) = {{exp(S_w)} \\over {\\sum_{w'\\in V} exp(S_{w'})}}$$\n- **Basics: What are we trying to do?**\n\t- At each time step $t$, our model computes a vector of scores for each token in our vocabulary, $S \\in \\mathbb{R}^V$). Then, we compute a probability distribution $P$  over $w \\in V$ using these scores:\n\t  \n\t  ![](notes/lectures/stanford%20CS224n/image/스크린샷%202023-03-14%20오후%205.14.39.png)\n\t- At inference time, our decoding algorithm defines a function to select a token from this distribution:\n\t  $${\\hat{y_t}} = g(P(y_t|\\{y_{\u003ct}\\})$$\n\t  - In the equation above, $\\color{red}g(\\cdot)$ is your decoding algorithm.\n\t  - We train the model to minimize **the negative loglikelihood** of predicting the next token in the sequence:\n\t    $$L_t = -logP(y_t^* | \\{y_{\u003ct}^*\\})$$\n\t- The label at each step is the actual word $\\color{blue}y_t^*$ in the training sequence.\n\t- This token is often called the “gold” or “ground truth” token.\n\t- This algorithm is often called **“teacher forcing”**.\n\n- **Maximum Likelihood Training** (i.e., **teacher forcing**)","lastmodified":"2023-03-14T08:55:46.503505566Z","tags":null},"/notes/linear-algebra/%EC%84%A0%ED%98%95%EB%8F%85%EB%A6%BD%EA%B3%BC-%EC%84%A0%ED%98%95%EC%A2%85%EC%86%8D":{"title":"선형독립과 선형종속","content":"\n## 1. 선형 독립 - Linearly Independent\n\n![https://blog.kakaocdn.net/dn/BlzTy/btqL2xc3Lmh/GDr0naqkeZjxyHMEaHNZ71/img.png](https://blog.kakaocdn.net/dn/BlzTy/btqL2xc3Lmh/GDr0naqkeZjxyHMEaHNZ71/img.png)\n\n$R_n$ 공간에서 vector $\\left \\{ v_1, \\cdots, v_p \\right \\}$ 가 있을 때 만약 **벡터 방정식이 trivial solution(자명해)만 갖고 있을 시에 `선형 독립`이라고 한다.** 즉, trivial solution만 있으면 linearly independent이다. trivial solution만 존재한다는 의미는 free variable이 없다는 의미이다.\n\n## **2. 선형 종속 - Linearly Dependent**\n\n벡터 방정식 $c_1v_1+...+c_pv_p=0$에서 **weight c1,...,cp 중 하나라도 non zero면 `선형 종속`이라고 한다.** nontrivial solution을 갖고 있으면 최소 1개가 nonzero, 여러개가 zero 일 수 있으므로 linear combination으로 표현되지 않을 수 있다.\n\n## 선형종속 \u0026 선형독립과 방정식의 해\n\n![LA1](notes/images/LA1.png)\n\n위 그림은 열벡터(재료벡터, an) * 가중치(xn)의 선형결합으로 표현될 수 있는데, 이때 재료벡터의 차원수(3)보다 미지수 갯수(xn= 4)가 더 많으므로, 해가 무수히 많게 된다.\n\n반대로,\n\n![LA2](notes/images/LA2.png)\n\n이 그림은 재료벡터의 차원수(4)가 미지수 개수(xn = 2)보다 많은데, 이 경우에는 없거나 무수히 많게 된다. 즉, 케이스 바이 케이스이다. 언제 해가 없냐면, 위 그림처럼 두 재료벡터의 원소가 서로의 배수일 경우이다. 이러면 스팬이 평행사변형을 이루지 못하고 선형이 되기 때문에, 서로 선형종속이 되어버려 해가 존재하지 않게 된다.\n\n마지막으로, 재료벡터의 차원 수와 미지수 개수가 같다면,\n\n-   **재료 벡터끼리 선형 독립이면, 해가 하나이다.**\n-   **재료 벡터끼리 선형 종속이면, 해가 무수히 많다.**","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/linear-algebra/%EC%84%A0%ED%98%95%EC%8B%9C%EC%8A%A4%ED%85%9C":{"title":"선형시스템","content":"## 선형시스템\n\n$$ a_{1}x_{1} + a_{2}x_{2}+a_{3}x_{3} + ... + a_{n}x_{n} = b $$\n\n$$ \\begin{align*} a^Tx = b \\\\ where \\, a = \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ ... \\\\ a_{n} \\\\ \\end{bmatrix} and\u0026 \\, x = \\begin{bmatrix} x{1} \\\\ x{2} \\\\ ... \\\\ x_{n} \\\\ \\end{bmatrix} \\end{align*} $$\n\n## 역행렬\n\n만약 행렬 $A$가 역행렬이 가능하다면(invertible), 가능한 해는 오직 하나이며 그것은 $x = A^{-1}b$ 이다.\n\n역행렬이 가능한지는 두 가지 방식으로 알 수 있다. 행렬 $A = \\begin{bmatrix} a \u0026 b \\\\ c \u0026 d \\\\ \\end{bmatrix}$일때,\n\n1.  $ad - bc =0$ 이거나(= denominator가 0이면),\n2.  $a:b = c:d$ 이면 역행렬이 존재하지 않는다.\n\n단, 이것은 2차원일 때의 이야기다.\n\n만약 행렬 $A$가 역행렬이 존재하지 않는다면,\n\n1.  해가 없거나\n2.  가능한 해가 무수히 많다는 이야기다.\n\n\u003e 만약 정사각 행렬이 아니라면?\n\n$m = equations, \\, n = variables$ 이라고 할 때,\n\n-   m \u003c n: 가능한 해가 무수히 많다.\n-   n \u003c m: 해가 존재하지 않는다.\n\n## 내적과 외적\n\n- 내적 Inner product: 벡터들로부터 스칼라를 만드는 것\n\n$$\n\\begin{equation} \\begin{bmatrix} 3 \u0026 2 \u0026 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 3 \\\\ 5 \\\\ \\end{bmatrix}\n= \n\\begin{bmatrix} 14 \\end{bmatrix} \\end{equation} \n$$\n\n- 외적 Outer product: 두 열벡터를 곱하여 큰 매트릭스를 만드는 것\n\n$$ \\begin{equation} \\begin{bmatrix} 1 \\\\ 3 \\\\ 5 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 \u0026 2 \\\\ \\end{bmatrix}\n=\n\\begin{bmatrix} 1 \u0026 2 \\\\ 3 \u0026 6 \\\\ 5 \u0026 10 \\\\ \\end{bmatrix} \\end{equation} $$","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/linear-algebra/%EC%84%A0%ED%98%95-%EB%B3%80%ED%99%98Linear-Transformation":{"title":"선형 변환(Linear Transformation)","content":"\n## 선형 변환(Linear Transformation)\n\n\u003e : 함수의 다른 말. 공간 상에서 선형으로 변환되는 걸 상상하라.\n\n$y = 3x + 1$ 처럼, bias가 붙어 있는 것은 선형 변환이 아니라 affine 변환이다.\n\n![](notes/images/스크린샷%202023-01-10%20오후%205.44.42.png)\nStandard basis vector를 하나하나 다 넣어보면 $T(x)$를 알 수 있다.\n위의 수식에서, **Transformation을 가하는 순서에 상관없이 값이 동일하다.\n\n## Linear Transformation in Neural Networks\n\n![](notes/images/스크린샷%202023-01-10%20오후%205.58.07.png)\n\n상수벡터가 끼어있으면 Affine Transformation\ncolumn combination으로 선형변환으로 바꿀 수 있다!\n→ 1을 추가해주면 선형변환으로 바꿀 수 있다.","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/linear-algebra/Linear-Algebra":{"title":"Linear Algebra","content":"\n- [선형시스템](notes/linear%20algebra/선형시스템.md)\n- [Span \u0026 Basis](notes/linear%20algebra/Span%20\u0026%20Basis.md)\n- [선형독립과 선형종속](notes/linear%20algebra/선형독립과%20선형종속.md)\n- [선형 변환(Linear Transformation)](notes/linear%20algebra/선형%20변환(Linear%20Transformation).md)\n\n","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/linear-algebra/Span-Basis":{"title":"Span \u0026 Basis","content":"## Span(생성)\n\n[https://youtu.be/9F4PZ_1orF0](https://youtu.be/9F4PZ_1orF0)\n\n## 기저와 차원(Basis, Dimension)\n\n[(선형대수학) 1.3 Basis, Dimension](https://elementary-physics.tistory.com/6)\n\n$\\vec{v}_1, \\vec{v}_2,..., \\vec{v}_r$의 `linear combination`으로 만들 수 있는 모든 vector들의 집합을 set spanned by $\\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_r$이라고 부르고 \n\n$$\\mathrm{span}(\\left \\{\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_r \\right \\})$$\n\n이라고 표현한다. (이 집합은 기존 vector space의 부분 집합이면서 동시에 그 자신이 vector space가 되는데 이러한 경우, 이 집합을 기존 vector space의 `subspace`라고 부른다.)\n\n### Basis of Vector Space\n\n거의 대부분 vector space들은 무한히 많은 vector들을 가지고 있기 때문에 이들 전부를 각각 다루기보다는 적당히 대표적인 vector들을 이용하여 모든 vector에 대한 분석을 할 수 있으면 좋을 것이다. 이러한 vector들은 vector space 전체를 표현할 수 있어야 하므로,\n\n1.  모든 vector들은 대표적인 vector들의 linear combination으로 나타낼 수 있어야 하고 (**spanned set이 vector space 전체가 되어야 하고**)\n\n최대한 적은 수의 vector들로 표현하는 것이 유리할 것이므로 linearly dependent한 vector들은 linearly independent한 vector들만 남겨놓는다면,\n\n2. **서로 linearly independent** 해야 한다.\n\n이러한 두 가지 조건을 만족하는 vector들의 집합을 `basis`라고 부른다.\n\n\u003e [!info] DEFINITION: Basis of Vector Space  \n\u003e   \n\u003e Vector space V의 부분 집합 B가\n\u003e 1. $span(B)=V$\n\u003e 2. B의 vector들은 서로 linearly independent\n\u003e\n\u003e하면 B를 V의 basis라고 부른다.\n\n**Basis를 구성하는 vector의 수보다 많은 수의 vector들은 linearly dependent**이다. 예를 들어, basis가 $$ \\{ \\vec{e}_1, \\vec{e}_2, \\cdots , \\vec{e}_n \\} $$\n이라면, vector $\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_n$ 은 각각이 basis vector의 linear combination\n\n$$\\vec{v}\\_i= \\sum_{j=1}^n a_{ij} \\vec{e}_j$$\n\n로 표현되고, 만약\n\n$$\\vec{0}= \\sum_{i=1}^{n+1} x_i \\vec{v}\\_i = \\sum_{i=1}^{n+1} \\sum_{j=1}^n x_i a_{ij} \\vec{e}_j\n$$\n이라면 basis의 정의에 따라,\n\n$$ \\sum_{i=1}^{n+1} a_{ij} x_i =0 $$\n\n이어야 하는데,\n\n이것은 미지수는 n+1개 이지만, 식은 n개 밖에 없는 연립방정식이다. 그러므로 이 연립방정식의 해는 없거나 무한히 많다. 그러나 모든 xi가 0인 명백한 해가 존재하므로, 이 연립방정식의 해는 무한히 많다. 그러므로 v→1, v→2, ... , v→n+1는 linearly dependent이다.\n\n이러한 이유로 **basis의 vector의 개수**는 매우 특별한 수라고 하겠다. 이 수를 **vector space의** `dimension`이라고 부른다. 만약 basis의 vector의 수가 유한하다면 그 vector space를 finite dimensional vector space라고 부르고 유한하지 않다면 infinite dimensional vector space라고 부른다.\n\nBasis에서 중요한 점은 **basis 선택은 유일하지 않다**는 점이다. 단지 임의로 위의 두 조건에 맞게 basis를 구성할 수 있다.\n\n## Sum of Rank-1 Outer Products\n\n\u003e 그 장점이 무엇인가?\n\n![sum of rank_1](notes/images/sum%20of%20rank_1.png)\n\n100_50개의 숫자를 단지 150_10개의 숫자로 표현할 수 있다. 정확하진 않지만, 근사적으로.","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/ml/%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5-Un-supervised-Learning":{"title":"비지도학습 (Un-supervised Learning)","content":"\n## 비지도학습 예시\n\n-   **군집 ( Clustering )**\n\t- K-평균 ( K-Means )  \n\t- DBSCAN  \n\t- 계층 군집 분석 ( Hierarchical Cluster Analysis : HCA )\n-   **이상치/특이치탐지( Anomaly / Novelty Detection )**\n\t- One-class SCM\n\t- Isolation Forest\n-   **시각화 ( Visualization ) \u0026 차원축소 ( Dimension Reduction )**\n\t- 주성분 분석 ( Principal Component Analysis : PCA )\n\t- 커널 PCA ( Kernel PCA )\n\t- 지역적 선형 임베딩 ( Locally Linear Embedding : LLE )\n\t- T-SNE ( t-distributed stochastic neighbor embedding )\n-   연관규칙학습 ( Association Rule Learning )","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/ml/%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5-Supervised-Learning":{"title":"지도학습 (Supervised Learning)","content":"## 지도학습 대표 알고리즘\n\n1.  K-Nearest Neighbors (KNN)\n2. Linear Regression\n3. Logistic Regression\n4. Support Vector Machines (SVM)\n5. Decision Tree / Random Forest\n6. Gradient Boosting Algorithms ([XGB](notes/ml/XGB%20Modeling.md), LGB, CatGB, NGB 등)\n7. Neural Networks","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/ml/EDA-Visualization":{"title":"EDA \u0026 Visualization","content":"TODO","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/ml/Hyper-parameter-tuning":{"title":"Hyper parameter tuning","content":"\n## 그리드 탐색(Grid Search)\n-   알고리즘 내 효과적인 하이퍼파라미터 조합을 찾을 때까지, 탐색하고자 하는 하이퍼파라미터와 그에 해당하는 시도해볼 값들을 지정하여 하이퍼파라미터 튜닝을 진행하는 것\n-  사이킷런의 `GridSearchCV` 클래스를 사용하여, 미리 설정한 하이퍼파라미터 조합에 대해 교차검증을 진행하고, 이를 통해 평가를 할 수 있다\n- `RandomForestRegressor`에 `GridSearchCV` 적용하기\n\t-   3X4개의 Hyperparameter Tuning\n\t-   2X3개의 Hyperparameter Tuning\n\t-   5회 Cross Validation 진행\n\n```python\nfrom sklearn.model_selection import GridSearchCV\n\n  \n\nparam_grid = [\n# try 12 (3×4) combinations of hyperparameters\n{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n\n# then try 6 (2×3) combinations with bootstrap set as False\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}\n]\n\n  \n\nforest_reg = RandomForestRegressor(random_state=42)\n\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n\t\t\tscoring='neg_mean_squared_error',\n\t\t\treturn_train_score=True)\n\ngrid_search.fit(housing_prepared, housing_labels)\n```\n\n`RandomForestRegressor`에 `GridSearchCV `적용 후 하이퍼파라미터 별 평가 점수 확인\n\n```python\ncvres = grid_search.cv_results_\n\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n\tprint(np.sqrt(-mean_score), params)\n```\n\n```\n\u003e\u003e\u003e \n65005.182970763315 {'max_features': 2, 'n_estimators': 3} 55582.91015494046 {'max_features': 2, 'n_estimators': 10} 52745.33887865031 {'max_features': 2, 'n_estimators': 30} 60451.18914812725 {'max_features': 4, 'n_estimators': 3} 53062.818497303946 {'max_features': 4, 'n_estimators': 10} 50663.79774079741 {'max_features': 4, 'n_estimators': 30} 57998.07162873506 {'max_features': 6, 'n_estimators': 3} 52042.04702364244 {'max_features': 6, 'n_estimators': 10} 50028.060190761295 {'max_features': 6, 'n_estimators': 30} 58308.44501796401 {'max_features': 8, 'n_estimators': 3} 52082.74313186547 {'max_features': 8, 'n_estimators': 10} 50165.81805010987 {'max_features': 8, 'n_estimators': 30} 62709.54311517104 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3} 54062.01766032325 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10} 60613.541905953585 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3} 53742.988651846914 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10} 59387.46561811065 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3} 52826.41762121993 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n```\n\n\n## 랜덤 탐색(Random Search)\n- `GridSearchCV`를 진행할 하이퍼파라미터 조합의 수가 너무 많을 때, `RandomizedSearchCV`를 활용하면 조합 내에서 임의의 하이퍼파라미터를 대입하여 지정한 횟수만큼 평가하게 됨  \n- 주요 장점 2가지\n\t1.  랜덤 탐색을 1,000회 반복하면, 각 하이퍼파라미터마다 각기 다른 1,000개 경우를 탐색할 수 있음\n\t2. 반복 횟수를 단순히 조절하는 것 만으로도 하이퍼파라미터에 투입할 컴퓨팅 자원을 제어할 수 있음\n\n```python\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n'n_estimators': randint(low=1, high=200),\n'max_features': randint(low=1, high=8),\n}\n\nforest_reg = RandomForestRegressor(random_state=42)\n\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions = param_distribs, n_iter=10, cv=5,\n\t\t\t\tscoring='neg_mean_squared_error',\n\t\t\t\trandom_state=42)\n\nrnd_search.fit(housing_prepared, housing_labels)\n```\n\n```python\ncvres = rnd_search.cv_results_\n\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n\tprint(np.sqrt(-mean_score), params)\n\n\u003e\u003e\u003e\n49462.596134607906 {'max_features': 7, 'n_estimators': 180} 51676.97211565583 {'max_features': 5, 'n_estimators': 15} 50827.83871022729 {'max_features': 3, 'n_estimators': 72} 51117.698297994146 {'max_features': 5, 'n_estimators': 21} 49585.185219390754 {'max_features': 7, 'n_estimators': 122} 50836.040148806715 {'max_features': 3, 'n_estimators': 75} 50746.890270152086 {'max_features': 3, 'n_estimators': 88} 49788.190631507045 {'max_features': 5, 'n_estimators': 100} 50574.565725719985 {'max_features': 3, 'n_estimators': 150} 65153.787556165735 {'max_features': 5, 'n_estimators': 2}\n```\n\n위와 같이 랜덤 탐색을 이용하는 게 더 나은 결과가 나올 수 있다. \n\n---\n\n\u003e [!info] Useful info  \n\u003e   \n\u003e 테스트 데이터 세트로 시스템 평가하기\n\n테스트 데이터 세트에서 설명변수와 타겟변수를 얻은 후, 기존의 변환 파이프라인을 통해 `transform`을 진행 ( 테스트의 경우 `fit_transform`와 같은 학습 과정이 포함된 변환은 진행하지 않음 )\n\n```python\nfinal_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\n\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\n```\n\n```python\nfinal_rmse\n\u003e\u003e 47362.98158022501\n```\n\n오차 값을 단일 추정값으로 확인하는 것이 아니라, 얼마나 정확한지를 `Scipy.stats.t.interval()` 기반 95% 신뢰 구간 계산을 통해 점검해볼 수 있다.\n\n```python\nfrom scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,loc=squared_errors.mean(),\nscale=stats.sem(squared_errors)))\n```\n\n```python\n\u003e\u003e\u003e array([45397.61151846, 49249.98392646])\n```","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/ml/MAE-and-MSE":{"title":"MAE and MSE","content":"MAE와 MSE는 회귀 모델의 성능을 평가하기 위한 두 가지 **손실 함수**입니다[1](https://www.kaggle.com/getting-started/52081). MAE는 Mean Absolute Error의 약자로, 예측값과 실제값의 절대값 차이의 평균입니다. MSE는 Mean Squared Error의 약자로, 예측값과 실제값의 제곱 차이의 평균입니다.\n\nMAE와 MSE는 다음과 같은 차이점이 있습니다[1](https://www.kaggle.com/getting-started/52081)[2](https://stats.stackexchange.com/questions/582238/mae-vs-mse-for-linear-regression)[3](https://m.blog.naver.com/PostView.naver?blogId=heygun\u0026logNo=221516529668)[4](https://stephenallwright.com/mse-vs-mae/):\n\n-   **MAE는 오차에 대해 선형적으로 반응하고, MSE는 오차에 대해 비선형적으로 반응합니다.** 즉, MSE는 큰 오차에 더 큰 패널티를 부여하고 작은 오차에 더 작은 패널티를 부여합니다. 반면에 MAE는 모든 오차에 동일한 가중치를 부여합니다.\n\n\u003e[!info] key difference  \n\u003e   \n\u003eThe key difference between squared error and absolute error is that squared error punishes large errors to a greater extent than absolute error, **as the errors are squared instead of just calculating the difference.**\n\n- **MAE는 이상치(outlier)에 대해 더 강인하고(MSE보다 영향을 덜 받고), MSE는 이상치에 대해 더 민감합니다(MAE보다 영향을 많이 받습니다).** 이상치가 많은 데이터셋에서는 MAE가 MSE보다 더 신뢰할 수 있는 지표일 수 있습니다.\n-   **MAE를 최소화하는 값은 중앙값(median)이고, MSE를 최소화하는 값은 평균(mean)입니다.** 중앙값은 평균보다 이상치에 영향을 덜 받기 때문에 MAE가 MSE보다 이상치에 강인하다고 할 수 있습니다.\n-   MSE는 기울기 하강법(gradient descent)과 같은 최적화 알고리즘에서 쉽게 계산할 수 있는 편미분(derivative)을 가지고 있습니다. 반면에 MAE는 편미분이 0인 지점에서 연속성이 깨지기 때문에 최적화하기 어렵습니다.\n\n어떤 손실 함수를 사용할지 결정하는 것은 문제의 특성과 목적에 따라 달라집니다. **일반적으로 큰 오차를 허용하지 않거나 정확한 예측 값을 원한다면 MSE를 사용하고, 작은 오차를 허용하거나 이상치가 많은 데이터셋을 다룬다면 MAE를 사용하는 것이 좋습니다**[3](https://m.blog.naver.com/PostView.naver?blogId=heygun\u0026logNo=221516529668).","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/ml/Recall-and-Precision":{"title":"Recall and Precision","content":"\nRecall과 precision은 분류 문제에서 성능을 평가하는 지표입니다. Recall은 실제로 관련된 예제 중에서 검색된 예제의 비율이고, precision은 검색된 예제 중에서 관련된 예제의 비율입니다. Recall과 precision은 서로 trade-off 관계에 있습니다. 즉, 한 쪽을 높이면 다른 쪽이 낮아집니다.\n\n**Recall이 중요한 경우는 실제로 관련된 예제를 놓치지 않아야 하는 경우입니다.** 예를 들어, 암 진단이나 스팸 필터링 같은 경우에는 실제로 암 환자나 스팸 메일을 정상으로 분류하는 것(FN)이 큰 위험이 될 수 있습니다. 따라서 recall을 높여서 FN을 줄여야 합니다.\n\n**Precision이 중요한 경우는 검색된 예제가 정확하게 관련되어야 하는 경우입니다.** 예를 들어, 웹 검색이나 상품 추천 같은 경우에는 사용자에게 관련 없는 결과나 상품을 보여주는 것(FP)이 사용자 만족도를 떨어뜨릴 수 있습니다. 따라서 precision을 높여서 FP를 줄여야 합니다.","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/ml/SMOTE":{"title":"SMOTE","content":"\n`SMOTE`: Synthetic Minority Over-sampling Technique은 불균형한 데이터셋에서 분류기의 성능을 향상시키기 위한 방법입니다. 불균형한 데이터셋이란 한 클래스가 다른 클래스보다 훨씬 많은 비율로 존재하는 경우를 말합니다. **SMOTE는 소수 클래스의 예제를 인공적으로 생성하여 소수 클래스를 오버샘플링하는 방식으로 작동합니다.** 인공적인 예제는 소수 클래스의 가까운 이웃들을 기반으로 생성됩니다. SMOTE는 C4.5, Ripper, Naive Bayes와 같은 분류기에 적용할 수 있으며, ROC 곡선 아래 면적(AUC)을 사용하여 평가됩니다.\n\nSMOTE의 동작 원리는 다음과 같습니다. **소수 클래스의 예제를 선택하고 그 예제와 가장 가까운 k개의 이웃을 찾습니다. 그 다음에 이웃들과의 차이 벡터에 임의의 수를 곱하여 새로운 예제를 생성합니다.** 이 과정을 반복하여 원하는 만큼 소수 클래스의 예제를 늘립니다. 이렇게 하면 소수 클래스가 과대표현되지 않고 다양한 패턴을 학습할 수 있습니다.\n\n**장점**:\n\n-   무작위 오버샘플링에서 발생할 수 있는 모델 오버피팅을 완화합니다. 인공적인 예제를 생성하기 때문에 인스턴스의 복제가 아닙니다\n-   정보의 손실이 없습니다\n-   구현하고 해석하기 쉽습니다\n\n**단점**:\n\n-   인공적인 예제를 생성할 때 SMOTE는 이웃 클래스나 잡음에 대해 고려하지 않습니다. [따라서 소수 클래스와 다수 클래스 사이의 경계에 가까운 예제들을 생성할 수 있습니다](https://www.datacamp.com/tutorial/diving-deep-imbalanced-data)[2](https://www.datacamp.com/tutorial/diving-deep-imbalanced-data).\n-   [고차원 데이터셋에서는 효과가 떨어질 수 있습니다](https://www.dominodatalab.com/blog/smote-oversampling-technique)[3](https://www.dominodatalab.com/blog/smote-oversampling-technique).","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/ml/Sensitivity-and-Specificity":{"title":"Sensitivity and Specificity","content":"\n**민감도(Sensitivity)**: recall과 동일하며, 실제 양성을 간과하지 않는 정도라고 볼 수 있으므로 오탐이 적습니다.\n$$\nSensitivity = {TP \\over {TP+FN}}\n$$\n\n**특이도(Specificity)**: 실제 음성률이라고도 하며, 실제 음성으로 정확하게 식별되는 실제 음성 비율, 즉 실제 음성으로 분류되는 정도(오탐이 적음)를 측정합니다.\n$$\nSpecificity = {TN \\over TN+FP}\n$$\n\n\n따라서 민감도는 오탐을 피하는 정도를 정량화하며, 특이도는 오탐에 대해 동일한 기능을 수행합니다.\n\n이 두 가지 중 하나에 집중하는 것이 중요할 수 있는 시나리오는 다음과 같습니다:\n\n민감도: 질병이 있는 것으로 정확하게 식별되는 아픈 사람의 비율입니다.\n특이도: 해당 질환이 없는 것으로 정확하게 식별된 건강한 사람의 비율입니다.\n","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/ml/XGB-Modeling":{"title":"XGB Modeling","content":"\n## 1. XGB explained\n\n### XGBoost의 특징\n\n- 병렬 처리 가능\n- GPU 지원이 가능\n- 추가적으로 **정규화 기능, Tree pruning 기능, Early Stopping, 내장된 교차검증과 결측치 처리** 등\n\n### XGBoost의 대표적인 파라미터\n\n**다룰 수 있는 파라미터가 많기 때문에 Customizing이 용이하다.**\n\n- n_estimators (int) : 내부에서 생성할 결정 트리의 개수\n- max_depth (int) : 생성할 결정 트리의 높이\n- learning_rate (float) : 훈련량, 학습 시 모델을 얼마나 업데이트할지 결정하는 값\n- colsample_bytree (float) : 열 샘플링에 사용하는 비율\n- subsample (float) : 행 샘플링에 사용하는 비율\n- reg_alpha (float) : L1 정규화 계수\n- reg_lambda (float) : L2 정규화 계수\n- booster (str) : 부스팅 방법 (gblinear / gbtree / dart)\n- random_state (int) : 내부적으로 사용되는 난수값\n- n_jobs (int) : 병렬처리에 사용할 CPU 수\n\n## 2. Usage\n\n### 분류 문제\n```python\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\n# 원래 여기 데이터에는 검증 데이터를 넣어야함 Test 데이터 넣으면 안됨!\n# 검증 데이터 넣어주어서 교차검증 해보도록하기\nevals = [(x_test, y_test)]\nxgb_wrapper = XGBClassifier(n_estimators=100, learning_rate=0.1,\n                           max_depth=3)\n                           \n# eval_metric넣어주면서 검증 데이터로 loss 측정할 때 사용할 metric 지정\nxgb_wrapper.fit(x_train, y_train, early_stopping_rounds=200,\n               eval_set=evals, eval_metric='auc')\n# Prediction 1\nprint('Train Score : {}'.format(xgb_wrapper.score(X_train, y_train)))\nprint('Test Score : {}'.format(xgb_wrapper.score(X_test,y_test)))\n\n# Prediction 2\npreds = xgb_wrapper.predict(x_test)\npreds_proba = xgb_wrapper.predict_proba(x_test)[:, 1]\nprint(preds_proba[:10])\n\n# 모델 평가\nxgb_roc_score = roc_auc_score(\n    y_test,\n    xgb_wrapper.predict_proba(X_test)[:, 1]\n)\n```\n\n### Missing value는?\n\nXGBoost는 누락 값에 대해서 어떻게 대응할 지 알아서 정하도록 설정되어 있다. 따라서 우리가 해줘야 되는 가공 작업은 단순히 **누락값을 전부 0으로 바꿔주는 것**이다. \n\n```python\n# 누락된 값이 있는 행의 개수 = 누락 데이터 개수\nlen(df.loc[df['Total_Charges'] == ' '])\n\n# 누락된 행 직접보기\ndf.loc[df['Total_Charges'] == ' ']\n\n# 직접 바꾸기\ndf.loc[(df['Total_Charges'] == ' '), 'Total_Charges'] = 0\n```\n\n### Categorical values는?\n\n```python\npd.get_dummies(X, columns = ['Payment_Method'])\n```\n\n### 성능 알아보기\n```python\nval_error = mean_squared_error(y_val, y_pred)\nprint(\"XGB's Validation MSE:\", val_error)\n```\n\n### 회귀 문제\n```python\nfrom xgboost import XGBRegressor # XGBRegressor 모델 선언 후 Fitting\nxgbr = XGBRegressor() \nxgbr.fit(x_train, y_train) # Fitting된 모델로x_valid를 통해 예측을 진행 \n\n# Prediction 1\nprint('Train Score : {}'.format(xgbr.score(X_train, y_train)))\nprint('Test Score : {}'.format(xgbr.score(X_test,y_test)))\n\n# Prediction 2\ny_pred = xgbr.predict(x_valid)\n```\n\nXGBoost는 feature별 중요도를 plot 해주는 라이브러리를 개별적으로 제공한다. feature별 중요도를 보기 위해서 간단하게 시각화하는 코드는 다음과 같다.\n\n```python\n# feature별 중요도 시각화하기\nfrom xgboost import plot_importance\n\nfig, ax = plt.subplots(figsize=(9,11))\nplot_importance(xgb_wrapper, ax)\n```\n","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/paper-review":{"title":"📑 Paper Review","content":"\n## Contents\n\n- **Common sense**\n\t- [📄 GLUCOSE](notes/paper-review/GLUCOSE.md)\n- **Hate, abusive language detection**\n\t- [📄 On Cross-Dataset Generalization in Automatic Detection of Online Abuse](notes/paper-review/On%20Cross-Dataset%20Generalization%20in%20Automatic%20Detection%20of%20Online%20Abuse.md) \n\t- [📄 Towards generalisable hate speech detection](notes/paper-review/Towards%20generalisable%20hate%20speech%20detection.md)\n\t- [📄 Challenges and frontiers in abusive content detect](notes/paper-review/Challenges%20and%20frontiers%20in%20abusive%20content%20detect.md)\n\t- [📄 Studying Generalisability Across Abusive Language Detection Datasets](notes/paper-review/Studying%20Generalisability%20Across%20Abusive%20Language%20Detection%20Datasets.md)\n\t- [📄 Hate speech detection is not as easy as you may think](notes/paper-review/Hate%20speech%20detection%20is%20not%20as%20easy%20as%20you%20may%20think.md)\n- **Conversational AI**\n\t- [📄 Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge](notes/paper-review/Call%20for%20Customized%20Conversation.md)\n\t- [📄 LittleBird - Efficient Faster \u0026 Longer Transformer for QuestionAnswering](notes/paper-review/LittleBird%20-%20Efficient%20Faster%20\u0026%20Longer%20Transformer%20for%20QuestionAnswering.md)\n- **Language Models**\n\t- [📄 Language Models are Few-Shot Learners](notes/paper-review/Language%20Models%20are%20Few-Shot%20Learners.md)\n\t- [📄 InstructGPT](notes/paper-review/InstructGPT.md)\n","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/paper-review/Call-for-Customized-Conversation":{"title":"Call for Customized Conversation-Customized Conversation Grounding Persona and Knowledge","content":"\n## Abstarct\n\u003e Humans usually have conversations by making use of prior knowledge about a topic and background information of the people whom they are talking to. **However, existing conversational agents and datasets do not consider such comprehensive information, and thus they have a limitation in generating the utterances where the knowledge and persona are fused properly.** To address this issue, we introduce a call For Customized conversation **(FoCus) dataset** where the customized answers are built with the user’s persona and Wikipedia knowledge. (...)\n- 현재 상황과 한계를 찾아보자. \n\t- 사람은 대화의 주제나, 자기와 말하고 있는 사람의 배경 정보 등에 기대어 대화를 이어나간다. 그러나 현재 대화 시스템이나 데이터셋은 이러한 정보들을 이해하지 못하고 있으며, 따라서 지식이나 성격이 함께 적절히 융합된 발화를 생성하는 데에 한계가 있다. \n- 그래서 본 논문의 목적은?\n\t- 이에 본 논문은 이용자의 성격(persona)와 Wikipedia과 함께 구축된 FoCus 데이터셋을 발표했다.\n\n\u003e We examine whether the model reflects adequate persona and knowledge with our proposed two sub-tasks, **persona grounding (PG) and knowledge grounding (KG)**.\n- 그래서 그걸로 뭘 한 건데?\n\t- 모델이 적절히 페르소나와 지식을 반영하는지 확인해볼 것이다. \n\t- `Persona grounding (PG)` 와 `Knowledge grounding (KG)`라고 하는 서브 태스크를 통해서!\n\n## FoCus Dataset\n\n![Figure 2: Example dialog between Human and Machine in FoCus dataset](Example-dialog.png)\n- 그래서 이런 대화가 가능하도록 하고 싶다는 것이다. \n- 이용자와 대화를 진행하는 시스템이 실제 지식에 기반하면서('This place is called Sentosa'), 또한 이용자의 페르소나에 근거하여 말을 이어나가는 것('I believe you wish to visit Singapore.')!\n\n## Model\n\n![](Overview-of-model.png)\n\u003e We introduce the baseline models trained on our FoCus dataset, consisting of a `retrieval module` and a `dialog module`. The `retrieval module` retrieves the knowledge paragraphs related to a question, and the `dialog module` generates utterances of the machine by taking the retrieved knowledge paragraphs, human’s persona, and previous utterances as inputs.\n- 그래서 모델 아키텍쳐에는 두 개의 모듈이 들어간다. `retrieval module` 과 `dialog module` 이다. `retrieval module` 은 질문과 관련된 지식 파라그래프를 검색하고, `dialog module` 은 이것과 이용자의 페르소나 정보에 근거해서 모델의 발화를 생성해내는 것이다.\n\n![Table 3: Experimental results](Experimental-results.png)\n- 실험 결과, PG, KG에 모두 훈련된 BART와 GPT-2가 generation에서는 조금 성능이 낮지만 전반적으로 비슷한 성능을 보여주며, Grouding 서브 태스크에서는 가장 뛰어난 성능을 보여주었다. \n\n## Conclusion\n\u003e We hope that the researches aim to make dialog agents more attractive and knowledgeable with grounding abilities to be explored.\n- 우리는 연구자들이 더 있을 실제적 능력들과 함께, 대화 에이전트를 더욱 매력적이고 지식을 풍부히 가지도록 만드는 것을 목적으로 하기를 희망한다. \n- 끝맺음은 훈훈하게 하는구나. 대화 시스템은 더욱 매력적이게 될 수 있다. **감정 뿐만 아니라 지식, 페르소나를 모두 고려한 대화 시스템은 어떤 모습이 될까?**","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/paper-review/Challenges-and-frontiers-in-abusive-content-detect":{"title":"Challenges and frontiers in abusive content detect","content":"\n\u003e [!note] Note  \n\u003e \n\u003e 이번에 정리하는 논문은 \"abusive content detect\" 과제가 어떻게 발전했고, 지금 마주하고 있는 어려움은 무엇인지 소개하는 논문이다. \n\u003e 혐오표현 탐지 과제와 (완전히는 아니지만) 비슷한 과제이기에 비슷한 어려움들을 공유하고 있다. 주석 작업의 어려움, 개념 정의의 정확성 등...\n\u003e 개인적으로 인상 깊은 것은 '훈련 데이터와 다른 도메인에서 테스트가 이루어지면 성능이 어떻게 변할지'를 소개하는 파트이다.\n\n***\n\n\u003e Developing robust systems to detect abuse is a crucial part of online content moderation and plays a fundamental role in creating an open, safe and accessible Internet.\n-  욕설(abuse)을 탐지하기 위한 강력한 시스템을 개발하는 것은 온라인 콘텐츠 조정의 중요한 부분이며 **개방적이고 안전하며 접근 가능한 인터넷을 만드는 데 근본적인 역할을 한다**.\n\n\u003e Advances in machine learning and NLP have led to marked improvements in abusive content detection systems’ performance (Fortuna \u0026 Nunes, 2018; Schmidt \u0026 Wiegand, 2017). For instance, in 2018 Pitsilis et al. trained a classification system on Waseem and Hovy’s 16,000 tweet dataset and achieved an F-Score of 0.932, compared against Waseem and Hovy’s original 0.739; a 20-point increase (Pitsilis, Ramampiaro, \u0026 Langseth, 2018; Waseem \u0026 Hovy, 2016).\n-  **머신 러닝과 NLP의 발전은 모욕 콘텐츠 감지 시스템의 성능을 현저하게 향상시켰다(Fortuna \u0026 Nunes, 2018; Schmidt \u0026 Wiegand, 2017).** 예를 들어, 2018년 Pitsilis 등은 Wasem과 Hovy의 16,000개의 트윗 데이터 세트에 대한 분류 시스템을 훈련하고 Wasem과 Hovy의 원래 0.739와 비교하여 0.932의 F-Score를 달성했다(Pitsilis, Ramampiaro, \u0026 Langseth, 2018; Wasem \u0026 Hovy, 2016).\n\n\u003e Researchers have also addressed numerous tasks beyond binary abusive content classification, including identifying the target of abuse and its strength as well as automatically moderating content (Burnap \u0026 Williams, 2016; Davidson, Warmsley, Macy, \u0026 Weber, 2017; Santos, Melnyk, \u0026 Padhi, 2018).\n- 연구자들은 또한 학대 대상을 식별하는 것뿐만 아니라 콘텐츠 자동 조절을 포함하여 이진 욕설 콘텐츠 분류를 넘어 수많은 과제를 해결했다(Burnap \u0026 Williams, 2016; Davidson, Warmsley, Macy, Weber, 2017; Santos, Melnyk, \u0026 Padhi, 2018). \n\n\u003e (…) what type of abusive content it is identified as. This is a social and theoretical task: **there is no objectively ‘correct’ definition** or single set of pre-established criteria which can be applied.\n- (...) 어떤 유형의 욕설 콘텐츠로 식별되는지도 중요하다. 이러한 분류는 사회적이고 이론적인 과제이기에, **객관적으로 '올바른' 정의나 적용할 수 있는 사전 설정된 기준의 단일 집합은 없다.**\n\n\u003e Detecting abusive content generically is an important aspiration for the field. **However, it is very difficult because abusive content is so varied.** Research which purports to address the generic task of detecting abuse is typically actually addressing something much more specific. This can often be discerned from the datasets, which may contain systematic biases towards certain types and targets of abuse. For instance, the dataset by Davidson et al. is used widely for tasks described generically as abusive content detection yet it is highly skewed towards racism and sexism (Davidson et al., 2017).\n- 욕설 콘텐츠를 전반적으로 감지하는 것은 현장의 중요한 포부다. 하지만 이는 욕설의 내용이 다양하기 때문에 매우 어렵다. **욕설을 탐지하는 일반적인 과제를 해결한다고 주장하는 연구는 일반적으로 훨씬 더 구체적인 것(specific)을 다루고 있다.** 이것은 종종 데이터 세트에서 식별될 수 있으며, 이는 특정 유형 및 욕설 대상에 대한 체계적인 편견을 포함할 수 있다. **예를 들어, Davidson 등의 데이터 세트는 일반적으로 욕설 콘텐츠 탐지로 설명되는 작업에 널리 사용되지만 인종차별과 성차별 쪽으로 크게 치우쳐 있다(Davidson 등, 2017).** \n\n\u003e Waseem et al. suggest that one of the main differences between subtasks is whether content is ‘directed towards a specific entity or is directed towards a generalized group’ (Waseem et al., 2017).\n- `Waseem et al.`은 하위 작업 간의 주요 차이점 중 하나는 콘텐츠가 '특정 엔티티를 지향하는지, 아니면 일반화된 그룹을 지향하는지'라고 제안한다(Wasee et al., 2017).\n\n\n\u003e A key distinction is whether abuse is explicit or implicit (Waseem et al., 2017; Zampieri et al., 2019).\n\n\n\u003e Some of the main problems are (1) researchers use terms which are not well-defined, (2) different concepts and terms are used across the field for similar work, and (3) the terms which are used are theoretically problematic.\n- 주요 문제 중 일부는 (1) 연구자들이 잘 정의되지 않은 용어를 사용하고, (2) 유사한 작업에 대해 현장 전반에 걸쳐 다른 개념과 용어를 사용하며, (3) 사용되는 용어가 이론적으로 문제가 있다는 것이다.\n\n\u003e **Annotation.** \n\u003e Annotation is a notoriously difficult task, reflected in the low levels of inter-annotator agreement reported by most publications, particularly on more complex multi-class tasks (Sanguinetti, Poletto, Bosco, Patti, \u0026 Stranisci, 2018). Noticeably, van Aken suggests that Davidson et al.’s widely used hate and offensive language dataset has up to 10% of its data mislabeled (van Aken et al., 2018).\n- 주석은 악명높게 어려운 작업으로, 특히 더 복잡한 다중 클래스 작업에 대해 대부분의 연구들에서 보고한 주석 간 합의(inter-annotator agreement)의 낮은 수준에 반영된다(Sanguinetti, Poletto, Bosco, Patti, \u0026 Stranisci, 2018). 독특하게,  **반 에이켄은 데이비드슨 등의 널리 사용되는 혐오 및 공격적인 언어 데이터 세트가 최대 10%의 데이터 레이블이 잘못 지정되었음을 시사한다(반 에이켄 외, 2018).**\n\n\u003e Few publications provide details of their annotation process or annotation guidelines. Providing such information is the norm in social scientific research and is viewed as an integral part of verifying others’ findings and robustness (Bucy \u0026 Holbert, 2013). In line with the recommendations of Sabou et al., we advocate that annotation guidelines and processes are shared where possible (Sabou, Bontcheva, Derczynski, \u0026 Scharl, 2014) and that the field also works to develop best practices.\n- 주석 프로세스 또는 주석 지침에 대한 자세한 내용을 제공하는 연구들은 거의 없다. **이러한 정보를 제공하는 것은 사회과학 연구의 표준이며**, 타인의 발견과 견고성을 검증하는 데 필수적인 부분으로 간주된다(Bucy \u0026 Holbert, 2013). **Sabou 등의 권고에 따라, 우리는 주석 지침과 프로세스가 가능한 곳에서 공유되고(Sabou, Bontcheva, Derczynski, \u0026 Scharl, 2014) 이 분야도 모범 사례를 개발하기 위해 노력해야 한다고 주장한다.**\n\n\u003e Ensuring that abusive content detection systems can be applied across different domains is one of the most difficult but also important frontiers in existing research. Thus far, efforts to address this has been unsuccessful. Burnap and Williams train systems on one type of hate speech (e.g. racism) and apply them to another (e.g. sexism) and find that performance drops considerably (Burnap \u0026 Williams, 2016)\n- 욕설 콘텐츠 탐지 시스템이 서로 다른 영역에 걸쳐 적용될 수 있도록 보장하는 것은 기존 연구에서 가장 어렵지만 중요한 분야 중 하나이다. 지금까지, 이것을 해결하려는 노력은 성공하지 못했다. **Burnap과 Williams는 한 유형의 혐오 발언(예: 인종차별)에 대해 시스템을 훈련시키고 다른 유형의 혐오 발언(예: 성차별)에 적용하며 성능이 상당히 떨어진다는 것을 발견한다(Burnap \u0026 Williams, 2016).**\n\n\n\u003e Karan and Šnajder use a simple methodology to show the huge differences in performance when applying classifiers on different datasets without domain-specific tuning (Karan \u0026 Šnajder, 2018). Noticeably, in the EVALITA hate speech detection shared task, participants were asked to (1) train and test a system on Twitter data, (2) on Facebook data and (3) to train on Twitter and test on Facebook (and vice versa). **Even the best performing teams reported their systems scored around 10 to 15 F1 points fewer on the cross-domain task.**\n- Karan과 Shnajder는 도메인별 튜닝 없이 서로 다른 데이터 세트에 분류기를 적용할 때 성능에서 큰 차이를 보여주기 위해 간단한 방법론을 사용한다(Karan \u0026 Shnajder, 2018). 눈에 띄게, EVALITA 혐오 발언 탐지 공유 작업에서, **참가자들에게 (1) 트위터 데이터에 대한 시스템 훈련 및 테스트, (2) 페이스북 데이터에 대한 시스템 훈련 및 테스트, (3) 트위터에 대한 훈련 및 페이스북에서 테스트(및 그 반대)를 요청하였다.** 최고의 성과를 거둔 팀들조차 그들의 시스템이 교차 도메인 작업에서 약 10-15의 F1 점수를 더 적게 받았다고 보고했다.\n\n***\n\n\u003e [!info] Reference  \n\u003e \n\u003e Bertie Vidgen, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott Hale, and Helen Margetts. 2019. [Challenges and frontiers in abusive content detection](https://aclanthology.org/W19-3509). In _Proceedings of the Third Workshop on Abusive Language Online_, pages 80–93, Florence, Italy. Association for Computational Linguistics.\n\n","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/paper-review/GLUCOSE":{"title":"GLUCOSE: GeneraLized and COntextualized Story Explanations","content":"\u003e[!info] Reference\n\u003eNasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon, David Buchanan, Lauren Berkowitz, Or Biran, and Jennifer Chu-Carroll. 2020. [GLUCOSE: GeneraLized and COntextualized Story Explanations](https://aclanthology.org/2020.emnlp-main.370). In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4569–4586, Online. Association for Computational Linguistics.\n\n이 논문은 Elemental Cognition이라는 AI기업 연구자들이 여럿 참여하였으며 2020 EMNLP 컨퍼런스에서 Honourable Mention Papers에 오른 논문이다. 블로그에 처음 포스팅하는 논문으로 이 논문을 정한 까닭은 **상식 추론 데이터셋** 을 상당히 흥미로운 방식으로 수집한 연구이기 때문이다. 인간의 인지 심리학에 영향을 받아 사건의 인과 관계를 10차원으로 정의하고, 이에 맞춰 상식 추론 데이터셋을 만들었다니...역시 EMNLP에는 아무나 투고하는 것이 아니다.  \n\n## Motivtion\n- 사람은 무언가를 읽거나 들을 때, **암시적인 상식 추론(implicit commonsense inferences)**을 만들어 무엇이 일어났고 왜 일어났는지를 이해한다.\n- 그러나 AI system은 reading comprehension이나 dialogue과 같은 태스크에 있어서 여전히 인간과 같은 상식 추론 능력을 보이지 못하고 있다.\n- 그 이유는...\n\t- ‘상식(commonsense knowledge)’을 대규모로 휙득할 방법이 없기 때문에\n\t- 그러한 지식들을 최신의 AI 시스템에 통합시킬 방법이 없기 때문에\n\n## Claims\n- GlUCOSE 상식 추론 프레임워크를 통해 이러한 보틀넥을 해결할 수 있다.\n- GLUCOSE 데이터셋은 사건의 인과 설명을 10가지 차원을 제공하며, 또한 구체적인 스토리와 일반화된 법칙을 제공한다.\n\n## Significance\n- 암시적인 상식(Commonsense knowledge)를 대규모로 수집할 방법을 수립했다.\n- 기존의 사전학습 언어모델을 GLUCOSE에 훈련시키면 처음 보는 이야기에도 인간과 비슷한 정도의 상식 추론이 가능해진다.\n\n  \n## Introduction\n\n다음같은 상황을 가정해보자.\n\n- 한 아이 앞에서 차가 방향을 틀었다.\n- 그 아이가 재빨리 자전거를 돌렸다.\n- 그 아이가 자전거에서 떨어졌다.\n- 그 아이의 무릎이 까졌다.\n\n이 이야기를 읽으며 사람은 문장간의 **'인과 추론'** 을 만들어낼 수 있다. 예를 들어,\n\n- '한 아이 앞에서 차가 방향을 틀었다.'\n- **그리고 그건** '그 아이가 재빨리 자전거를 돌렸다.'를 초래하고\n- **그리고 그건** '그 아이가 자전거에서 떨어졌다.' 를 초래하고\n- **그리고 그건** '그 아이의 무릎이 까졌다.'를 초래했다.\n\n이러한 방식으로 말이다. **사람은 이처럼 어떻게 이야기 속 특정한 사건이 특정한 결과를 이끌었는지 묘사하는 '인과적 사슬'을 만들 수 있다**\n\n그러나 AI system은 reading comprehension이나 dialogue과 같은 태스크에 있어서 여전히 인간과 같은 상식 추론 능력을 보이지 못하고 있다. 이는 두 가지 이유가 있는데, 첫째로 **암시적인 상식을 대규모로 획득할 길이 없으며**, 둘째로 그러한 지식을 **최신의 AI 시스템에 융합시킬 길이 없기** 때문이다.\n\n이러한 상황을 해결하고 나온 것이 바로 GLUCOSE이다. GLUCOSE라는 이름도 바로 이 프레임워크가 AI를 위해 할 수 있는 기능을 비유적으로 보여주고 있는데, 사람의 지식 활동이 뇌 속의 글루코스 용량에 따라 좌우되는 것처럼, AI 시스템이 기본적 사고를 할 수 있도록 하는 연료가 되라는 의미에서 글루코스라고 지었다고 한다. 좋은 논문은 역시 이름도 잘 짓는다.\n\n앞에서도 언급했지만, GLUCOSE 데이터는 아주 흥미로운 규칙으로 구축되었다. 논문의 표현을 가져오면 다음과 같다.\n  \n\u003e S라는 짧은 이야기의 X라는 선택된 문장이 주어지면, GLUCOSE는 X와 관련된, 인간의 인지 심리학에 영향을 받은 10가지 차원의 commonsense causal explanations을 정의한다.\n\n뿐만 아니라, GLUCOSE는 commonsense knowledge를 세상에 관한 ‘미니 이론’이라고 할 수 있는 **‘반정형 추론 법칙(semi-structured inference rules)'** 의 형태로 인코딩하고, 각각은 구체적인 이야기에 근거한다는 특징도 있다.","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/paper-review/Hate-speech-detection-and-racial-bias-mitigation-in-social-media-based-on-BERT-model":{"title":"Hate speech detection and racial bias mitigation in social media based on BERT model","content":"\n\u003e To define automated methods with a promising performance for hate speech detection in social media, Natural Language Processing (NLP) has been used jointly with classic Machine Learning (ML) [2–4] and Deep Learning (DL) techniques [6, 15, 16]. The majority of contributions in classic supervised machine learning-based methods, for hate speech detection, rely on different text mining-based features or user-based and platform-based metadata [4, 17, 18], which require them to define an applicable feature extraction method and prevent them to generalize their approach to new datasets and platforms. **However, recent advancements in deep neural networks and transfer learning approaches allow the research community to address these limitations.**\n- 소셜 미디어에서 혐오 발언 탐지를 위한 유망한 성능을 가진 자동화된 방법을 정의하기 위해, 자연어 처리(NLP)는 고전적인 머신 러닝(ML)[2–4] 및 딥 러닝(DL) 기술[6, 15, 16]과 함께 사용되어 왔다. 혐오 발언 감지를 위한 고전적인 지도 기계 학습 기반 방법의 기여의 대부분은 서로 다른 텍스트 마이닝 기반 features 또는 사용자 기반 및 플랫폼 기반 메타데이터[4, 17, 18]에 의존하며, 이는 해당 기능 추출 방법을 정의하고 새로운 데이터 세트 및 계획에 대한 접근 방식을 일반화하는 것을 어렵게 한다. **그러나 최근 심층 신경망과 전이 학습 접근법의 발전은 연구 커뮤니티가 이러한 한계를 해결할 수 있도록 한다.**\n- 자연어 처리 ← 머신 러닝 \u0026 딥러닝 ← 머신 러닝의 한계를 극복한 딥러닝\n- 이러한 순서가 자연스럽네. ","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/paper-review/Hate-speech-detection-is-not-as-easy-as-you-may-think":{"title":"Hate speech detection is not as easy as you may think","content":"\n\u003e[!info] Reference  \n\u003e\n\u003eAymé Arango, Jorge Pérez, and Barbara Poblete. 2019. Hate Speech Detection is Not as Easy as You May Think: A Closer Look at Model Validation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19). Association for Computing Machinery, New York, NY, USA, 45–54. https://doi.org/10.1145/3331184.3331262\n\n---\n\n## Introduction\n\n\u003e Despite the apparent difficulty of the hate speech detection problem evidenced by social-media providers, current state-of- the-art approaches reported in the literature show near-perfect performance. Within-dataset experiments on labeled hate speech datasets using supervised learning achieve F1 scores above 93% [3, 7–9]. Nevertheless, there are only a few studies towards determining how generalizable the resulting models are, beyond the data collection upon which they were built on, nor on the factors that may affect this property [10]. Furthermore, recent literature that surveys current work also views the state-of-the-art under a more conservative and cautious light [3,10].\n- 소셜 미디어 제공자가 증명하는 혐오 발언 탐지 문제의 명백한 어려움에도 불구하고, 문헌에 보고된 현재 최첨단 접근 방식은 거의 완벽한 성능을 보여준다. 지도 학습을 사용하여 레이블링된 혐오 발언 데이터 세트에 대한 데이터 세트 내 실험은 93% 이상의 F1 점수를 달성한다. \n- **그럼에도 불구하고 결과 모델이 구축된 데이터 수집을 넘어, 결과 모델의 일반화 정도를 결정하는 데에는 몇 가지 연구만이 있다.** 또한, 현재 작업을 조사하는 최근 문헌에서도 최첨단 기술을 보다 보수적이고 신중한 관점에서 바라본다.\n\n\u003e [!note] Aim of this paper  \n\u003e   \n\u003e \"(...) measure how these models would perform on similar yet different datasets.\"\n\n- 비슷하지만 다른 데이터셋. 거기에서 모델의 성능을 보는 것. 이게 핵심이다.\n\n## Related work\n\n\u003e **There is some recent work on testing the generalization of the state-of-the-art methods to other datasets and domains [8–10].** Most of this work has been focused on Deep Learning methods. \n\u003e Agrawal and Awekar [8] test the performance of models trained on tweets [11] classifying on Wikpedia data [33] and Formspring data [34]. The authors show that transfer learning from Twitter to the two other domains performs poorly achieving less than 10% F1. \n\u003e In a similar study, Dadvar and Eckert [9] perform transfer learning from Twitter to a dataset of Youtube comments [35] showing a performance of 15% F1. \n\u003e Gröndahl et al. [10] present a comprehensive study reproducing several state-of-the-art models. \n\u003e Specially important for us is the experiment transferring Badjatiya et al.’s model [7] trained on the Waseem and Hovy’s dataset [11] to two other similarly labeled tweet datasets [16,17]. Even in this case the performance drops significantly, obtaining 33% and 47% F1 in those sets. This is a 40+% drop from the 93% F1 reported by Badjatiya et al. [7]. \n\u003e From these results, **Gröndahl et al. [10] draw as a conclusion that model architecture is less important than the type of data and labeling criteria being used.** \n\u003e In this paper our results are coherent with those of Gröndahl et al. [10]. However, we take our research a step further by investigating why this issue occurs.\n- 훌륭한 개괄이다. 이렇게 써야 하는데. ","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/paper-review/InstructGPT":{"title":"InstructGPT","content":"The [OpenAI API is powered by GPT-3 language models](https://openai.com/blog/gpt-3-apps/) which can be coaxed to perform natural language tasks using carefully engineered text prompts. **But these models can also generate outputs that are untruthful, toxic, or reflect harmful sentiments.** This is in part because GPT-3 is trained to predict the next word on a large dataset of Internet text, rather than to safely perform the language task that the user wants. In other words, these models aren’t _aligned_ with their users.\n\n...\n\nTo train InstructGPT models, our core technique is [reinforcement learning from human feedback (RLHF)](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/), a method we helped pioneer in our earlier alignment research. **This technique uses human preferences as a reward signal to fine-tune our models**, which is important as the safety and alignment problems we are aiming to solve are complex and subjective, and aren’t fully captured by simple automatic metrics.\n\n출처: https://openai.com/blog/instruction-following/#birds-migrate\n\n---\n\n","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/paper-review/Language-Models-are-Few-Shot-Learners":{"title":"Language Models are Few-Shot Learners","content":"\n\u003e [!note] Goal  \n\u003e   \n\u003e 이 논문에서 저자들은 GPT-3 언어 모델을 사용하여 가설을 테스트하고 있다. GPT-3는 1,750억 개의 매개 변수를 가지고 있으며 문맥 정보를 사용하여 학습할 수 있다. 저자들은 이 모델을 사용하여 작업에 빠르게 적응할 수 있는 능력과 few shot 시나리오에서 학습에 대한 숙련도를 평가하고 있다.\n\n\u003e PLM을 이용한 Fine-tuning이 만능인가?\n\nThe approach of pre-trained language models has had significant success and has allowed for more efficient tasks, **but it requires having a task-specific dataset for fine-tuning.** This means that to get strong performance on any specific task, a dataset of a large number of examples related to that task is needed.\n\n\u003e 아니다, fine-tuning을 위한 데이터셋이 필요하다. 이러한 한계를 없애야하는 이유는 여러가지가 있다.\n\n- First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models.\n- 그리고 무엇보다, 사람은 그렇게 학습하지 않아. 사람에게 무언가를 요구할 때는 간단한 문장이면 충분하다. \n\n\u003e 그러면, 다른 방향이 있는가?\n\n- One potential route towards addressing these issues is **meta-learning**– which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1).\n- **Meta-learning** is a technique used to train a model in such a way that it can develop a set of skills and be able to recognize patterns quickly, which helps it in adapting to a specific task when it is being used. Basically, it prepares the model to become more efficient at a task by 'learning' from a variety of training experiences first.\n- = **In-context learning**.\n\t- In-context learning is a method of teaching a language model to complete a task from contextual information provided by natural language instructions and/or a few demonstrations of the task. It is possible that this method of learning will become more successful as larger language models are developed; that is, models with more parameters, or units processing the data. This means that with larger language models, the in-context learning abilities would improve in performance.\n\n## Term definition\n\n- **Fine-Tuning (FT)** has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. \n\t- The main advantage of fine-tuning is strong performance on many benchmarks. \n\t- The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [MPL19 ], and the potential to exploit spurious features of the training data [GSL+18 , NK19 ], potentially resulting in an unfair comparison with human performance. \n- **Few-Shot (FS)** is the term we will use in this work to refer to the setting where **the model is given a few demonstrations of the task at inference time** as conditioning [RWC+19 ], but **no weight updates** are allowed.\n\t- Basically, the model receives these examples, uses them to make a prediction, and then **doesn't get to adjust its internal parameters** to learn from its mistakes.\n- **One-Shot (1S)** is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that **it most closely matches the way in which some tasks are communicated to humans.**\n- **Zero-Shot (0S)** is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task.\n\n\u003e [!note] Tip  \n\u003e   \n\u003e When building a machine learning model, the batch size and learning rate are important factors that must be considered. Generally, for larger models, you can use a larger batch size - which means that more data is fed in at once during training - but a smaller learning rate, which dictates the pace at which the model learns.\n\n## GPT in reading comprehension\nOn DROP [DWD+19 ], a dataset testing discrete reasoning and numeracy in the context of reading comprehension, **GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline** from the original paper but is still well below both human performance (...)\n\n## Limitations\n- First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, **it still has notable weaknesses in text synthesis and several NLP tasks.** On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs.\n- A limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that **they are both expensive and inconvenient to perform inference on**, which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is **distillation** [HVD15 ] of large  models down to a manageable size for specific tasks.\n- Finally, GPT-3 shares some limitations common to most deep learning systems – **its decisions are not easily interpretable**, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on.","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/paper-review/LittleBird-Efficient-Faster-Longer-Transformer-for-QuestionAnswering":{"title":"LittleBird - Efficient Faster \u0026 Longer Transformer for QuestionAnswering","content":"\n\u003e [!warning] 선행 개념\n\u003e - sliding window attention from BigBird (Zaheer et al., 2020)\n\u003e - linear bias to attention from ALiBi (Press et al., 2021) \n\u003e - pack and unpack attention from LUNA (Ma et al., 2021)\n\n\u003e BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism.\n- BERT 모델의 단점: long inputs에서 한계가 있다.\n- 이를 보완하기 위해 `BigBird` 등이 제안되었으나, 충분치 않았고 그래서 이 연구 'LittleBird'를 제안한다. \n\n\u003e (...)LittleBird, a novel model based on BigBird with improved speed and memory footprint while maintaining accuracy.\n\n- **특징 1:** method for position representation\n\t- 기존의 `positional encoding` 방식은 long input에서 한계가 있었다.\n\t- 그래서, *we devise a new method based on the Attention with Linear Biases (`ALiBi`) that is  fast, flexible, and also effective in QA tasks.*\n- **특징 2:** method of capturing global information\n\t- We show that replacing them with modified `pack and unpack attention` (Ma et al., 2021) is more effective.\n- **특징 3:** efficient way to train a model for long sequences\n\t- `Padding Insertion`, which makes the model robust to long inputs while training on short inputs.\n\t- `distillation method`\n- **이 모델의 기여점:** This is a significant benefit for low-resource languages where large amounts of long text data are difficult to obtain.\n\n### Pretraining objectives for Question  Answering\n\nHowever, **Masked LM (MLM) is suboptimal for extractive QA task.** Joshi et al. (2020) proposed `SpanBERT`, which is pretrained by a span-level masking scheme whose lengths follows geometric distribution and **it outperformed BERT with MLM** in the most of tasks, especially extractive QA. They proved that training objective predicting spans rather than tokens generates better representations especially for span selection tasks.\n\n## LittleBird Architecture\n\n### Bidirectional ALiBi\n- (...) because **ALiBi was designed for causal language modeling, not autoencoding language modeling**, each query can attend to keys to the left of itself only, not keys further away or to the right in ALiBi.\n- Therefore, we devised `BiALiBi` (Bidirectional ALiBi), which is improved version of ALiBi to **suit the autoencoding language model**. BiALiBi has the same attention function as ALiBi, but differs only in the method of calculating the distance matrix.\n\n### Sliding window attention \u0026 Pack unpack attention\n- (...) we completely eliminated random attention from our model.\n- We also reduced the number of global tokens and removed global-local attention, they were replaced with pack and unpack attention.\n- To effectively replace random and global attention, we employed pack and unpack attention (Ma et al., 2021).\n\n## Conclusion and Limitations\n- We propose `LittleBird`, which is more efficient in terms of memory and computational time than existing Transformer models for long sequences, and its effective way to train. \n\t- It combines a novel position encoding method, BiALiBi, and pack \u0026 unpack with sliding window attention to achieve high speed and accuracy, particularly in question answering tasks for long documents. \n\t- The distillation and training method with Padding Insertion allows  the model to be trained by reusing the existing pre-trained language model for short inputs and work well for long inputs even if trained on short inputs.\n\n- (...) causal masking cannot be applied to the pack and unpack attention due to its characteristics, which means that **LittleBird cannot be used to the decoder-only autoregressive language model.**\n\n\u003e [!note] 참고  \n\u003e   \n\u003e Causal masking is a technique used to ensure that the decoder can only rely on the tokens earlier in the sequence when making its prediction. However, LittleBird uses a method called 'pack and unpack attention', which prevents causal masking from being used, meaning that LittleBird cannot be used to make predictions one token at a time (known as an autoregressive language model).\n\u003e An example of a decoder-only autoregressive model is a **machine translation system**, where the input text is encoded into an intermediate representation and then decoded one token at a time to generate the output.","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/paper-review/On-Cross-Dataset-Generalization-in-Automatic-Detection-of-Online-Abuse":{"title":"On Cross-Dataset Generalization in Automatic Detection of Online Abuse","content":"\n\u003e[!info] Reference  \n\u003e\n\u003eNejadgholi, I., \u0026 Kiritchenko, S. (2020). On cross-dataset generalization in automatic detection of online abuse. _arXiv preprint arXiv:2010.07414_.\n\n---\n\n## Research Questions\n\n\u003e Test and training sets were created for each dataset by performing a stratified split of 20% vs 80%, with the larger part used for training the models. The training sets were further subdivided, keeping 1/8 shares of them as separate validation sets during development and fine-tuning of the hyper-parameters.\n\n- Fine-tuning에서의 일반적인 방법을 말하고 있다. 전체 데이터를 label 분포를 유지한 채로 `train`, `test` 으로 나누고, 이후 `train`에서 `validation`을 다시 나눈다. 특히 `test` 데이터셋은 훈련에 쓰이지 않는데, 이후 학습한 모델의 일반화 성능을 평가할 때 사용한다. 그래서 `test` 데이터셋에서 성능이 잘 나온다면, 해당 모델이 다른 데이터셋에서도 성능이 잘 나올 것이라는 가설을 세울 수 있다.  \n- 그러나 본 논문은 이 가설에 의문을 제기한다. \n\n\u003e (...) the aim here, in contrast, was to see **how well the best models (that may have learnt some dataset-specific biases) performed on other datasets.** **This was done to investigate how well state-of-the-art systems perform in a real-life scenario**, i.e., when exposed to data from other domains, with the hypothesis that a model trained on one dataset that exhibits comparatively reasonable results on other datasets can be expected to generalise well.\n\n- 이 논문의 목적은 그렇게 한 데이터셋을 잘 학습한(아마도 그 데이터셋에 내재한 편향도 잘 학습한) 모델이 다른 데이터셋에 얼마나 성능이 좋은지 보는 것이다. 이건 실제 세계에서의 상황과 유사한데, 모델은 결국 다른 도메인에서 생성된 데이터에 노출될 수 밖에 없기 때문이다. \n- 이를 통해 *'한 데이터셋을 잘 학습하여 좋은 성능을 내는 모델이라면, 다른 데이터셋에도 잘 일반화를 할 수 있을 것*'이라는 가설을 실제로 확인해보는 것이다. \n\n\n## 실험 방법\n\n\u003e To explore how well the Toxic class from the Wiki-dataset generalizes to other types of offensive behaviour, **we train a binary classifier (Toxic vs. Normal) on the Wiki-dataset (combining the train, development and test sets) and test it on the Out-of-Domain Test set.** This classifier is expected to predict a positive (Toxic) label for the instances of classes Founta-Abusive, Founta-Hateful, Waseem-Sexism and Waseem-Racism, and a negative (Normal) label for the tweets in the Founta-Normal class. We fine-tune a BERT-based classifier (Devlin et al., 2019) with a linear prediction layer, the batch size of 16 and the learning rate of 2 × 10−5 for 2 epochs.\n\n- 저자들은 Wiki-dataset으로 훈련한 모델이 다른 데이터셋에 얼마나 잘 일반화하는가를 보기 위해, binary classifier를 wiki-Dataset으로 훈련시키고 *'도메인 외 테스트셋(the Out-of-Domain Test set)'* 에 이를 테스트 했다. 모델은 BERT를 사용했다.\n\n\n## 실험 결과\n\n![Table 3](notes/images/table3.png)\n\n\u003e Results: The overall performance of the classifier on the Out-of-Domain test set is quite high: weighted macro-averaged F1 = 0.90.\n- 저자들의 예상과 달리 전체적인 Out-of-Domain test 성능은 높은 편이었다. 그러나 Waseem 데이터셋의 Sexist, Racist class를 분류하는 데에는 Wiki-Dataset의 Toxic class로 훈련된 모델이 적합하지 않다는 사실을 확인했다. \n\n### Formulation에 대한 논의 \n#key-observation\n\u003e The impact of task formulation: From task formulations described in Section 3, observe that the Wiki-dataset defines the class Toxic in a general way. The class Founta-Abusive is also a general formulation of offensive behaviour. The similarity of these two definitions is reflected clearly in our results.\n- 흥미로운 분석은 Formulation에 대한 것이다. 먼저 Wiki dataset의 Tosic class에 대한 정의는 다음과 같다 : 'The class Toxic comprises rude, hateful, aggressive, disrespectful or unreasonable comments that are likely to make a person leave a conversation'.\n- 그런데 이것이, Waseem 데이터셋의 Sexist, Racist class를 분류하기에는 다소 일반적인 정의라는 것이다. \n\n## Impact of Data Size on Generalizability\n#data-size\n\n\u003e Observe that the average accuracies remain unchanged when the dataset’s size triples at the same class balance ratio. This finding contrasts with the general assumption that more training data results in a higher classification performance.\n\n- 다음으로 저자는 또 흥미로운 포인트를 하나 더 확인했다. \n- 만약 class의 비율이 변하지 않는다면 데이터의 크기가 커지더라도 정확도(`accuracy`)는 변하지 않는다는 것이다. 이는 더 많은 훈련데이터가 항상 높은 분류 성능을 낸다는 general assumption과 반대되는 결과이다.\n\n## Discussion\n\n\u003e In the task of online abuse detection, both False Positive and False Negative errors can lead to significant harm as one threatens the freedom of speech and ruins people’s reputations, and the other ignores hurtful behaviour.\n- False Positive와 False Negative는 표현의 자유를 위협할 수 있다. \n\n\u003e We suggest evaluating each class (both positive and negative) separately taking into account the potential costs of different types of errors.\n- 그리고 저자들은 각 class를 따로 평가하는 것을 제안했다. ","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/paper-review/Studying-Generalisability-Across-Abusive-Language-Detection-Datasets":{"title":"Studying Generalisability Across Abusive Language Detection Datasets","content":"\n\u003e [!note] Reference  \n\u003e \n\u003e Steve Durairaj Swamy, Anupam Jamatia, and Björn Gambäck. 2019. [Studying Generalisability across Abusive Language Detection Datasets](https://aclanthology.org/K19-1088). In _Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)_, pages 940–950, Hong Kong, China. Association for Computational Linguistics.\n\n## Previous Work\n\n\u003e Abusive language detection has served as an umbrella term for a wide variety of subtasks. Research in the field has typically focused on a particular subtask: Hate Speech (Davidson et al., 2017; Founta et al., 2018; Gao and Huang, 2017; Golbeck et al., 2017), Sexism/Racism (Waseem and Hovy, 2016), Cyberbullying (Xu et al., 2012; Dadvar et al., 2013), Trolling and Aggression (Kumar et al., 2018a), and so on. \n\u003e Datasets for these tasks have been collected from various social media platforms, such as Twitter (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018; Burnap and Williams, 2015; Golbeck et al., 2017), Facebook (Kumar et al., 2018a), Instagram (Hosseinmardi et al., 2015; Zhong et al., 2016), Yahoo! (Nobata et al., 2016; Djuric et al., 2015; Warner and Hirschberg, 2012), YouTube (Dinakar et al., 2011), and Wikipedia (Wulczyn et al., 2017), with annotation typically carried out on crowdsourcing platforms such as CrowdFlower (Figure Eight)1 and Amazon Mechanical Turk.\n- Abusive language detection에 대한 해외 연구 레퍼런스가 잘 설명되어 있다.\n\n\u003e In the ‘OffensEval’ shared task (Zampieri et al., 2019b), the use of contextual embeddings such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) exhibited the best results.\n- BERT가 등장하고 탐지 연구에 쓰이기 시작. \n\n#key-observation \n\u003e Generalisability of a model has also come under considerable scrutiny. Works such as Karan and Šnajder (2018) and Gröndahl et al. (2018) have shown that **models trained on one dataset tend to perform well only when tested on the same dataset.**\n- 이 부분이 내 연구의 핵심과 관련이 깊다. \n\n\u003e Additionally, Gröndahl et al. (2018) showed how adversarial methods such as typos and word changes could bypass existing state-of- the-art abusive language detection systems.\n- \"All you need is “love”: Evading hate speech detection\" 논문 저자이다. \n\n\u003e Fortuna et al. (2018) concurred, stating that although models perform better on the data they are trained on, slightly improved performance can be obtained when adding more training data from other social media.\n- Fortuna et al. (2018) 연구도 있었구나. 다른 도메인에서 수집한 데이터를 더 추가해 훈련시키면 더 좋은 성능을 얻게 된다고 했었네. 지금의 일반화 가능성에 대한 논의가 여기에서 출발했다.\n\n## Preliminary Feature and Model Study\n\n\u003e However, fine-tuning was carried out on the mod- els’ hyper-parameters, such as sequence length, drop out, and class weights. Test and training sets were created for each dataset by performing a stratified split of 20% vs 80%, with the larger part used for training the models. The training sets were further subdivided, keeping 1/8 shares of them as separate validation sets during devel- opment and fine-tuning of the hyper-parameters.\n- **그러나 시퀀스 길이, 드롭아웃 및 클래스 가중치와 같은 모델의 초 매개 변수에 대해 미세 조정이 수행되었다. 테스트 및 훈련 세트는 각 데이터 세트에 대해 20% 대 80%의 계층화된 분할을 수행하였고, 이후 더 큰 부분을 모델 훈련에 사용하였다.**  훈련 세트는 더욱 세분화되어 하이퍼 파라미터의 개발 및 미세 조정 중에 1/8 공유를 별도의 검증 세트로 유지하였다.\n- 모델 훈련에 대해서는 이렇게 설명하면 되겠다.\n\n\u003e The best models used a learning rate of e−5 and batch size 32 with varying maximum sequence lengths between 60 and 70. Other parameters worth mentioning are the number of epochs and the Linear Warm-up Proportion.\n- 모델 하이퍼파라미터의 경우에는 이렇게 표현하면 된다.\n- 어라, 그런데 이거 보다보니 [On Cross-Dataset Generalization in Automatic Detection of Online Abuse](notes/paper-review/On%20Cross-Dataset%20Generalization%20in%20Automatic%20Detection%20of%20Online%20Abuse.md) 이랑 설명이 똑같다. \n\n![Table 1: Overview of the datasets by Davidson et al., Founta et al., Waseem and Hovy, and Zampieri et al.](Datasets-overview.png)\n- 전체 데이터셋의 개요는 위 그림과 같다. \n\n## Cross-Dataset Training and Testing\n\n![Table 4: Cross-dataset test results (accuracy and macro-F1)](Cross-dataset-test-results.png)\n\u003e Considerable performance drops can be observed when going from a large training dataset to a small test set (i.e., Founta et al.’s results when tested on the Waseem and Hovy dataset) and vice versa. This is in line with a similar conclusion by Karan and Šnajder(2018). \n- 큰 데이터셋부터 작은 데이터셋으로 갈 때 퍼포먼스 하락이 보인다. \n\n\u003e The most interesting observation is that **datasets with larger percentages of positive samples tend to** **generalise better than datasets with fewer positive samples**, in particular when **tested against dissimilar datasets**. For example, we see that the models trained on the Davidson et al. dataset, which contains a majority of offensive tags, perform well when tested on the Founta et al. dataset, which contains a majority of non-offensive tags.\n- 가장 흥미로운 관찰은 양성 샘플의 비율이 큰 데이터 세트가 특히 다른 데이터 세트에 대해 테스트할 때 양성 샘플이 적은 데이터 세트보다 더 잘 일반화되는 경향이 있다는 것이다. 예를 들어, 대다수의 공격 태그를 포함하는 Davidson 등 데이터 세트에 대해 훈련된 모델은 대다수의 비공격 태그를 포함하는 Founta 등 데이터 세트에서 테스트될 때 성능이 우수하다는 것을 알 수 있다.\n- 내 연구도 이런 것이 있나 확인해보자. \n\n## Discussion and Conclusion\n\n\u003e Second, experiments showing that datasets with larger percentages of positive samples generalise better than datasets with fewer positive samples when tested against a dissimilar dataset (at least within the same platform, e.g., Twitter), which indicates that a more balanced dataset is healthier for generalisation.\n\n\u003e **An overall conclusion is that the data is more important than the model when tackling Abusive Language Detection.**\n- 데이터, 데이터! 결국은 데이터의 중요성을 말하며 이 논문은 끝을 낸다. ","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/paper-review/Towards-generalisable-hate-speech-detection":{"title":"Towards generalisable hate speech detection","content":"\n\u003e [!info] Reference  \n\u003e \n\u003e Yin, W., \u0026 Zubiaga, A. (2021). Towards generalisable hate speech detection: a review on obstacles and solutions. _PeerJ Computer Science_, _7_, e598.\n\n---\n\n## Generalisation\n\n\u003e Most if not all proposed hate speech detection models rely on supervised machine learning methods, where the ultimate purpose is for the model to learn the real relationship between features and predictions through training data, which generalises to previously unobserved inputs (Goodfellow, Bengio \u0026 Courville, 2016). The generalisation performance of a model measures how well it fulfils this purpose.\n- 제안된 혐오 발언 탐지 모델은 대부분 supervised 기계 학습 방법에 의존하며, 궁극적인 목적은 모델이 이전에 관찰되지 않은 입력으로 일반화하는 훈련 데이터를 통해 기능과 예측 사이의 실제 관계를 학습하는 것이다(Goodfellow, Bengio \u0026 Courville, 2016). 모델의 일반화 성과는 이 목적을 얼마나 잘 달성하는지 측정한다.\n\n\u003e The ultimate purpose of studying automatic hate speech detection is to facilitate the alleviation of the harms brought by online hate speech. To fulfil this purpose, hate speech detection models need to be able to deal with the constant growth and evolution of hate speech, regardless of its form, target, and speaker.\n- 자동 혐오표현 탐지를 연구하는 궁극적인 목적은 온라인 혐오표현이 가져오는 해악의 완화를 용이하게 하는 것이다. 이러한 목적을 달성하기 위해, 혐오표현 탐지 모델은 형태, 대상 및 화자에 관계없이 혐오 발언의 지속적인 성장과 진화를 처리할 수 있어야 한다.\n\n#key-observation \n\u003eRecent research has raised concerns on the generalisability of existing models (Swamy, Jamatia \u0026 Gambäck, 2019). Despite their impressive performance on their respective test sets, **the performance significantly dropped when the models are applied to a different hate speech dataset.** This means that the assumption that test data of existing datasets represent the distribution of future cases is not true, and that **the generalisation performance of existing models have been severely overestimated** (Arango, Prez \u0026 Poblete, 2020). This lack of generalisability undermines the practical value of these hate speech detection models.\n- 최근 연구는 기존 모델의 일반화 가능성에 대한 우려를 제기했다(Swamy, Jamatia \u0026 Gambeck, 2019 : [Studying Generalisability Across Abusive Language Detection Datasets](notes/paper-review/Studying%20Generalisability%20Across%20Abusive%20Language%20Detection%20Datasets.md)).  **각각의 테스트 세트에서 인상적인 성능에도 불구하고 모델이 다른 혐오 음성 데이터 세트에 적용될 때 성능이 크게 떨어졌다.** 이는 기존 데이터 세트의 테스트 데이터가 미래의 사례 분포를 나타낸다는 가정이 사실이 아니며, **기존 모델의 일반화 성능이 심각하게 과대 평가되었다는 것을 의미한다**(Arango, Pres \u0026 Poblete, 2020 : [Hate speech detection is not as easy as you may think](notes/paper-review/Hate%20speech%20detection%20is%20not%20as%20easy%20as%20you%20may%20think.md)). 이러한 일반성의 부족은 이러한 혐오표현 탐지 모델의 실질적인 가치를 훼손한다.\n\n\u003e[!note] Note  \n\u003e\n\u003e이 부분이 내가 하고 있는 연구의 핵심이다! 모델의 일반화 성능이 과대평가되어 있다는 것. 한국어 데이터셋과 모델로도 비슷한 결과가 나오는지 보는 것.\n\n## Data\n\n\u003e [예시 1]\n\u003e For example, in Wiegand, Ruppenhofer \u0026 Kleinbauer (2019)’s study, FastText models (Joulin et al., 2017a) trained on three datasets (Kaggle, Founta, Razavi) achieved F1 scores above 70 when tested on one another, **while models trained or tested on datasets outside this group achieved around 60 or less**.\n- 모델이 훈련된 것과 다른 데이터셋에서는 모델의 성능이 떨어진다는 결과가 있다.\n\n#key-observation \n\u003e Founta and OLID produced models that performed well on each other. The source of such differences are usually traced back to search terms (Swamy, Jamatia \u0026 Gambäck, 2019), topics covered (Nejadgholi \u0026 Kiritchenko, 2020; Pamungkas, Basile \u0026 Patti, 2020), label definitions (Pamungkas \u0026 Patti, 2019; Pamungkas, Basile \u0026 Patti, 2020; Fortuna, Soler-Company \u0026 Wanner, 2021), and data source platforms (Glavaš, Karan \u0026 Vulić, 2020; Karan \u0026 Šnajder, 2018).\n- 서로 테스트 성능이 잘 나오는 데이터셋은 그 근원을 따라가보면 알 수 있는 사실이 있다. 예를 들면,` Founta`와 `OLID` 데이터셋은 서로 비슷한 데이터를 공유하고 있다. \n\n\u003e Fortuna, Soler \u0026 Wanner (2020) used averaged word embeddings (Bojanowski et al., 2017; Mikolov et al., 2018) to compute the representations of classes from different datasets, and compared classes across datasets. **One of their observations is that Davidson’s ‘‘hate speech’’ is very different from Waseem’s ‘‘hate speech’’, ‘‘racism’’, ‘‘sexism’’, while being relatively close to HatEval’s ‘‘hate speech’’ and Kaggle’s ‘‘identity hate’’.** This echoes with experiments that showed poor generalisation of models from Waseem to HatEval (Arango, Prez \u0026 Poblete, 2020) and between Davidson and Waseem (Waseem, Thorne \u0026 Bingel, 2018; Gröndahl et al., 2018).\n- 혐오표현 데이터셋에서 자주 나오는 단어들인 'hate speech', 'racism', 'sexism' 등도 워드 임베딩을 통해 살펴보니 데이터셋마다 그 의미가 다르다는 관찰이 나왔다. 이는 당연히 모델 성능의 일반화에도 악영향을 끼쳤고 말이다. \n\n\u003e In terms of what properties of a dataset lead to more generalisable models, there are frequently mentioned factors (...)\n\n\u003e Biases in the samples are also frequently mentioned. **Wiegand, Ruppenhofer \u0026 Kleinbauer (2019) hold that less biased sampling approaches produce more generalisable models.** This was later reproduced by Razo \u0026 Kübler (2020) and also helps explain their results with the two datasets that have the least positive cases. Similarly, Pamungkas \u0026 Patti (2019) mentioned that a wider coverage of phenomena lead to more generalisable models. \n- `Wiegand, Ruppenhofer \u0026 Kleinbauer (2019)` 연구에서 언급한 바와 같이, 조금이라도 더 일반화가 잘 되는 모델을 만드려면 sampling을 덜 치우치게 해주어야 한다. 훈련시 `sampler`를 잘 만들어야겠다.\n\n\u003e Another way of looking at generalisation and similarity is by comparing differences between individual classes across datasets (Nejadgholi \u0026 Kiritchenko, 2020; Fortuna, Soler \u0026 Wanner, 2020; Fortuna, Soler-Company \u0026 Wanner, 2021), as opposed to comparing datasets as a whole.\n- 이 논문에서도 class 개별로 비교하라고 주장하는구나. [On Cross-Dataset Generalization in Automatic Detection of Online Abuse](notes/paper-review/On%20Cross-Dataset%20Generalization%20in%20Automatic%20Detection%20of%20Online%20Abuse.md) 에서 주장하는 것과 맞물린다. \n\n## OBSTACLES TO GENERALISABLE HATE SPEECH DETECTION\n\n\u003e Hate speech detection, which is largely focused on social media, shares similar challenges to other social media tasks and has its specific ones, **when it comes to the grammar and vocabulary used.** Such user language style introduces challenges to generalisability at the data source, mainly by making it difficult to utilise common NLP pre-training approaches.\n- 혐오표현 탐지는 문법이나 어휘 관련해서 어려움이 많다는 특징이 있다. \n\n\u003e On social media, syntax use is generally more casual, such as the omission of punctuation (Blodgett \u0026 O’Connor, 2017). Alternative spelling and expressions are also used in dialects (Blodgett \u0026 O’Connor, 2017), to save space, and to provide emotional emphasis (Baziotis, Pelekis \u0026 Doulkeridis, 2017). Sanguinetti et al. (2020) provided extensive guidelines for studying such phenomena syntactically.\n- 그래서 혐오표현 탐지 연구를 할 때는 KcELECTRA가 그나마 괜찮겠구나. 이러한 케이스가 많은 데이터로 사전학습된 모델이니까. 실제로 성능도 가장 괜찮고.\n\n\u003e Qian et al. (2018) found that rare words and implicit expressions are the two main causes of false negatives; Van Aken et al. (2018) compared several models that used pre-trained word embeddings, and found that rare and unknown words were present in 30% of the false negatives of Wikipedia data and 43% of Twitter data.\n- 또한 rare words, implicit expressions는 false negatives를 증가시킨다. 이는 따라서 하나의 도메인에서만 수집한 데이터셋이 가지는 한계일 수 밖에 없겠다. 이를 극복하려면 여러 도메인에서 데이터를 수집해야겠네.\n\n\u003eIndeed, BERT (Devlin et al., 2019) and its variants have demonstrated top performances at hate or abusive speech detection challenges recently (Liu, Li \u0026 Zou, 2019; Mishra \u0026 Mishra, 2019).\n- BERT 계열 모델은 혐오표현 탐지에서도 여전히 top 퍼포먼스를 보인다. \n\n\u003eIt is particularly challenging to acquire labelled data for hate speech detection as knowledge or relevant training is required of the annotators. As a high-level and abstract concept, the judgement of ‘‘hate speech’’ is subjective, needing extra care when processing annotations. Hence, datasets are usually not big in size.\n- 혐오표현 데이터셋은 '혐오표현'을 정의하는 것 자체가 주관적이기 때문에, 주석 처리에 추가적인 힘이 들고 따라서 큰 사이즈로 만들어지기 어렵다.\n\n\u003e Moreover, **different studies are based on varying definitions of ‘‘hate speech’’, as seen in different annotation guidelines** (Table 5). Despite all covering the same two main aspects (directly attack or promote hate towards), datasets vary by their wording, what they consider a target (any group, minority groups, specific minority groups), and their clarifications on edge cases.\n\u003e Davidson and HatEval both distinguished ‘‘hate speech’’ from ‘‘offensive language’’, while ‘‘uses a sexist or racist slur’’ is in Waseem’s guidelines to mark a case positive of hate, **blurring the boundary of offensive and hateful.**\n\u003e Additionally, as both HatEval and Waseem specified the types of hate (towards women and immigrants; racism and sexism), hate speech that fell outside of these specific types were not included in the positive classes, while Founta and Davidson included any type of hate speech.\n- 또한, 다른 주석 지침(표 5)에서 볼 수 있듯이, 다양한 연구는 \"혐오 발언\"의 다양한 정의를 기반으로 한다. 모든 것이 동일한 두 가지 주요 측면을 포함함에도 불구하고 데이터 세트는 표현, 대상으로 간주하는 것(모든 그룹, 소수 그룹, 특정 소수 그룹) 및 엣지 사례에 대한 명확화에 따라 다르다. \n- Davidson과 HatEval은 모두 \"hate speech\"와 \"offensive language\"를 구분했으며, \"성차별적 또는 인종차별적 비방 사용\"은 Waseem의 가이드라인에 hate로 표시하여 offensive와 hate의 경계를 모호하게 한다. \n- 또한, HatEval과 Waseem이 혐오의 유형(여성과 이민자에 대한 것; 인종 차별과 성차별)을 명시함에 따라, 이러한 특정 유형에서 벗어난 혐오 발언은 긍정적인 등급에 포함되지 않았고, 반면 Fonta와 Davidson은 모든 유형의 혐오 발언을 포함시켰다.\n\n","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/talks/ChatGPT-Is-a-Blurry-JPEG-of-the-Web":{"title":"ChatGPT Is a Blurry JPEG of the Web","content":"...\n\nIt retains much of the information on the Web, in the same way that a retains much of the information of a higher-resolution image, but, if you’re looking for an exact sequence of bits, you won’t find it; **all you will ever get is an approximation.** But, because the approximation is presented in the form of grammatical text, which ChatGPT excels at creating, it’s usually acceptable. **You’re still looking at a blurry , but the blurriness occurs in a way that doesn’t make the picture as a whole look less sharp.**\n\n...\n\nLarge language models identify statistical regularities in text. Any analysis of the text of the Web will reveal that phrases like “supply is low” often appear in close proximity to phrases like “prices rise.” A chatbot that incorporates this correlation might, when asked a question about the effect of supply shortages, respond with an answer about prices increasing. **If a large language model has compiled a vast number of correlations between economic terms—so many that it can offer plausible responses to a wide variety of questions—should we say that it actually understands economic theory?** Models like ChatGPT aren’t eligible for the Hutter Prize for a variety of reasons, one of which is that **they don’t reconstruct the original text precisely—i.e., they don’t perform lossless compression.** But is it possible that their lossy compression nonetheless indicates real understanding of the sort that A.I. researchers are interested in?\n\n...\n\nImagine what it would look like if ChatGPT were a lossless algorithm. If that were the case, it would always answer questions by providing a verbatim quote from a relevant Web page. We would probably regard the software as only a slight improvement over a conventional search engine, and be less impressed by it. The fact that ChatGPT rephrases material from the Web instead of quoting it word for word makes it seem like a student expressing ideas in her own words, rather than simply regurgitating what she’s read; it creates the illusion that ChatGPT understands the material. **In human students, rote memorization isn’t an indicator of genuine learning, so ChatGPT’s inability to produce exact quotes from Web pages is precisely what makes us think that it has learned something. When we’re dealing with sequences of words, lossy compression looks smarter than lossless compression.**\n\n...\n\nEven if it is possible to restrict large language models from engaging in fabrication, should we use them to generate Web content? This would make sense only if our goal is to repackage information that’s already available on the Web. Some companies exist to do just that—we usually call them content mills. Perhaps the blurriness of large language models will be useful to them, as a way of avoiding copyright infringement. Generally speaking, though, I’d say that anything that’s good for content mills is not good for people searching for information. **The rise of this type of repackaging is what makes it harder for us to find what we’re looking for online right now; the more that text generated by large language models gets published on the Web, the more the Web becomes a blurrier version of itself.**\n\nThere is very little information available about OpenAI’s forthcoming successor to ChatGPT, GPT-4. But I’m going to make a prediction: when assembling the vast amount of text used to train GPT-4, the people at OpenAI will have made every effort to exclude material generated by ChatGPT or any other large language model. If this turns out to be the case, it will serve as unintentional conrmation that the analogy between large language models and lossy compression is useful. Repeatedly resaving a creates more compression artifacts, because more information is lost every time. **It’s the digital equivalent of repeatedly making photocopies of photocopies in the old days. The image quality only gets worse.**\n\u003e 시뮬라크르(simulacre)가 떠오르는 문단이다. 시뮬라크르의 개념은 플라톤까지 거슬러 올라 가게 되는데 플라톤에 의하면 사람이 살고 있는 **이 세계는 원형인 '이데아', 복제물인 '현실', 복제의 복제물인 '세계'로 이루어져 있다.** 여기서 시뮬라크르는 **복제물을 다시 복제한 것을 말한다**. \n\nIndeed, a useful criterion for gauging a large language model’s quality might be the willingness of a company to use the text that it generates as training material for a new model. If the output of ChatGPT isn’t good enough for GPT-4, we might take that as an indicator that it’s not good enough for us, either. Conversely, if a model starts generating text so good that it can be used to train new models, then that should give us condence in the quality of that text. (I suspect that such an outcome would require a major breakthrough in the techniques used to build these models.) **If and when we start seeing models producing output that’s as good as their input, then the analogy of lossy compression will no longer be applicable.**\n\u003e 이 문단에서도 논증되듯, LLM의 아웃풋을 다시 LLM을 학습시키는 데에 사용하지 못한다는 것이 바로 아웃풋이 시뮬라크르라는 반증이라고 생각한다. 그럼 이데아는 무엇일까? 실제 사람들이 작성하는 글이 이데아일까? **시뮬라크르는 이데아가 될 수 있을까?**\n\n...\n\nThere’s nothing magical or mystical about writing, but it involves more than placing an existing document on an unreliable photocopier and pressing the Print button. It’s possible that, in the future, we will build an A.I. that is capable of writing good prose based on nothing but its own experience of the world. The day we achieve that will be momentous indeed—but that day lies far beyond our prediction horizon. In the meantime, it’s reasonable to ask, What use is there in having something that rephrases the Web? If we were losing our access to the Internet forever and had to store a copy on a private server with limited space, a large language model like ChatGPT might be a good solution, assuming that it could be kept from fabricating. But we aren’t losing our access to the Internet. So just how much use is a blurry , when you still have the original?\n\u003e **원본(Web; idea)이 있는데 왜 복제품(ChatGPT; simulacre)을 봐야 하는가?**","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null},"/notes/talks/What-Is-ChatGPT-Doing-and-Why-Does-It-Work":{"title":"What Is ChatGPT Doing … and Why Does It Work?","content":"","lastmodified":"2023-03-14T08:55:46.511505579Z","tags":null}}