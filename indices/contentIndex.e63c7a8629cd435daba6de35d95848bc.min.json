{"/":{"title":"🌱 Hayul's digital garden","content":"\n안녕하세요?  \n저의 개발 블로그, **'Hayul's digital garden'** 입니다.  \n아래 **Contents**를 통해 포스팅을 탐색할 수 있습니다. \n\n## Contents\n\n###  [👩‍💻 Coding Test](notes/coding-test.md)\n###  [📑 Paper Review](notes/paper-review.md)\n### [⚙️ Algorithms](notes/Algorithms.md)\n### [🦾 Machine Learning](notes/Machine%20Learning.md)","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/%E1%84%90%E1%85%A1%E1%84%80%E1%85%A6%E1%86%BA-%E1%84%82%E1%85%A5%E1%86%B7%E1%84%87%E1%85%A5":{"title":"타겟 넘버","content":"\nn개의 음이 아닌 정수들이 있습니다. 이 정수들을 순서를 바꾸지 않고 적절히 더하거나 빼서 타겟 넘버를 만들려고 합니다. 예를 들어 [1, 1, 1, 1, 1]로 숫자 3을 만들려면 다음 다섯 방법을 쓸 수 있습니다.\n\n```\n-1+1+1+1+1 = 3\n+1-1+1+1+1 = 3\n+1+1-1+1+1 = 3\n+1+1+1-1+1 = 3\n+1+1+1+1-1 = 3\n```\n\n사용할 수 있는 숫자가 담긴 배열 numbers, 타겟 넘버 target이 매개변수로 주어질 때 숫자를 적절히 더하고 빼서 타겟 넘버를 만드는 방법의 수를 return 하도록 solution 함수를 작성해주세요.\n\n### 제한사항\n\n-   주어지는 숫자의 개수는 2개 이상 20개 이하입니다.\n-   각 숫자는 1 이상 50 이하인 자연수입니다.\n-   타겟 넘버는 1 이상 1000 이하인 자연수입니다.\n\n### 입출력 예\n\n| numbers         | target | return |\n| --------------- | ------ | ------ |\n| [1, 1, 1, 1, 1] | 3      | 5      |\n| [4, 1, 2, 1]                |      4  |  2      |\n\n### 입출력 예 설명\n\n**입출력 예 #1**\n\n문제 예시와 같습니다.\n\n**입출력 예 #2**\n\n```\n+4+1-2+1 = 4\n+4-1+2-1 = 4\n```\n\n-   총 2가지 방법이 있으므로, 2를 return 합니다.\n\n---\n## 다른 사람 풀이\n\n```python\nfrom collections import deque\n\ndef solution(numbers, target):\n    answer = 0\n    queue = collections.deque([(0, 0)])\n    while queue:\n        current_sum, num_idx = queue.popleft()\n\n        if num_idx == len(numbers):\n            if current_sum == target:\n                answer += 1\n        else:\n            number = numbers[num_idx]\n            queue.append((current_sum + number, \n            num_idx + 1))\n            queue.append((current_sum - number, \n            num_idx + 1))\n\n    return answer\n```\n\n\u003e [!note] Note  \n\u003e   \n\u003e 이 문제는 풀지 못했다...\n\u003e [DFS, BFS](notes/DFS,%20BFS.md) 알고리즘을 제대로 알고 있으면 풀 수 있었겠지만, 나는 이론만 알고 있지 실제로 사용할 능력을 못 되었다. \n\u003e 알고리즘 공부 제대로 하자. \n\n","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EA%B5%AC%EB%AA%85%EB%B3%B4%ED%8A%B8":{"title":"구명보트","content":"무인도에 갇힌 사람들을 구명보트를 이용하여 구출하려고 합니다. 구명보트는 작아서 한 번에 최대 **2명**씩 밖에 탈 수 없고, 무게 제한도 있습니다.\n\n예를 들어, 사람들의 몸무게가 [70kg, 50kg, 80kg, 50kg]이고 구명보트의 무게 제한이 100kg이라면 2번째 사람과 4번째 사람은 같이 탈 수 있지만 1번째 사람과 3번째 사람의 무게의 합은 150kg이므로 구명보트의 무게 제한을 초과하여 같이 탈 수 없습니다.\n\n구명보트를 최대한 적게 사용하여 모든 사람을 구출하려고 합니다.\n\n사람들의 몸무게를 담은 배열 people과 구명보트의 무게 제한 limit가 매개변수로 주어질 때, 모든 사람을 구출하기 위해 필요한 구명보트 개수의 최솟값을 return 하도록 solution 함수를 작성해주세요.\n\n## 제한사항\n\n-   무인도에 갇힌 사람은 1명 이상 50,000명 이하입니다.\n-   각 사람의 몸무게는 40kg 이상 240kg 이하입니다.\n-   구명보트의 무게 제한은 40kg 이상 240kg 이하입니다.\n-   구명보트의 무게 제한은 항상 사람들의 몸무게 중 최댓값보다 크게 주어지므로 사람들을 구출할 수 없는 경우는 없습니다.\n\n## 입출력 예\n\n| people           | limit | return |\n| ---------------- | ----- | ------ |\n| [70, 50, 80, 50] | 100   | 3      |\n| [70, 80, 50]     | 100   | 3      |\n\n\n## 나의 풀이\n\n```python\ndef solution(people, limit) :\n    gone = 0\n    people.sort()\n\n    a = 0\n    b = len(people) - 1\n    \n    while a \u003c b :\n        if people[b] + people[a] \u003c= limit :\n            a += 1\n            gone += 1\n        b -= 1\n    \n    answer = len(people) - gone\n    \n    return answer\n```\n\n이 문제에서 사용된  [Two Pointers](notes/Two%20Pointers.md) 알고리즘은 1차원 배열에서 두 개의 포인터를 조작하여 기존의 방식보다 빠르게 원하는 결과를 얻어내는 알고리즘이다.\n","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EA%B8%B0%EB%8A%A5%EA%B0%9C%EB%B0%9C":{"title":"기능개발","content":"\n-  프로그래머스 팀에서는 기능 개선 작업을 수행 중입니다. 각 기능은 진도가 100%일 때 서비스에 반영할 수 있습니다.  \n- 또, 각 기능의 개발속도는 모두 다르기 때문에 뒤에 있는 기능이 앞에 있는 기능보다 먼저 개발될 수 있고, 이때 뒤에 있는 기능은 앞에 있는 기능이 배포될 때 함께 배포됩니다.  \n- 먼저 배포되어야 하는 순서대로 작업의 진도가 적힌 정수 배열 progresses와 각 작업의 개발 속도가 적힌 정수 배열 speeds가 주어질 때 각 배포마다 몇 개의 기능이 배포되는지를 `return` 하도록 `solution` 함수를 완성하세요.  \n\n## 제한 사항\n\n-   작업의 개수(progresses, speeds배열의 길이)는 100개 이하입니다.\n-   작업 진도는 100 미만의 자연수입니다.\n-   작업 속도는 100 이하의 자연수입니다.\n-   배포는 하루에 한 번만 할 수 있으며, 하루의 끝에 이루어진다고 가정합니다. 예를 들어 진도율이 95%인 작업의 개발 속도가 하루에 4%라면 배포는 2일 뒤에 이루어집니다.\n\n## 입출력 예\n\n**입출력 예 #1**\n첫 번째 기능은 93% 완료되어 있고 하루에 1%씩 작업이 가능하므로 7일간 작업 후 배포가 가능합니다.두 번째 기능은 30%가 완료되어 있고 하루에 30%씩 작업이 가능하므로 3일간 작업 후 배포가 가능합니다. 하지만 이전 첫 번째 기능이 아직 완성된 상태가 아니기 때문에 첫 번째 기능이 배포되는 7일째 배포됩니다.세 번째 기능은 55%가 완료되어 있고 하루에 5%씩 작업이 가능하므로 9일간 작업 후 배포가 가능합니다.\n\n따라서 7일째에 2개의 기능, 9일째에 1개의 기능이 배포됩니다.\n\n**입출력 예 #2**\n모든 기능이 하루에 1%씩 작업이 가능하므로, 작업이 끝나기까지 남은 일수는 각각 5일, 10일, 1일, 1일, 20일, 1일입니다. 어떤 기능이 먼저 완성되었더라도 앞에 있는 모든 기능이 완성되지 않으면 배포가 불가능합니다.\n\n따라서 5일째에 1개의 기능, 10일째에 3개의 기능, 20일째에 2개의 기능이 배포됩니다.\n\n## 풀이\n\n```python\ndef solution(progresses, speeds):\n    Q=[]\n    for p, s in zip(progresses, speeds):\n        if len(Q)==0 or Q[-1][0]\u003c-((p-100)//s):\n            Q.append([-((p-100)//s),1])\n        else:\n            Q[-1][1]+=1\n    return [q[1] for q in Q]\n```\n","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EB%8B%A4%EC%9D%8C-%ED%81%B0-%EC%88%AB%EC%9E%90":{"title":"다음 큰 숫자","content":"\n자연수 n이 주어졌을 때, n의 다음 큰 숫자는 다음과 같이 정의 합니다.\n\n-   조건 1. n의 다음 큰 숫자는 n보다 큰 자연수 입니다.\n-   조건 2. n의 다음 큰 숫자와 n은 2진수로 변환했을 때 1의 갯수가 같습니다.\n-   조건 3. n의 다음 큰 숫자는 조건 1, 2를 만족하는 수 중 가장 작은 수 입니다.\n\n예를 들어서 78(1001110)의 다음 큰 숫자는 83(1010011)입니다.\n\n**자연수 n이 매개변수로 주어질 때, n의 다음 큰 숫자를 return 하는 solution 함수를 완성해주세요.**\n\n### 제한 사항\n-   n은 1,000,000 이하의 자연수 입니다.\n\n### 입출력 예\n| n   | result |\n| --- | ------ |\n| 78  | 83     |\n| 15    |      23  |\n\n### 입출력 예 설명\n- 입출력 예#1  \n\t- 문제 예시와 같습니다.  \n- 입출력 예#2  \n\t- 15(1111)의 다음 큰 숫자는 23(10111)입니다.\n--- \n\n## 좋은 풀이\n  \n```python\ndef solution(n):\n    num1 = bin(n).count('1')\n    while True:\n        n = n + 1\n        if num1 == bin(n).count('1'):\n            break\n    return n\n```\n  \n## 나의 풀이\n  \n```python\nfrom collections import Counter\n\ndef find_one(k):\n    return Counter(list(format(k, 'b')))['1']\n\ndef solution(n):\n    answer = n\n    while True:\n        answer += 1\n        if find_one(n) == find_one(answer):\n            break\n    return answer\n```\n   \n## 둘의 차이점\n우선 `Counter()`를 쓸 이유가 없었다. 왜냐하면 문자열 뒤에는` .count()`를 사용할 수 있기 때문이다. 굳이 `Counter()`를 쓰기 위해 리스트로 변환하는 작업도 필요가 없었던 것이다. 즉, 연산 낭비이다. 기억하자, 문자열에서 특정한 문자의 갯수를 알고 싶다면 `.count()`를 쓰면 된다는 것. ","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EB%91%90-%ED%81%90-%ED%95%A9-%EA%B0%99%EA%B2%8C-%EB%A7%8C%EB%93%A4%EA%B8%B0":{"title":"두 큐 합 같게 만들기","content":"\n길이가 같은 두 개의 큐가 주어집니다. 하나의 큐를 골라 원소를 추출(pop)하고, 추출된 원소를 **다른 큐**에 집어넣는(insert) 작업을 통해 각 큐의 원소 합이 같도록 만들려고 합니다. 이때 필요한 작업의 최소 횟수를 구하고자 합니다. 한 번의 pop과 한 번의 insert를 합쳐서 작업을 1회 수행한 것으로 간주합니다.\n\n큐는 먼저 집어넣은 원소가 먼저 나오는 구조입니다. 이 문제에서는 큐를 배열로 표현하며, 원소가 배열 앞쪽에 있을수록 먼저 집어넣은 원소임을 의미합니다. 즉, pop을 하면 배열의 첫 번째 원소가 추출되며, insert를 하면 배열의 끝에 원소가 추가됩니다. 예를 들어 큐 `[1, 2, 3, 4]`가 주어졌을 때, pop을 하면 맨 앞에 있는 원소 1이 추출되어 `[2, 3, 4]`가 되며, 이어서 5를 insert하면 `[2, 3, 4, 5]`가 됩니다.\n\n다음은 두 큐를 나타내는 예시입니다.\n\n```\nqueue1 = [3, 2, 7, 2]\nqueue2 = [4, 6, 5, 1]\n```\n\n두 큐에 담긴 모든 원소의 합은 30입니다. 따라서, 각 큐의 합을 15로 만들어야 합니다. 예를 들어, 다음과 같이 2가지 방법이 있습니다.\n\n1.  queue2의 4, 6, 5를 순서대로 추출하여 queue1에 추가한 뒤, queue1의 3, 2, 7, 2를 순서대로 추출하여 queue2에 추가합니다. 그 결과 queue1은 [4, 6, 5], queue2는 [1, 3, 2, 7, 2]가 되며, 각 큐의 원소 합은 15로 같습니다. 이 방법은 작업을 7번 수행합니다.\n2.  queue1에서 3을 추출하여 queue2에 추가합니다. 그리고 queue2에서 4를 추출하여 queue1에 추가합니다. 그 결과 queue1은 [2, 7, 2, 4], queue2는 [6, 5, 1, 3]가 되며, 각 큐의 원소 합은 15로 같습니다. 이 방법은 작업을 2번만 수행하며, 이보다 적은 횟수로 목표를 달성할 수 없습니다.\n\n따라서 각 큐의 원소 합을 같게 만들기 위해 필요한 작업의 최소 횟수는 2입니다.\n\n길이가 같은 두 개의 큐를 나타내는 정수 배열 `queue1`, `queue2`가 매개변수로 주어집니다. 각 큐의 원소 합을 같게 만들기 위해 필요한 작업의 최소 횟수를 return 하도록 solution 함수를 완성해주세요. 단, 어떤 방법으로도 각 큐의 원소 합을 같게 만들 수 없는 경우, -1을 return 해주세요.\n\n---\n\n### 제한사항\n\n-   1 ≤ `queue1`의 길이 = `queue2`의 길이 ≤ 300,000\n-   1 ≤ `queue1`의 원소, `queue2`의 원소 ≤ 10^9\n-   주의: 언어에 따라 합 계산 과정 중 산술 오버플로우 발생 가능성이 있으므로 long type 고려가 필요합니다.\n\n---\n\n### 입출력 예\n| queue1       | queue2        | result |\n| ------------ | ------------- | ------ |\n| [3, 2, 7, 2] | [4, 6, 5, 1]  | 2      |\n| [1, 2, 1, 2] | [1, 10, 1, 2] | 7      |\n| [1, 1]             |  [1, 5]             |   -1     |\n\n---\n\n### 입출력 예 설명\n\n**입출력 예 #1**\n\n문제 예시와 같습니다.\n\n**입출력 예 #2**\n\n두 큐에 담긴 모든 원소의 합은 20입니다. 따라서, 각 큐의 합을 10으로 만들어야 합니다. queue2에서 1, 10을 순서대로 추출하여 queue1에 추가하고, queue1에서 1, 2, 1, 2와 1(queue2으로부터 받은 원소)을 순서대로 추출하여 queue2에 추가합니다. 그 결과 queue1은 [10], queue2는 [1, 2, 1, 2, 1, 2, 1]가 되며, 각 큐의 원소 합은 10으로 같습니다. 이때 작업 횟수는 7회이며, 이보다 적은 횟수로 목표를 달성하는 방법은 없습니다. 따라서 7를 return 합니다.\n\n**입출력 예 #3**\n\n어떤 방법을 쓰더라도 각 큐의 원소 합을 같게 만들 수 없습니다. 따라서 -1을 return 합니다.\n\n---\n\n### 나의 풀이\n\n```python\nfrom collections import deque\n\ndef act(queue1, queue2, reverse = None):\n    if reverse == None:\n        gone = queue1[0]\n        queue2.append(queue1.popleft())\n    else:\n        gone = queue2[0]\n        queue1.append(queue2.popleft())\n\n    return queue1, queue2, gone\n\ndef solution(queue1, queue2):\n    answer = 0\n\n    queue1 = deque(queue1)\n    queue2 = deque(queue2)\n\n    #먼저 기준을 잡아두고 시작하자.\n    flag =  len(queue1)\n    left = sum(queue1) \n    right = sum(queue2)\n\n    while left != right:\n        if left \u003e right:\n            queue1, queue2, gone = act(queue1, queue2)\n            answer += 1\n            left -= gone\n            right += gone\n        elif left \u003c right:\n            queue1, queue2, gone = act(queue1, queue2,\n\t\t\t\t\t\t\t        reverse = True)\n            answer += 1\n            right -= gone\n            left += gone\n        else:\n            break\n        if answer \u003e flag * 3:\n            return -1\n\n    return answer\n```\n\n역시 `deque`가 빠르다. 괜히 리스트로 풀려고 했다가 시간초과가 자꾸 떠서 시간만 날려먹었네...이렇게 명시적으로 `FIFO` 놀이를 시킨다면 고민하지말고  `deque`를 불러오자. \n\n그것과는 별개로, 마지막 `-1` 리턴하는 코드가 미완성이다. 이건 소위 야매로 풀어버린거라...이 점은 더 생각해봐야겠다. 그래도 `act()` 함수를 새로 만들어서 푼 것은 나쁘지 않은 선택이었던 것 같다. ","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EB%A9%80%EB%A6%AC%EB%9B%B0%EA%B8%B0":{"title":"멀리뛰기","content":"\n효진이는 멀리 뛰기를 연습하고 있습니다. 효진이는 한번에 1칸, 또는 2칸을 뛸 수 있습니다. 칸이 총 4개 있을 때, 효진이는\n\n```\n(1칸, 1칸, 1칸, 1칸)  \n(1칸, 2칸, 1칸)  \n(1칸, 1칸, 2칸)  \n(2칸, 1칸, 1칸)  \n(2칸, 2칸)  \n```\n\n의 5가지 방법으로 맨 끝 칸에 도달할 수 있습니다. \n\n**멀리뛰기에 사용될 칸의 수 n이 주어질 때, 효진이가 끝에 도달하는 방법이 몇 가지인지 알아내, 여기에 1234567를 나눈 나머지를 리턴하는 함수, solution을 완성하세요.** \n\n예를 들어 4가 입력된다면, 5를 return하면 됩니다.\n\n**제한 사항**\n\n-   n은 1 이상, 2000 이하인 정수입니다.\n\n**입출력 예**\n\n| n   | result |\n| --- | ------ |\n| 4   | 5      |\n| 3   | 3      |\n\n**입출력 예 설명**\n\n입출력 예 #1  \n위에서 설명한 내용과 같습니다.\n\n입출력 예 #2\n\n(2칸, 1칸)  \n(1칸, 2칸)  \n(1칸, 1칸, 1칸)  \n\n총 3가지 방법으로 멀리 뛸 수 있습니다.\n\n## 문제 풀이\n\n이 문제는 [Dynamic Programming](notes/Dynamic%20Programming.md)을 이용하는 문제이다. `dp[n]` 은 `n`칸을 갈 수 있는 방법의 수이다. 그래서 `dp[1]`부터 `dp[n]`까지 경우의 수를 찾아 더해준다. 두 번째 for문에서는 1칸만으로 가는 경우의 수를 더해주고, 2칸도 사용해서 가는 경우의 수를 또 더해준다. \n\n결과적으로, 갈 수 있는 칸이 `[1,2]` 칸이기 때문에 `can-step`이 항상 `can-1` 또는 `can-2`가 되어 피보나치와 같아지게 된다. 이 코드는 만약 `[1,2]` 칸이 아니라 더 여러 개의 칸을 갈 수 있는 문제였어도 그대로 적용할 수 있다. \n\n피보나치로 푸는 코드는 다음과 같다.\n\n```python\ndef solution(n):\n    dp = [1] + [0] * n\n    dp[0], dp[1] = 1, 1\n    for i in range(2, n+1):\n        dp[i] = (dp[i-1] + dp[i-2]) % 1234567\n    return dp[n]\n```","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5-Un-supervised-Learning":{"title":"비지도학습 (Un-supervised Learning)","content":"\n## 비지도학습 예시\n\n-   **군집 ( Clustering )**\n\t- K-평균 ( K-Means )  \n\t- DBSCAN  \n\t- 계층 군집 분석 ( Hierarchical Cluster Analysis : HCA )\n-   **이상치/특이치탐지( Anomaly / Novelty Detection )**\n\t- One-class SCM\n\t- Isolation Forest\n-   **시각화 ( Visualization ) \u0026 차원축소 ( Dimension Reduction )**\n\t- 주성분 분석 ( Principal Component Analysis : PCA )\n\t- 커널 PCA ( Kernel PCA )\n\t- 지역적 선형 임베딩 ( Locally Linear Embedding : LLE )\n\t- T-SNE ( t-distributed stochastic neighbor embedding )\n-   연관규칙학습 ( Association Rule Learning )","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EC%88%AB%EC%9E%90-%EB%AC%B8%EC%9E%90%EC%97%B4%EA%B3%BC-%EC%98%81%EB%8B%A8%EC%96%B4":{"title":"숫자 문자열과 영단어","content":"\n![img1.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/d31cb063-4025-4412-8cbc-6ac6909cf93e/img1.png)\n\n네오와 프로도가 숫자놀이를 하고 있습니다. 네오가 프로도에게 숫자를 건넬 때 일부 자릿수를 영단어로 바꾼 카드를 건네주면 프로도는 원래 숫자를 찾는 게임입니다.  \n  \n다음은 숫자의 일부 자릿수를 영단어로 바꾸는 예시입니다.\n\n-   1478 → \"one4seveneight\"\n-   234567 → \"23four5six7\"\n-   10203 → \"1zerotwozero3\"\n\n이렇게 숫자의 일부 자릿수가 영단어로 바뀌어졌거나, 혹은 바뀌지 않고 그대로인 문자열 `s`가 매개변수로 주어집니다. `s`가 의미하는 원래 숫자를 return 하도록 solution 함수를 완성해주세요.\n\n---\n### 제한사항\n\n-   1 ≤ `s`의 길이 ≤ 50\n-   `s`가 \"zero\" 또는 \"0\"으로 시작하는 경우는 주어지지 않습니다.\n-   return 값이 1 이상 2,000,000,000 이하의 정수가 되는 올바른 입력만 `s`로 주어집니다.\n\n---\n### 입출력 예\n\n| s                    | result      |\n| -------------------- | ----------- |\n| `\"one4seveneight\"`   | 1478        |\n| `\"23four5six7\"`      | 234567 |\n| `\"2three45sixseven\"` | 234567      |\n|        `\"123\"`              |     123        |\n\n---\n### 입출력 예 설명\n\n**입출력 예 #1**\n\n-   문제 예시와 같습니다.\n\n**입출력 예 #2**\n\n-   문제 예시와 같습니다.\n\n**입출력 예 #3**\n\n-   \"three\"는 3, \"six\"는 6, \"seven\"은 7에 대응되기 때문에 정답은 입출력 예 2와 같은 234567이 됩니다.\n-   입출력 예 2와 3과 같이 같은 정답을 가리키는 문자열이 여러 가지가 나올 수 있습니다.\n\n**입출력 예 #4**\n\n-   `s`에는 영단어로 바뀐 부분이 없습니다.\n---\n### 제한시간 안내\n-   정확성 테스트 : 10초\n***\n\n## 내 풀이\n\n```python\ndef solution(s):\n    result = ''\n    eng = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n    for idx, num in enumerate(eng):\n        if num in s:\n            s = s.replace(num, str(idx))\n        result = s\n            \n    return int(result)\n```\n\n`enumerate` 가 반환하는 인덱스와 영문 숫자가 일치해서 가능한 방법이었다.\n주어진 문자열에 해당 영문 숫자가 있는지 확인하고, 있다면 `replace`로 대체했다.\n이후 result를 계속 업데이트하고 최종적으로 `int`를 씌워서 반환하면 끝이다.","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EC%88%AB%EC%9E%90-%EC%B9%B4%EB%93%9C-%EB%82%98%EB%88%84%EA%B8%B0":{"title":"숫자 카드 나누기","content":"\n철수와 영희는 선생님으로부터 숫자가 하나씩 적힌 카드들을 절반씩 나눠서 가진 후, 다음 두 조건 중 하나를 만족하는 가장 큰 양의 정수 `a`의 값을 구하려고 합니다.\n\n1.  철수가 가진 카드들에 적힌 모든 숫자를 나눌 수 있고 영희가 가진 카드들에 적힌 모든 숫자들 중 하나도 나눌 수 없는 양의 정수 `a`\n2.  영희가 가진 카드들에 적힌 모든 숫자를 나눌 수 있고, 철수가 가진 카드들에 적힌 모든 숫자들 중 하나도 나눌 수 없는 양의 정수 `a`\n\n예를 들어, 카드들에 10, 5, 20, 17이 적혀 있는 경우에 대해 생각해 봅시다. 만약, 철수가 [10, 17]이 적힌 카드를 갖고, 영희가 [5, 20]이 적힌 카드를 갖는다면 두 조건 중 하나를 만족하는 양의 정수 a는 존재하지 않습니다. 하지만, 철수가 [10, 20]이 적힌 카드를 갖고, 영희가 [5, 17]이 적힌 카드를 갖는다면, 철수가 가진 카드들의 숫자는 모두 10으로 나눌 수 있고, 영희가 가진 카드들의 숫자는 모두 10으로 나눌 수 없습니다. 따라서 철수와 영희는 각각 [10, 20]이 적힌 카드, [5, 17]이 적힌 카드로 나눠 가졌다면 조건에 해당하는 양의 정수 a는 10이 됩니다.\n\n철수가 가진 카드에 적힌 숫자들을 나타내는 정수 배열 `arrayA`와 영희가 가진 카드에 적힌 숫자들을 나타내는 정수 배열 `arrayB`가 주어졌을 때, 주어진 조건을 만족하는 가장 큰 양의 정수 a를 return하도록 solution 함수를 완성해 주세요. 만약, 조건을 만족하는 a가 없다면, 0을 return 해 주세요.\n\n---\n\n### 제한사항\n\n-   1 ≤ `arrayA`의 길이 = `arrayB`의 길이 ≤ 500,000\n-   1 ≤ `arrayA`의 원소, `arrayB`의 원소 ≤ 100,000,000\n-   `arrayA`와 `arrayB`에는 중복된 원소가 있을 수 있습니다.\n\n---\n\n### 입출력 예\n\n| arrayA   | arrayB  | result |\n| -------- | ------- | ------ |\n| [10, 17] | [5, 20] | 0      |\n| [10, 20] | [5, 17] | 10     |\n| [14, 35, 119]         |     [18, 30, 102]    |     7   |\n\n---\n\n### 입출력 예 설명\n\n**입출력 예 #1**\n\n-   문제 예시와 같습니다.\n\n**입출력 예 #2**\n\n-   문제 예시와 같습니다.\n\n**입출력 예 #3**\n\n-   철수가 가진 카드에 적힌 숫자들은 모두 3으로 나눌 수 없고, 영희가 가진 카드에 적힌 숫자는 모두 3으로 나눌 수 있습니다. 따라서 3은 조건에 해당하는 양의 정수입니다. 하지만, 철수가 가진 카드들에 적힌 숫자들은 모두 7로 나눌 수 있고, 영희가 가진 카드들에 적힌 숫자는 모두 7로 나눌 수 없습니다. 따라서 최대값인 7을 return 합니다.\n\n---\n## 나의 풀이\n\n```python\nfrom math import gcd\n\ndef solution(arrayA, arrayB):\n    gcd1, gcd2 = arrayA[0], arrayB[0]\n    for each1, each2 in zip(arrayA[1:], arrayB[1:]):\n        gcd1, gcd2 = gcd(each1, gcd1), gcd(each2, gcd2)\n    answer = []\n    for each1 in arrayA:\n        if each1 % gcd2 == 0:\n            break\n    else:\n        answer.append(gcd2)\n    for each2 in arrayB:\n        if each2 % gcd1 == 0:\n            break\n    else:\n        answer.append(gcd1)\n    return max(answer) if answer else 0\n```\n\n최대공약수(GCD)를 구해서 조건을 적용해보고, 조건이 모두 충족되면 answer 리스트에 저장한다. 그리고 answer에서 최대값을 꺼내서 return하면 끝.","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EC%8A%A4%ED%82%AC%ED%8A%B8%EB%A6%AC":{"title":"스킬트리","content":"선행 스킬이란 어떤 스킬을 배우기 전에 먼저 배워야 하는 스킬을 뜻합니다.\n\n예를 들어 선행 스킬 순서가 `스파크 → 라이트닝 볼트 → 썬더`일때, 썬더를 배우려면 먼저 라이트닝 볼트를 배워야 하고, 라이트닝 볼트를 배우려면 먼저 스파크를 배워야 합니다.\n\n위 순서에 없는 다른 스킬(힐링 등)은 순서에 상관없이 배울 수 있습니다. 따라서 `스파크 → 힐링 → 라이트닝 볼트 → 썬더`와 같은 스킬트리는 가능하지만, `썬더 → 스파크`나 `라이트닝 볼트 → 스파크 → 힐링 → 썬더`와 같은 스킬트리는 불가능합니다.\n\n선행 스킬 순서 `skill`과 유저들이 만든 스킬트리를 담은 배열 `skill_trees`가 매개변수로 주어질 때, 가능한 스킬트리 개수를 return 하는 solution 함수를 작성해주세요.\n\n### 제한 조건\n\n-   스킬은 알파벳 대문자로 표기하며, 모든 문자열은 알파벳 대문자로만 이루어져 있습니다.\n-   스킬 순서와 스킬트리는 문자열로 표기합니다.\n    -   예를 들어, `C → B → D` 라면 \"CBD\"로 표기합니다\n-   선행 스킬 순서 skill의 길이는 1 이상 26 이하이며, 스킬은 중복해 주어지지 않습니다.\n-   skill_trees는 길이 1 이상 20 이하인 배열입니다.\n-   skill_trees의 원소는 스킬을 나타내는 문자열입니다.\n    -   skill_trees의 원소는 길이가 2 이상 26 이하인 문자열이며, 스킬이 중복해 주어지지 않습니다.\n\n### 입출력 예\n\n| skill | skill_trees | return |\n| ----- | ----------- | ------ |\n| `\"CBD\"`      |  `[\"BACDE\", \"CBADF\", \"AECB\", \"BDA\"]`           |      2  |\n\n### 입출력 예 설명\n\n-   \"BACDE\": B 스킬을 배우기 전에 C 스킬을 먼저 배워야 합니다. 불가능한 스킬트립니다.\n-   \"CBADF\": 가능한 스킬트리입니다.\n-   \"AECB\": 가능한 스킬트리입니다.\n-   \"BDA\": B 스킬을 배우기 전에 C 스킬을 먼저 배워야 합니다. 불가능한 스킬트리입니다.\n\n## 나의 풀이\n\n```python\nfrom collections import deque\n\ndef solution(skill, skill_trees):\n    answer = 0\n\n    for skills in skill_trees:\n        skill_list = deque(list(skill))\n\n        for s in skills:\n            if s in skill:\n                if s != skill_list.popleft():\n                    break\n        else:\n            answer += 1\n\n    return answer\n```\n\n1. 먼저 각 스킬트리를 deque에 태운다. 이 deque는 왼쪽부터 제거될 예정.\n2. 그래서 만약 한 스킬트리의 한 글자가 skill에 포함되어 있다면, \n\t1. deque에서 가장 왼쪽의 글자를 빼고 둘을 비교한다.\n\t2. 둘이 같다면 계속하고, 다르다면 중지한다.\n\t3. 이때, deque의 길이는 하나 줄어들게 된다.\n3. 계속 비교해서 내려가고, 조건을 충족시키면 answer에 계속 더해 return 하면 끝. ","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EC%98%81%EC%96%B4-%EB%81%9D%EB%A7%90%EC%9E%87%EA%B8%B0":{"title":"영어 끝말잇기","content":"\n1부터 n까지 번호가 붙어있는 n명의 사람이 영어 끝말잇기를 하고 있습니다. 영어 끝말잇기는 다음과 같은 규칙으로 진행됩니다.\n\n1. 1번부터 번호 순서대로 한 사람씩 차례대로 단어를 말합니다.\n2. 마지막 사람이 단어를 말한 다음에는 다시 1번부터 시작합니다.\n3. 앞사람이 말한 단어의 마지막 문자로 시작하는 단어를 말해야 합니다.\n4. 이전에 등장했던 단어는 사용할 수 없습니다.\n5. 한 글자인 단어는 인정되지 않습니다.\n\n다음은 3명이 끝말잇기를 하는 상황을 나타냅니다.\n\ntank → kick → know → wheel → land → dream → mother → robot → tank\n\n위 끝말잇기는 다음과 같이 진행됩니다.\n\n- 1번 사람이 자신의 첫 번째 차례에 tank를 말합니다.\n- 2번 사람이 자신의 첫 번째 차례에 kick을 말합니다.\n- 3번 사람이 자신의 첫 번째 차례에 know를 말합니다.\n- 1번 사람이 자신의 두 번째 차례에 wheel을 말합니다.\n- (계속 진행)\n\n끝말잇기를 계속 진행해 나가다 보면, 3번 사람이 자신의 세 번째 차례에 말한 tank 라는 단어는 이전에 등장했던 단어이므로 탈락하게 됩니다.\n\n사람의 수 n과 사람들이 순서대로 말한 단어 words 가 매개변수로 주어질 때, 가장 먼저 탈락하는 사람의 번호와 그 사람이 자신의 몇 번째 차례에 탈락하는지를 구해서 return 하도록 solution 함수를 완성해주세요.\n\n#### 제한 사항\n\n- 끝말잇기에 참여하는 사람의 수 n은 2 이상 10 이하의 자연수입니다.\n- words는 끝말잇기에 사용한 단어들이 순서대로 들어있는 배열이며, 길이는 n 이상 100 이하입니다.\n- 단어의 길이는 2 이상 50 이하입니다.\n- 모든 단어는 알파벳 소문자로만 이루어져 있습니다.\n- 끝말잇기에 사용되는 단어의 뜻(의미)은 신경 쓰지 않으셔도 됩니다.\n- 정답은 [ 번호, 차례 ] 형태로 return 해주세요.\n- 만약 주어진 단어들로 탈락자가 생기지 않는다면, [0, 0]을 return 해주세요.\n\n----\n\n#### 입출력 예\n\n| **n** | **words**                                                                                                                                                          | **result** |\n| ----- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------- |\n| 3     | [\"tank\", \"kick\", \"know\", \"wheel\", \"land\", \"dream\", \"mother\", \"robot\", \"tank\"]                                                                                      | [3,3]      |\n| 5     | [\"hello\", \"observe\", \"effect\", \"take\", \"either\", \"recognize\", \"encourage\", \"ensure\", \"establish\", \"hang\", \"gather\", \"refer\", \"reference\", \"estimate\", \"executive\"] | [0,0]      |\n| 2     | [\"hello\", \"one\", \"even\", \"never\", \"now\", \"world\", \"draw\"]                                                                                                          | [1,3]      |\n\n#### 입출력 예 설명\n\n입출력 예 #1\n\n3명의 사람이 끝말잇기에 참여하고 있습니다.\n\n- 1번 사람 : tank, wheel, mother\n- 2번 사람 : kick, land, robot\n- 3번 사람 : know, dream, `tank`\n\n와 같은 순서로 말을 하게 되며, 3번 사람이 자신의 세 번째 차례에 말한 `tank`라는 단어가 1번 사람이 자신의 첫 번째 차례에 말한 `tank`와 같으므로 3번 사람이 자신의 세 번째 차례로 말을 할 때 처음 탈락자가 나오게 됩니다.\n\n입출력 예 #2\n\n5명의 사람이 끝말잇기에 참여하고 있습니다.\n\n- 1번 사람 : hello, recognize, gather\n- 2번 사람 : observe, encourage, refer\n- 3번 사람 : effect, ensure, reference\n- 4번 사람 : take, establish, estimate\n- 5번 사람 : either, hang, executive\n\n와 같은 순서로 말을 하게 되며, 이 경우는 주어진 단어로만으로는 탈락자가 발생하지 않습니다. 따라서 [0, 0]을 return하면 됩니다.\n\n입출력 예 #3\n\n2명의 사람이 끝말잇기에 참여하고 있습니다.\n\n- 1번 사람 : hello, even, `now`, draw\n- 2번 사람 : one, never, world\n\n와 같은 순서로 말을 하게 되며, 1번 사람이 자신의 세 번째 차례에 'r'로 시작하는 단어 대신, n으로 시작하는 `now`를 말했기 때문에 이때 처음 탈락자가 나오게 됩니다.\n\n---\n\n## 나의 풀이\n\n```python\ndef solution(n, words):\n    for p in range(1, len(words)):\n        if words[p-1][-1] != words[p][0] or words[p] in words[:p]:\n            return [(p%n) + 1, (p//n) + 1]\n    else:\n        return([0,0])\n```\n\n### 해석\n처음에는 단어와 사람 번호를 같이 딕셔너리에 저장하려고 했다. 그러나 계속 코드가 복잡해지자 생각을 바꾸고 위와 같이 코드를 새로 작성했다. 이 문제의 핵심은 두 가지이다.\n\n- **이전 단어의 마지막 글자와 현재 단어의 첫 글자가 일치하는가**\n- **이전에 나왔던 단어가 다시 나오는가**\n\n처음 것은 인덱스로 간단하게 확인하면 되는데, 두번째 것은 슬라이싱으로 확인하는 방법이 신선한 점이다. 결국 단어 리스트가 주어졌을 때, 현재 단어 이전에 똑같은 단어가 있는지만 확인하면 되는 것이므로. `LV-2` 이지만 짧게 해결할 수 있는 문제였다.","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EC%98%AC%EB%B0%94%EB%A5%B8-%EA%B4%84%ED%98%B8":{"title":"올바른 괄호","content":"\n괄호가 바르게 짝지어졌다는 것은 '(' 문자로 열렸으면 반드시 짝지어서 ')' 문자로 닫혀야 한다는 뜻입니다. 예를 들어\n\n-   \"()()\" 또는 \"(())()\" 는 올바른 괄호입니다.\n-   \")()(\" 또는 \"(()(\" 는 올바르지 않은 괄호입니다.\n\n'(' 또는 ')' 로만 이루어진 문자열 `s`가 주어졌을 때, 문자열 `s`가 올바른 괄호이면 `true`를 `return` 하고, 올바르지 않은 괄호이면 `false`를 `return` 하는 `solution` 함수를 완성해 주세요.\n\n## 제한사항\n\n-   문자열 `s`의 길이 : 100,000 이하의 자연수\n-   문자열 `s`는 '(' 또는 ')' 로만 이루어져 있습니다.\n\n\n## 입출력 예\n| s        | answer |\n| -------- | ------ |\n| \"()()\"   | true   |\n| \"(())()\" | true   |\n| \")()(\"   | false  |\n| \"(()(\"   | false  |\n\n\n## 나의 풀이\n\n```python\ndef solution(s):\n    answer = True\n    flag = 0\n    for n in s :\n        if flag \u003c 0 :\n            return False\n        if n == '(' :\n            flag += 1\n        else :\n            flag -= 1\n    return True if flag == 0 else False\n```\n\n이 문제는 스택을 사용하는 문제이다. `flag` 용도로 쓸 변수를 선언하고 해당 변수를 이용하여 괄호가 올바른지를 체크했다. 만약에 `flag`가 음수가 되는 경우는 `)`가 먼저 나오는 경우라서 바로 올바르지 않기 때문에 `False`를 리턴했고, `(`이 나오는 경우는 `flag`를 증가시키고 `)`이 나오는 경우 `flag`를 감소시켜서 만약에 `0`이 되면 괄호가 열린만큼 닫힌거라 `True`를 리턴했고 `0`보다 크면 `(`가 더 많이 떴다는 뜻으로 `False`를 리턴했다.\n","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EC%A0%95%EC%88%98-%EC%82%BC%EA%B0%81%ED%98%95":{"title":"정수 삼각형","content":"\n![스크린샷 2018-09-14 오후 5.44.19.png](https://grepp-programmers.s3.amazonaws.com/files/production/97ec02cc39/296a0863-a418-431d-9e8c-e57f7a9722ac.png)\n\n위와 같은 삼각형의 꼭대기에서 바닥까지 이어지는 경로 중, 거쳐간 숫자의 합이 가장 큰 경우를 찾아보려고 합니다. 아래 칸으로 이동할 때는 대각선 방향으로 한 칸 오른쪽 또는 왼쪽으로만 이동 가능합니다. 예를 들어 3에서는 그 아래칸의 8 또는 1로만 이동이 가능합니다.\n\n삼각형의 정보가 담긴 배열 `triangle`이 매개변수로 주어질 때, 거쳐간 숫자의 최댓값을 `return` 하도록 `solution` 함수를 완성하세요.\n\n### 제한사항\n\n-   삼각형의 높이는 1 이상 500 이하입니다.\n-   삼각형을 이루고 있는 숫자는 0 이상 9,999 이하의 정수입니다.\n\n### 입출력 예\n\n| triangle                                                  | result |\n| --------------------------------------------------------- | ------ |\n| [\\[7], [3, 8], [8, 1, 0], [2, 7, 4, 4], [4, 5, 2, 6, 5]\\] | 30     |\n\n## 나의 오답\n\n```python\ndef solution(triangle):\n    curr_answer = 0\n    k = 0\n    for i in range(len(triangle)-1, 1, -1):\n        if k not in [0, len(triangle[i])-1]:\n            triangle[i-1][k-1] += triangle[i][k]\n            triangle[i-1][k] += triangle[i][k]\n\n        elif k == 0:\n            triangle[i-1][k] += triangle[i][k]\n        else:\n            triangle[i-1][k-1] += triangle[i][k]\n        k += 1\n        if k == len(triangle[i])-1:\n            k = 0\n    left = triangle[0][0] + triangle[1][0]\n    right = triangle[0][0] + triangle[1][1]\n    prev_answer = max(left, right)\n    if prev_answer \u003e curr_answer:\n        curr_answer = prev_answer\n    \n    return curr_answer\n```\n\n이건 오답이다. \n아래의 정답과 비교하자. \n\n## 다른 사람의 풀이\n\n```python\ndef solution(triangle):\n\n    height = len(triangle)\n\n    while height \u003e 1:\n        for i in range(height - 1):\n            triangle[height-2][i] += max([triangle[height-1][i], triangle[height-1][i+1]])\n        height -= 1\n\n    answer = triangle[0][0]\n    return answer\n```\n\n나도 이렇게 **삼각형을 거꾸로 시작하는 방법**을 생각했는데...\n`for i in range(len(triangle)-1, 1, -1):` 이런 걸로 하나하나 내려갈 필요 없이, `while height \u003e 1:`으로 한 다음, `height -= 1`으로 내려가면 되었구나. 괜히 복잡하게 생각했다. \n\n그리고 따로 [`DP 테이블`](notes/Dynamic%20Programming.md) 을 사용할 필요 없이, 아래 칸의 값을 현재 칸의 좌 우의 값의 합으로 저장해버리면 된다. 나는 여기에서 뺄셈을 사용하는 바람에 조건문이 더 추가되어 버렸는데, 그냥 `i + 1`로 해버려도 된다. \n","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5-Supervised-Learning":{"title":"지도학습 (Supervised Learning)","content":"## 지도학습 대표 알고리즘\n\n1.  K-Nearest Neighbors (KNN)\n2. Linear Regression\n3. Logistic Regression\n4. Support Vector Machines (SVM)\n5. Decision Tree / Random Forest\n6. Gradient Boosting Algorithms ([XGB](notes/XGB%20Modeling.md), LGB, CatGB, NGB 등)\n7. Neural Networks","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/%ED%82%A4%ED%8C%A8%EB%93%9C-%EB%88%84%EB%A5%B4%EA%B8%B0":{"title":"키패드 누르기","content":"\n스마트폰 전화 키패드의 각 칸에 다음과 같이 숫자들이 적혀 있습니다.\n\n![kakao_phone1.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/4b69a271-5f4a-4bf4-9ebf-6ebed5a02d8d/kakao_phone1.png)\n\n이 전화 키패드에서 왼손과 오른손의 엄지손가락만을 이용해서 숫자만을 입력하려고 합니다.  \n맨 처음 왼손 엄지손가락은 `*` 키패드에 오른손 엄지손가락은 `#` 키패드 위치에서 시작하며, 엄지손가락을 사용하는 규칙은 다음과 같습니다.\n\n1.  엄지손가락은 상하좌우 4가지 방향으로만 이동할 수 있으며 키패드 이동 한 칸은 거리로 1에 해당합니다.\n2.  왼쪽 열의 3개의 숫자 `1`, `4`, `7`을 입력할 때는 왼손 엄지손가락을 사용합니다.\n3.  오른쪽 열의 3개의 숫자 `3`, `6`, `9`를 입력할 때는 오른손 엄지손가락을 사용합니다.\n4.  가운데 열의 4개의 숫자 `2`, `5`, `8`, `0`을 입력할 때는 두 엄지손가락의 현재 키패드의 위치에서 더 가까운 엄지손가락을 사용합니다.  \n    4-1. 만약 두 엄지손가락의 거리가 같다면, 오른손잡이는 오른손 엄지손가락, 왼손잡이는 왼손 엄지손가락을 사용합니다.\n\n순서대로 누를 번호가 담긴 배열 numbers, 왼손잡이인지 오른손잡이인 지를 나타내는 문자열 hand가 매개변수로 주어질 때, 각 번호를 누른 엄지손가락이 왼손인 지 오른손인 지를 나타내는 연속된 문자열 형태로 return 하도록 solution 함수를 완성해주세요.\n\n## 제한사항\n\n-   numbers 배열의 크기는 1 이상 1,000 이하입니다.\n-   numbers 배열 원소의 값은 0 이상 9 이하인 정수입니다.\n-   hand는 `\"left\"` 또는 `\"right\"` 입니다.\n    -   `\"left\"`는 왼손잡이, `\"right\"`는 오른손잡이를 의미합니다.\n-   왼손 엄지손가락을 사용한 경우는 `L`, 오른손 엄지손가락을 사용한 경우는 `R`을 순서대로 이어붙여 문자열 형태로 return 해주세요.\n\n## 입출력 예\n\n| numbers                           | hand      | result          |\n| --------------------------------- | --------- | --------------- |\n| [1, 3, 4, 5, 8, 2, 1, 4, 5, 9, 5] | `\"right\"` | `\"LRLLLRLLRRL\"` |\n| [7, 0, 8, 2, 8, 3, 1, 5, 7, 6, 2] | `\"left\"`  | `\"LRLLRRLLLRR\"` |\n| [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]    | `\"right\"` | `\"LLRLLRLLRL\"`  |\n\n## 나의 풀이\n\n```python\ndef solution(numbers, hand):\n    pad = {\n        1:(0,0), 2:(0,1), 3:(0,2),\n        4:(1,0), 5:(1,1), 6:(1,2),\n        7:(2,0), 8:(2,1), 9:(2,2),\n        '*':(3,0), 0:(3,1), '#':(3,2)\n    }\n    L_prev_key = pad['*']\n    R_prev_key = pad['#']\n    answer = ''\n\n    for key in numbers:\n        if key in [1,4,7]:\n            answer += 'L'\n            L_prev_key = pad[key]\n        elif key in [3,6,9]:\n            answer += 'R'\n            R_prev_key = pad[key]\n        else:\n            L_distance = abs(L_prev_key[0] - pad[key][0]) + abs(L_prev_key[1] - pad[key][1])\n            R_distance = abs(R_prev_key[0] - pad[key][0]) + abs(R_prev_key[1] - pad[key][1])\n            if L_distance \u003c R_distance:\n                answer += 'L'\n                L_prev_key = pad[key]\n            elif L_distance \u003e R_distance:\n                answer += 'R'\n                R_prev_key = pad[key]\n            else:\n                if hand == 'right':\n                    answer += 'R'\n                    R_prev_key = pad[key]\n                else:\n                    answer += 'L'\n                    L_prev_key = pad[key]\n    return answer\n```\n\n이 문제는 좌표가 필요한 문제이다. 좌표 튜플을 각 번호에 해당하는 딕셔너리에 저장하고 시작하면 된다. ","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null},"/notes/Algorithms":{"title":"⚙️ Algorithms","content":"- [⚙️ Two Pointers](notes/Two%20Pointers.md)\n- [⚙️ Dynamic Programming](notes/Dynamic%20Programming.md)\n- [⚙️ DFS, BFS](notes/DFS,%20BFS.md)","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/Call-for-Customized-Conversation":{"title":"Call for Customized Conversation-Customized Conversation Grounding Persona and Knowledge","content":"\n## Abstarct\n\u003e Humans usually have conversations by making use of prior knowledge about a topic and background information of the people whom they are talking to. **However, existing conversational agents and datasets do not consider such comprehensive information, and thus they have a limitation in generating the utterances where the knowledge and persona are fused properly.** To address this issue, we introduce a call For Customized conversation **(FoCus) dataset** where the customized answers are built with the user’s persona and Wikipedia knowledge. (...)\n- 현재 상황과 한계를 찾아보자. \n\t- 사람은 대화의 주제나, 자기와 말하고 있는 사람의 배경 정보 등에 기대어 대화를 이어나간다. 그러나 현재 대화 시스템이나 데이터셋은 이러한 정보들을 이해하지 못하고 있으며, 따라서 지식이나 성격이 함께 적절히 융합된 발화를 생성하는 데에 한계가 있다. \n- 그래서 본 논문의 목적은?\n\t- 이에 본 논문은 이용자의 성격(persona)와 Wikipedia과 함께 구축된 FoCus 데이터셋을 발표했다.\n\n\u003e We examine whether the model reflects adequate persona and knowledge with our proposed two sub-tasks, **persona grounding (PG) and knowledge grounding (KG)**.\n- 그래서 그걸로 뭘 한 건데?\n\t- 모델이 적절히 페르소나와 지식을 반영하는지 확인해볼 것이다. \n\t- `Persona grounding (PG)` 와 `Knowledge grounding (KG)`라고 하는 서브 태스크를 통해서!\n\n## FoCus Dataset\n\n![Figure 2: Example dialog between Human and Machine in FoCus dataset](Example-dialog.png)\n- 그래서 이런 대화가 가능하도록 하고 싶다는 것이다. \n- 이용자와 대화를 진행하는 시스템이 실제 지식에 기반하면서('This place is called Sentosa'), 또한 이용자의 페르소나에 근거하여 말을 이어나가는 것('I believe you wish to visit Singapore.')!\n\n## Model\n\n![](Overview-of-model.png)\n\u003e We introduce the baseline models trained on our FoCus dataset, consisting of a `retrieval module` and a `dialog module`. The `retrieval module` retrieves the knowledge paragraphs related to a question, and the `dialog module` generates utterances of the machine by taking the retrieved knowledge paragraphs, human’s persona, and previous utterances as inputs.\n- 그래서 모델 아키텍쳐에는 두 개의 모듈이 들어간다. `retrieval module` 과 `dialog module` 이다. `retrieval module` 은 질문과 관련된 지식 파라그래프를 검색하고, `dialog module` 은 이것과 이용자의 페르소나 정보에 근거해서 모델의 발화를 생성해내는 것이다.\n\n![Table 3: Experimental results](Experimental-results.png)\n- 실험 결과, PG, KG에 모두 훈련된 BART와 GPT-2가 generation에서는 조금 성능이 낮지만 전반적으로 비슷한 성능을 보여주며, Grouding 서브 태스크에서는 가장 뛰어난 성능을 보여주었다. \n\n## Conclusion\n\u003e We hope that the researches aim to make dialog agents more attractive and knowledgeable with grounding abilities to be explored.\n- 우리는 연구자들이 더 있을 실제적 능력들과 함께, 대화 에이전트를 더욱 매력적이고 지식을 풍부히 가지도록 만드는 것을 목적으로 하기를 희망한다. \n- 끝맺음은 훈훈하게 하는구나. 대화 시스템은 더욱 매력적이게 될 수 있다. **감정 뿐만 아니라 지식, 페르소나를 모두 고려한 대화 시스템은 어떤 모습이 될까?**","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/Challenges-and-frontiers-in-abusive-content-detect":{"title":"Challenges and frontiers in abusive content detect","content":"\n\u003e [!note] Note  \n\u003e \n\u003e 이번에 정리하는 논문은 \"abusive content detect\" 과제가 어떻게 발전했고, 지금 마주하고 있는 어려움은 무엇인지 소개하는 논문이다. \n\u003e 혐오표현 탐지 과제와 (완전히는 아니지만) 비슷한 과제이기에 비슷한 어려움들을 공유하고 있다. 주석 작업의 어려움, 개념 정의의 정확성 등...\n\u003e 개인적으로 인상 깊은 것은 '훈련 데이터와 다른 도메인에서 테스트가 이루어지면 성능이 어떻게 변할지'를 소개하는 파트이다.\n\n***\n\n\u003e Developing robust systems to detect abuse is a crucial part of online content moderation and plays a fundamental role in creating an open, safe and accessible Internet.\n-  욕설(abuse)을 탐지하기 위한 강력한 시스템을 개발하는 것은 온라인 콘텐츠 조정의 중요한 부분이며 **개방적이고 안전하며 접근 가능한 인터넷을 만드는 데 근본적인 역할을 한다**.\n\n\u003e Advances in machine learning and NLP have led to marked improvements in abusive content detection systems’ performance (Fortuna \u0026 Nunes, 2018; Schmidt \u0026 Wiegand, 2017). For instance, in 2018 Pitsilis et al. trained a classification system on Waseem and Hovy’s 16,000 tweet dataset and achieved an F-Score of 0.932, compared against Waseem and Hovy’s original 0.739; a 20-point increase (Pitsilis, Ramampiaro, \u0026 Langseth, 2018; Waseem \u0026 Hovy, 2016).\n-  **머신 러닝과 NLP의 발전은 모욕 콘텐츠 감지 시스템의 성능을 현저하게 향상시켰다(Fortuna \u0026 Nunes, 2018; Schmidt \u0026 Wiegand, 2017).** 예를 들어, 2018년 Pitsilis 등은 Wasem과 Hovy의 16,000개의 트윗 데이터 세트에 대한 분류 시스템을 훈련하고 Wasem과 Hovy의 원래 0.739와 비교하여 0.932의 F-Score를 달성했다(Pitsilis, Ramampiaro, \u0026 Langseth, 2018; Wasem \u0026 Hovy, 2016).\n\n\u003e Researchers have also addressed numerous tasks beyond binary abusive content classification, including identifying the target of abuse and its strength as well as automatically moderating content (Burnap \u0026 Williams, 2016; Davidson, Warmsley, Macy, \u0026 Weber, 2017; Santos, Melnyk, \u0026 Padhi, 2018).\n- 연구자들은 또한 학대 대상을 식별하는 것뿐만 아니라 콘텐츠 자동 조절을 포함하여 이진 욕설 콘텐츠 분류를 넘어 수많은 과제를 해결했다(Burnap \u0026 Williams, 2016; Davidson, Warmsley, Macy, Weber, 2017; Santos, Melnyk, \u0026 Padhi, 2018). \n\n\u003e (…) what type of abusive content it is identified as. This is a social and theoretical task: **there is no objectively ‘correct’ definition** or single set of pre-established criteria which can be applied.\n- (...) 어떤 유형의 욕설 콘텐츠로 식별되는지도 중요하다. 이러한 분류는 사회적이고 이론적인 과제이기에, **객관적으로 '올바른' 정의나 적용할 수 있는 사전 설정된 기준의 단일 집합은 없다.**\n\n\u003e Detecting abusive content generically is an important aspiration for the field. **However, it is very difficult because abusive content is so varied.** Research which purports to address the generic task of detecting abuse is typically actually addressing something much more specific. This can often be discerned from the datasets, which may contain systematic biases towards certain types and targets of abuse. For instance, the dataset by Davidson et al. is used widely for tasks described generically as abusive content detection yet it is highly skewed towards racism and sexism (Davidson et al., 2017).\n- 욕설 콘텐츠를 전반적으로 감지하는 것은 현장의 중요한 포부다. 하지만 이는 욕설의 내용이 다양하기 때문에 매우 어렵다. **욕설을 탐지하는 일반적인 과제를 해결한다고 주장하는 연구는 일반적으로 훨씬 더 구체적인 것(specific)을 다루고 있다.** 이것은 종종 데이터 세트에서 식별될 수 있으며, 이는 특정 유형 및 욕설 대상에 대한 체계적인 편견을 포함할 수 있다. **예를 들어, Davidson 등의 데이터 세트는 일반적으로 욕설 콘텐츠 탐지로 설명되는 작업에 널리 사용되지만 인종차별과 성차별 쪽으로 크게 치우쳐 있다(Davidson 등, 2017).** \n\n\u003e Waseem et al. suggest that one of the main differences between subtasks is whether content is ‘directed towards a specific entity or is directed towards a generalized group’ (Waseem et al., 2017).\n- `Waseem et al.`은 하위 작업 간의 주요 차이점 중 하나는 콘텐츠가 '특정 엔티티를 지향하는지, 아니면 일반화된 그룹을 지향하는지'라고 제안한다(Wasee et al., 2017).\n\n\n\u003e A key distinction is whether abuse is explicit or implicit (Waseem et al., 2017; Zampieri et al., 2019).\n\n\n\u003e Some of the main problems are (1) researchers use terms which are not well-defined, (2) different concepts and terms are used across the field for similar work, and (3) the terms which are used are theoretically problematic.\n- 주요 문제 중 일부는 (1) 연구자들이 잘 정의되지 않은 용어를 사용하고, (2) 유사한 작업에 대해 현장 전반에 걸쳐 다른 개념과 용어를 사용하며, (3) 사용되는 용어가 이론적으로 문제가 있다는 것이다.\n\n\u003e **Annotation.** \n\u003e Annotation is a notoriously difficult task, reflected in the low levels of inter-annotator agreement reported by most publications, particularly on more complex multi-class tasks (Sanguinetti, Poletto, Bosco, Patti, \u0026 Stranisci, 2018). Noticeably, van Aken suggests that Davidson et al.’s widely used hate and offensive language dataset has up to 10% of its data mislabeled (van Aken et al., 2018).\n- 주석은 악명높게 어려운 작업으로, 특히 더 복잡한 다중 클래스 작업에 대해 대부분의 연구들에서 보고한 주석 간 합의(inter-annotator agreement)의 낮은 수준에 반영된다(Sanguinetti, Poletto, Bosco, Patti, \u0026 Stranisci, 2018). 독특하게,  **반 에이켄은 데이비드슨 등의 널리 사용되는 혐오 및 공격적인 언어 데이터 세트가 최대 10%의 데이터 레이블이 잘못 지정되었음을 시사한다(반 에이켄 외, 2018).**\n\n\u003e Few publications provide details of their annotation process or annotation guidelines. Providing such information is the norm in social scientific research and is viewed as an integral part of verifying others’ findings and robustness (Bucy \u0026 Holbert, 2013). In line with the recommendations of Sabou et al., we advocate that annotation guidelines and processes are shared where possible (Sabou, Bontcheva, Derczynski, \u0026 Scharl, 2014) and that the field also works to develop best practices.\n- 주석 프로세스 또는 주석 지침에 대한 자세한 내용을 제공하는 연구들은 거의 없다. **이러한 정보를 제공하는 것은 사회과학 연구의 표준이며**, 타인의 발견과 견고성을 검증하는 데 필수적인 부분으로 간주된다(Bucy \u0026 Holbert, 2013). **Sabou 등의 권고에 따라, 우리는 주석 지침과 프로세스가 가능한 곳에서 공유되고(Sabou, Bontcheva, Derczynski, \u0026 Scharl, 2014) 이 분야도 모범 사례를 개발하기 위해 노력해야 한다고 주장한다.**\n\n\u003e Ensuring that abusive content detection systems can be applied across different domains is one of the most difficult but also important frontiers in existing research. Thus far, efforts to address this has been unsuccessful. Burnap and Williams train systems on one type of hate speech (e.g. racism) and apply them to another (e.g. sexism) and find that performance drops considerably (Burnap \u0026 Williams, 2016)\n- 욕설 콘텐츠 탐지 시스템이 서로 다른 영역에 걸쳐 적용될 수 있도록 보장하는 것은 기존 연구에서 가장 어렵지만 중요한 분야 중 하나이다. 지금까지, 이것을 해결하려는 노력은 성공하지 못했다. **Burnap과 Williams는 한 유형의 혐오 발언(예: 인종차별)에 대해 시스템을 훈련시키고 다른 유형의 혐오 발언(예: 성차별)에 적용하며 성능이 상당히 떨어진다는 것을 발견한다(Burnap \u0026 Williams, 2016).**\n\n\n\u003e Karan and Šnajder use a simple methodology to show the huge differences in performance when applying classifiers on different datasets without domain-specific tuning (Karan \u0026 Šnajder, 2018). Noticeably, in the EVALITA hate speech detection shared task, participants were asked to (1) train and test a system on Twitter data, (2) on Facebook data and (3) to train on Twitter and test on Facebook (and vice versa). **Even the best performing teams reported their systems scored around 10 to 15 F1 points fewer on the cross-domain task.**\n- Karan과 Shnajder는 도메인별 튜닝 없이 서로 다른 데이터 세트에 분류기를 적용할 때 성능에서 큰 차이를 보여주기 위해 간단한 방법론을 사용한다(Karan \u0026 Shnajder, 2018). 눈에 띄게, EVALITA 혐오 발언 탐지 공유 작업에서, **참가자들에게 (1) 트위터 데이터에 대한 시스템 훈련 및 테스트, (2) 페이스북 데이터에 대한 시스템 훈련 및 테스트, (3) 트위터에 대한 훈련 및 페이스북에서 테스트(및 그 반대)를 요청하였다.** 최고의 성과를 거둔 팀들조차 그들의 시스템이 교차 도메인 작업에서 약 10-15의 F1 점수를 더 적게 받았다고 보고했다.\n\n***\n\n\u003e [!info] Reference  \n\u003e \n\u003e Bertie Vidgen, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott Hale, and Helen Margetts. 2019. [Challenges and frontiers in abusive content detection](https://aclanthology.org/W19-3509). In _Proceedings of the Third Workshop on Abusive Language Online_, pages 80–93, Florence, Italy. Association for Computational Linguistics.\n\n","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/DFS-BFS":{"title":"DFS, BFS","content":"\n\u003e [!note] Note  \n\u003e DFS와 BFS는 대표적인 그래프 탐색 알고리즘이다.\n\n## 깊이 우선 탐색(DFS)\n- DFS는 그래프에서 **깊은 부분을 우선적으로 탐색하는 알고리즘**이다.\n- DFS는 **스택 자료구조(혹은 재귀 함수)** 를 이용하며, 구체적인 동작 과정은 다음과 같다.\n\t1. 탐색 시작 노드를 스택에 삽입하고 방문 처리한다.\n\t2. 스택의 최상단 노드에 방문하지 않은 인접한 노드가 하나라도 있으면 그 노드를 스택에 넣고 방문 처리한다. 방문하지 않은 인접 노드가 없으면 스택에서 최상단 노드를 꺼낸다.\n\t3. 더 이상의 2번의 과정을 수행할 수 없을 때까지 반복한다.\n\n```python\n#각 노드가 연결된 정보를 표현(2차원 리스트)\ngraph = [\n\t\t [],      #편의를 위해 하나 더 큰 크기로 만든다. \n\t\t [2,3,8], #첫번째 노드와 연결된 노드들 정보\n\t\t [1,7],\n\t\t [1,4,5],\n\t\t [3,5],\n\t\t [3,4],\n\t\t [7],\n\t\t [2,6,8],\n\t\t [1,7]\n]\n\n#각 노드가 방문된 정보를 표현(1차원 리스트)\nvisited = [False] * 9\n\n#DFS 메서드 정의\ndef dfs(graph, v, visited):\n\t\n\t#현재 노드를 방문 처리\n\tvisited[v] = True\n\t\n\t#현재 노드와 연결된 다른 노드를 재귀적으로 방문\n\tfor i in graph[v]:\n\t\tif not visited[i]:\n\t\t\tdfs(graph, i, visited)\n\n#정의된 DFS 함수 호출\ndfs(graph, 1, visited)\n```\n\n## 너비 우선 탐색(BFS)\n- BFS는 그래프에서 **가까운 노드부터 우선적으로 탐색**하는 알고리즘이다. \n- BFS는 **큐 자료구조(queue)** 를 이용하며, 구체적인 동작 과정은 다음과 같다.\n\t1. 탐색 시작 노드를 큐에 삽입하고 방문 처리를 한다.\n\t2. 큐에서 노드를 꺼낸 뒤에 해당 노드의 인접 노드 중에서 방문하지 않은 노드를 모두 큐에 삽입하고 방문 처리한다. **(한번에! 삽입한다는 것이 특징이다.)**\n\t3. 더 이상 2번의 과정을 수행할 수 없을 때까지 반복한다. \n- 방문 기준은 '번호가 낮은 인접 노드'부터 간다. \n- **'최단 거리 문제'** 를 해결하기 위한 방법으로 많이 쓰인다.\n\n```python\nfrom collections import deque\n\n#각 노드가 연결된 정보를 표현(2차원 리스트)\ngraph = [\n\t\t [],      #편의를 위해 하나 더 큰 크기로 만든다. \n\t\t [2,3,8], #첫번째 노드와 연결된 노드들 정보\n\t\t [1,7],\n\t\t [1,4,5],\n\t\t [3,5],\n\t\t [3,4],\n\t\t [7],\n\t\t [2,6,8],\n\t\t [1,7]\n]\n\n#각 노드가 방문된 정보를 표현(1차원 리스트)\nvisited = [False] * 9\n\ndef bfs(graph, start, visited):\n\tqueue = deque([start])\n\t\n\t#현재 노드를 방문처리\n\tvisited[start] = True\n\n\t#큐가 빌 때까지 반복\n\twhile queue:\n\t\t\n\t\t#큐에서 하나의 원소를 뽑아내기\n\t\tv = queue.popleft()\n\t\t\n\t\t#아직 방문하지 않은 인접한 원소들을 큐에 삽입\n\t\tfor i in graph[v]:\n\t\t\tif not visited[i]:\n\t\t\t\tqueue.append(i)\n\t\t\t\tvisited[i] = True\n\nbfs(graph, 1, visited)\n\n```","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/Dynamic-Programming":{"title":"⚙️ Dynamic Programming","content":"\n\u003e 이번 포스팅에서는 _\"한 번 계산한 문제는 다시 계산하지 않도록 한다!\"_ 는 **다이나믹 프로그래밍(Dynamic Programming, 동적 계획법이라고도 함)**에 대해서 소개해보고 이를 Python으로 구현하는 방법에 대해 알아보자.\n\n\n다이나믹 프로그래밍은 **메모리 공간을 약간 더 사용**해서 연산 속도를 비약적으로 증가시키는 방법이다. 우선 다음과 같은 2가지 조건을 만족할 때 다이나믹 프로그래밍을 사용할 수 있다.\n\n1.  **큰 문제를 작은 문제로 나눌 수 있다.**\n2.  **작은 문제에서 구한 정답은 그것을 포함하는 큰 문제에서도 동일**하다.\n\n위 조건을 만족하는 대표적인 문제가 **피보나치 수열** 문제이다. 피보나치 수열은 다음과 같은 점화식을 만족하는 수열이다.\n\n$$\na_n=a_{n−1} + a_{n−2}, a_1=1 , a_2=1\n$$\n\n다이나믹 프로그래밍의 포인트는 바로 한 번 결과를 수행한 것을 메모리에 저장해 놓고 다음에 똑같은 결과가 필요하면 그 때 다시 연산하지 않고 메모리에 저장된 그 값을 가져와 쓰는 것이다.\n\n이러한 것을 **메모제이션(캐싱) 기법**이라고도 한다. 다음은 **재귀함수**를 사용한 다이나믹 프로그래밍으로 피보나치 수열을 구현한 코드이다.\n\n```python\nimport time\n\ndp = [0] * 50\n\ndef fibo(x):\n    if x == 1 or x == 2:\n        return 1\n    if dp[x] != 0:\n        return dp[x]\n    dp[x] = fibo(x-1) + fibo(x-2)\n    return dp[x]\n\nfor num in range(5, 40, 10):\n    start = time.time()\n    res = fibo(num)\n    print(res, '-\u003e 러닝타임:', round(time.time() - start, 2), '초')\n```\n\n이렇게 재귀함수를 사용해 구현하는 다이나믹 프로그래밍 방법은 메모제이션 기법을 활용한 `Top-Down` 방식이라고 한다.\n\n즉, 큰 문제를 해결하기 위해 작은 문제를 호출하는 것이다. \n\n반면에 재귀함수를 사용하지 않고 **단순 반복문**을 사용해 다이나믹 프로그래밍을 구현할 수 있다. 하단의 코드를 살펴보자.\n\n```python\ndp = [0] * 100\n\ndp[1] = 1 # 첫 번째 항\ndp[2] = 1 # 두 번째 항\nN = 99   # 피보나치 수열의 99번째 숫자는?\n\nfor i in range(3, N+1):\n    dp[i] = dp[i-1] + dp[i-2]\n\nprint(dp[N])\n```\n\n위와 같은 방식은 작은 문제부터 차근차근 답을 도출해서 큰 문제를 해결한다고 하여 `Bottom-Up` 방식이라고 한다. 참고로 Top-Down 방식에서는 이미 수행한 결과를 저장하는 것을 `메모제이션`, Bottom-Up 방식에서는 `DP 테이블`이라고 한다.\n\n일반적으론 단순 반복문을 활용하는 **Bottom-Up 방식으로 다이나믹 프로그래밍 방법을 해결**하라고 권장한다. 만약 재귀함수를 사용하는 Top-Down 방식을 사용하다 보면 재귀 횟수 제한 오류가 걸릴 수도 있기 때문이다.","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/EDA-Visualization":{"title":"EDA \u0026 Visualization","content":"TODO","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/GLUCOSE":{"title":"GLUCOSE: GeneraLized and COntextualized Story Explanations","content":"\u003e[!info] Reference\n\u003eNasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon, David Buchanan, Lauren Berkowitz, Or Biran, and Jennifer Chu-Carroll. 2020. [GLUCOSE: GeneraLized and COntextualized Story Explanations](https://aclanthology.org/2020.emnlp-main.370). In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4569–4586, Online. Association for Computational Linguistics.\n\n이 논문은 Elemental Cognition이라는 AI기업 연구자들이 여럿 참여하였으며 2020 EMNLP 컨퍼런스에서 Honourable Mention Papers에 오른 논문이다. 블로그에 처음 포스팅하는 논문으로 이 논문을 정한 까닭은 **상식 추론 데이터셋** 을 상당히 흥미로운 방식으로 수집한 연구이기 때문이다. 인간의 인지 심리학에 영향을 받아 사건의 인과 관계를 10차원으로 정의하고, 이에 맞춰 상식 추론 데이터셋을 만들었다니...역시 EMNLP에는 아무나 투고하는 것이 아니다.  \n\n## Motivtion\n- 사람은 무언가를 읽거나 들을 때, **암시적인 상식 추론(implicit commonsense inferences)**을 만들어 무엇이 일어났고 왜 일어났는지를 이해한다.\n- 그러나 AI system은 reading comprehension이나 dialogue과 같은 태스크에 있어서 여전히 인간과 같은 상식 추론 능력을 보이지 못하고 있다.\n- 그 이유는...\n\t- ‘상식(commonsense knowledge)’을 대규모로 휙득할 방법이 없기 때문에\n\t- 그러한 지식들을 최신의 AI 시스템에 통합시킬 방법이 없기 때문에\n\n## Claims\n- GlUCOSE 상식 추론 프레임워크를 통해 이러한 보틀넥을 해결할 수 있다.\n- GLUCOSE 데이터셋은 사건의 인과 설명을 10가지 차원을 제공하며, 또한 구체적인 스토리와 일반화된 법칙을 제공한다.\n\n## Significance\n- 암시적인 상식(Commonsense knowledge)를 대규모로 수집할 방법을 수립했다.\n- 기존의 사전학습 언어모델을 GLUCOSE에 훈련시키면 처음 보는 이야기에도 인간과 비슷한 정도의 상식 추론이 가능해진다.\n\n  \n## Introduction\n\n다음같은 상황을 가정해보자.\n\n- 한 아이 앞에서 차가 방향을 틀었다.\n- 그 아이가 재빨리 자전거를 돌렸다.\n- 그 아이가 자전거에서 떨어졌다.\n- 그 아이의 무릎이 까졌다.\n\n이 이야기를 읽으며 사람은 문장간의 **'인과 추론'** 을 만들어낼 수 있다. 예를 들어,\n\n- '한 아이 앞에서 차가 방향을 틀었다.'\n- **그리고 그건** '그 아이가 재빨리 자전거를 돌렸다.'를 초래하고\n- **그리고 그건** '그 아이가 자전거에서 떨어졌다.' 를 초래하고\n- **그리고 그건** '그 아이의 무릎이 까졌다.'를 초래했다.\n\n이러한 방식으로 말이다. **사람은 이처럼 어떻게 이야기 속 특정한 사건이 특정한 결과를 이끌었는지 묘사하는 '인과적 사슬'을 만들 수 있다**\n\n그러나 AI system은 reading comprehension이나 dialogue과 같은 태스크에 있어서 여전히 인간과 같은 상식 추론 능력을 보이지 못하고 있다. 이는 두 가지 이유가 있는데, 첫째로 **암시적인 상식을 대규모로 획득할 길이 없으며**, 둘째로 그러한 지식을 **최신의 AI 시스템에 융합시킬 길이 없기** 때문이다.\n\n이러한 상황을 해결하고 나온 것이 바로 GLUCOSE이다. GLUCOSE라는 이름도 바로 이 프레임워크가 AI를 위해 할 수 있는 기능을 비유적으로 보여주고 있는데, 사람의 지식 활동이 뇌 속의 글루코스 용량에 따라 좌우되는 것처럼, AI 시스템이 기본적 사고를 할 수 있도록 하는 연료가 되라는 의미에서 글루코스라고 지었다고 한다. 좋은 논문은 역시 이름도 잘 짓는다.\n\n앞에서도 언급했지만, GLUCOSE 데이터는 아주 흥미로운 규칙으로 구축되었다. 논문의 표현을 가져오면 다음과 같다.\n  \n\u003e S라는 짧은 이야기의 X라는 선택된 문장이 주어지면, GLUCOSE는 X와 관련된, 인간의 인지 심리학에 영향을 받은 10가지 차원의 commonsense causal explanations을 정의한다.\n\n뿐만 아니라, GLUCOSE는 commonsense knowledge를 세상에 관한 ‘미니 이론’이라고 할 수 있는 **‘반정형 추론 법칙(semi-structured inference rules)'** 의 형태로 인코딩하고, 각각은 구체적인 이야기에 근거한다는 특징도 있다.","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/H-index":{"title":"H-index","content":"H-Index는 과학자의 생산성과 영향력을 나타내는 지표입니다. 어느 과학자의 H-Index를 나타내는 값인 h를 구하려고 합니다. 위키백과[1](https://school.programmers.co.kr/learn/courses/30/lessons/42747/solution_groups?language=python3\u0026type=my#fn1)에 따르면, H-Index는 다음과 같이 구합니다.\n\n어떤 과학자가 발표한 논문 `n`편 중, `h`번 이상 인용된 논문이 `h`편 이상이고 나머지 논문이 h번 이하 인용되었다면 `h`의 최댓값이 이 과학자의 H-Index입니다.\n\n어떤 과학자가 발표한 논문의 인용 횟수를 담은 배열 `citations가` 매개변수로 주어질 때, 이 과학자의 H-Index를 `return` 하도록 `solution` 함수를 작성해주세요.\n\n### 제한사항\n\n-   과학자가 발표한 논문의 수는 1편 이상 1,000편 이하입니다.\n-   논문별 인용 횟수는 0회 이상 10,000회 이하입니다.\n\n### 입출력 예\n\n| citations | return |\n| --------- | ------ |\n| [3, 0, 6, 1, 5]          |     3   |\n\n### 입출력 예 설명\n\n이 과학자가 발표한 논문의 수는 5편이고, 그중 3편의 논문은 3회 이상 인용되었습니다. 그리고 나머지 2편의 논문은 3회 이하 인용되었기 때문에 이 과학자의 H-Index는 3입니다.\n\n## 나의 풀이\n\n```python\ndef solution(citations):\n\n    citations.sort()\n\n    h = [len(citations[x:])-1 for x in range(len(citations)) if len(citations[x:]) \u003e= citations[x]]\n    if len(h) != 0:\n       return h[-1]\n    else:\n        return len(citations)\n```\n\n\u003e[!warning] Warning  \n\u003e   \n\u003e이 문제에서 h는 꼭 citations 안에 있지 않다! \n\u003e예를 들어, citations = [6, 5, 5, 5, 3, 2, 1, 0] 이면, h = 4이다.\n\n문제 정독과 테스트 케이스의 중요함을 느끼십시오...\nh가 반드시 주어지는 줄 알고 한참을 헤맸다. 이러면 실전에서는 완전 나가리다. \n솔직히 문제가 헷갈리도록 나왔다고 생각하는데, \n그것과는 별개로 문제를 잘 읽자. \n그리고 반례를 항상 잘 떠올리자. \n\n## 다른 사람의 코드\n\n```python\ndef solution(citations):\n    citations = sorted(citations)\n    l = len(citations)\n    for i in range(l):\n        if citations[i] \u003e= l-i:\n            return l-i\n    return 0\n```\n\n이게 더 효율적인 코드로 보인다. 괜히 `h list`를 만들 필요가 없다. ","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/Hate-speech-detection-and-racial-bias-mitigation-in-social-media-based-on-BERT-model":{"title":"Hate speech detection and racial bias mitigation in social media based on BERT model","content":"\n\u003e To define automated methods with a promising performance for hate speech detection in social media, Natural Language Processing (NLP) has been used jointly with classic Machine Learning (ML) [2–4] and Deep Learning (DL) techniques [6, 15, 16]. The majority of contributions in classic supervised machine learning-based methods, for hate speech detection, rely on different text mining-based features or user-based and platform-based metadata [4, 17, 18], which require them to define an applicable feature extraction method and prevent them to generalize their approach to new datasets and platforms. **However, recent advancements in deep neural networks and transfer learning approaches allow the research community to address these limitations.**\n- 소셜 미디어에서 혐오 발언 탐지를 위한 유망한 성능을 가진 자동화된 방법을 정의하기 위해, 자연어 처리(NLP)는 고전적인 머신 러닝(ML)[2–4] 및 딥 러닝(DL) 기술[6, 15, 16]과 함께 사용되어 왔다. 혐오 발언 감지를 위한 고전적인 지도 기계 학습 기반 방법의 기여의 대부분은 서로 다른 텍스트 마이닝 기반 features 또는 사용자 기반 및 플랫폼 기반 메타데이터[4, 17, 18]에 의존하며, 이는 해당 기능 추출 방법을 정의하고 새로운 데이터 세트 및 계획에 대한 접근 방식을 일반화하는 것을 어렵게 한다. **그러나 최근 심층 신경망과 전이 학습 접근법의 발전은 연구 커뮤니티가 이러한 한계를 해결할 수 있도록 한다.**\n- 자연어 처리 ← 머신 러닝 \u0026 딥러닝 ← 머신 러닝의 한계를 극복한 딥러닝\n- 이러한 순서가 자연스럽네. ","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/Hate-speech-detection-is-not-as-easy-as-you-may-think":{"title":"Hate speech detection is not as easy as you may think","content":"\n\u003e[!info] Reference  \n\u003e\n\u003eAymé Arango, Jorge Pérez, and Barbara Poblete. 2019. Hate Speech Detection is Not as Easy as You May Think: A Closer Look at Model Validation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19). Association for Computing Machinery, New York, NY, USA, 45–54. https://doi.org/10.1145/3331184.3331262\n\n---\n\n## Introduction\n\n\u003e Despite the apparent difficulty of the hate speech detection problem evidenced by social-media providers, current state-of- the-art approaches reported in the literature show near-perfect performance. Within-dataset experiments on labeled hate speech datasets using supervised learning achieve F1 scores above 93% [3, 7–9]. Nevertheless, there are only a few studies towards determining how generalizable the resulting models are, beyond the data collection upon which they were built on, nor on the factors that may affect this property [10]. Furthermore, recent literature that surveys current work also views the state-of-the-art under a more conservative and cautious light [3,10].\n- 소셜 미디어 제공자가 증명하는 혐오 발언 탐지 문제의 명백한 어려움에도 불구하고, 문헌에 보고된 현재 최첨단 접근 방식은 거의 완벽한 성능을 보여준다. 지도 학습을 사용하여 레이블링된 혐오 발언 데이터 세트에 대한 데이터 세트 내 실험은 93% 이상의 F1 점수를 달성한다. \n- **그럼에도 불구하고 결과 모델이 구축된 데이터 수집을 넘어, 결과 모델의 일반화 정도를 결정하는 데에는 몇 가지 연구만이 있다.** 또한, 현재 작업을 조사하는 최근 문헌에서도 최첨단 기술을 보다 보수적이고 신중한 관점에서 바라본다.\n\n\u003e [!note] Aim of this paper  \n\u003e   \n\u003e \"(...) measure how these models would perform on similar yet different datasets.\"\n\n- 비슷하지만 다른 데이터셋. 거기에서 모델의 성능을 보는 것. 이게 핵심이다.\n\n## Related work\n\n\u003e **There is some recent work on testing the generalization of the state-of-the-art methods to other datasets and domains [8–10].** Most of this work has been focused on Deep Learning methods. \n\u003e Agrawal and Awekar [8] test the performance of models trained on tweets [11] classifying on Wikpedia data [33] and Formspring data [34]. The authors show that transfer learning from Twitter to the two other domains performs poorly achieving less than 10% F1. \n\u003e In a similar study, Dadvar and Eckert [9] perform transfer learning from Twitter to a dataset of Youtube comments [35] showing a performance of 15% F1. \n\u003e Gröndahl et al. [10] present a comprehensive study reproducing several state-of-the-art models. \n\u003e Specially important for us is the experiment transferring Badjatiya et al.’s model [7] trained on the Waseem and Hovy’s dataset [11] to two other similarly labeled tweet datasets [16,17]. Even in this case the performance drops significantly, obtaining 33% and 47% F1 in those sets. This is a 40+% drop from the 93% F1 reported by Badjatiya et al. [7]. \n\u003e From these results, **Gröndahl et al. [10] draw as a conclusion that model architecture is less important than the type of data and labeling criteria being used.** \n\u003e In this paper our results are coherent with those of Gröndahl et al. [10]. However, we take our research a step further by investigating why this issue occurs.\n- 훌륭한 개괄이다. 이렇게 써야 하는데. ","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/Hyper-parameter-tuning":{"title":"Hyper parameter tuning","content":"\n## 그리드 탐색(Grid Search)\n-   알고리즘 내 효과적인 하이퍼파라미터 조합을 찾을 때까지, 탐색하고자 하는 하이퍼파라미터와 그에 해당하는 시도해볼 값들을 지정하여 하이퍼파라미터 튜닝을 진행하는 것\n-  사이킷런의 `GridSearchCV` 클래스를 사용하여, 미리 설정한 하이퍼파라미터 조합에 대해 교차검증을 진행하고, 이를 통해 평가를 할 수 있다\n- `RandomForestRegressor`에 `GridSearchCV` 적용하기\n\t-   3X4개의 Hyperparameter Tuning\n\t-   2X3개의 Hyperparameter Tuning\n\t-   5회 Cross Validation 진행\n\n```python\nfrom sklearn.model_selection import GridSearchCV\n\n  \n\nparam_grid = [\n# try 12 (3×4) combinations of hyperparameters\n{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n\n# then try 6 (2×3) combinations with bootstrap set as False\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}\n]\n\n  \n\nforest_reg = RandomForestRegressor(random_state=42)\n\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n\t\t\tscoring='neg_mean_squared_error',\n\t\t\treturn_train_score=True)\n\ngrid_search.fit(housing_prepared, housing_labels)\n```\n\n`RandomForestRegressor`에 `GridSearchCV `적용 후 하이퍼파라미터 별 평가 점수 확인\n\n```python\ncvres = grid_search.cv_results_\n\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n\tprint(np.sqrt(-mean_score), params)\n```\n\n```\n\u003e\u003e\u003e \n65005.182970763315 {'max_features': 2, 'n_estimators': 3} 55582.91015494046 {'max_features': 2, 'n_estimators': 10} 52745.33887865031 {'max_features': 2, 'n_estimators': 30} 60451.18914812725 {'max_features': 4, 'n_estimators': 3} 53062.818497303946 {'max_features': 4, 'n_estimators': 10} 50663.79774079741 {'max_features': 4, 'n_estimators': 30} 57998.07162873506 {'max_features': 6, 'n_estimators': 3} 52042.04702364244 {'max_features': 6, 'n_estimators': 10} 50028.060190761295 {'max_features': 6, 'n_estimators': 30} 58308.44501796401 {'max_features': 8, 'n_estimators': 3} 52082.74313186547 {'max_features': 8, 'n_estimators': 10} 50165.81805010987 {'max_features': 8, 'n_estimators': 30} 62709.54311517104 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3} 54062.01766032325 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10} 60613.541905953585 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3} 53742.988651846914 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10} 59387.46561811065 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3} 52826.41762121993 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n```\n\n\n## 랜덤 탐색(Random Search)\n- `GridSearchCV`를 진행할 하이퍼파라미터 조합의 수가 너무 많을 때, `RandomizedSearchCV`를 활용하면 조합 내에서 임의의 하이퍼파라미터를 대입하여 지정한 횟수만큼 평가하게 됨  \n- 주요 장점 2가지\n\t1.  랜덤 탐색을 1,000회 반복하면, 각 하이퍼파라미터마다 각기 다른 1,000개 경우를 탐색할 수 있음\n\t2. 반복 횟수를 단순히 조절하는 것 만으로도 하이퍼파라미터에 투입할 컴퓨팅 자원을 제어할 수 있음\n\n```python\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n'n_estimators': randint(low=1, high=200),\n'max_features': randint(low=1, high=8),\n}\n\nforest_reg = RandomForestRegressor(random_state=42)\n\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions = param_distribs, n_iter=10, cv=5,\n\t\t\t\tscoring='neg_mean_squared_error',\n\t\t\t\trandom_state=42)\n\nrnd_search.fit(housing_prepared, housing_labels)\n```\n\n```python\ncvres = rnd_search.cv_results_\n\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n\tprint(np.sqrt(-mean_score), params)\n\n\u003e\u003e\u003e\n49462.596134607906 {'max_features': 7, 'n_estimators': 180} 51676.97211565583 {'max_features': 5, 'n_estimators': 15} 50827.83871022729 {'max_features': 3, 'n_estimators': 72} 51117.698297994146 {'max_features': 5, 'n_estimators': 21} 49585.185219390754 {'max_features': 7, 'n_estimators': 122} 50836.040148806715 {'max_features': 3, 'n_estimators': 75} 50746.890270152086 {'max_features': 3, 'n_estimators': 88} 49788.190631507045 {'max_features': 5, 'n_estimators': 100} 50574.565725719985 {'max_features': 3, 'n_estimators': 150} 65153.787556165735 {'max_features': 5, 'n_estimators': 2}\n```\n\n위와 같이 랜덤 탐색을 이용하는 게 더 나은 결과가 나올 수 있다. \n\n---\n\n\u003e [!info] Useful info  \n\u003e   \n\u003e 테스트 데이터 세트로 시스템 평가하기\n\n테스트 데이터 세트에서 설명변수와 타겟변수를 얻은 후, 기존의 변환 파이프라인을 통해 `transform`을 진행 ( 테스트의 경우 `fit_transform`와 같은 학습 과정이 포함된 변환은 진행하지 않음 )\n\n```python\nfinal_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\n\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\n```\n\n```python\nfinal_rmse\n\u003e\u003e 47362.98158022501\n```\n\n오차 값을 단일 추정값으로 확인하는 것이 아니라, 얼마나 정확한지를 `Scipy.stats.t.interval()` 기반 95% 신뢰 구간 계산을 통해 점검해볼 수 있다.\n\n```python\nfrom scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,loc=squared_errors.mean(),\nscale=stats.sem(squared_errors)))\n```\n\n```python\n\u003e\u003e\u003e array([45397.61151846, 49249.98392646])\n```","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/Machine-Learning":{"title":"Machine Learning","content":"\n## ML 대표 알고리즘\n\n-   Logistic Regression\n-   Decision Tree\n-   Naïve Bayes\n-   Support Vector Machine\n-   K Nearest Neighbors\n-   Decision Tree 기반 Ensemble 모형\n\t- Random Forest\n\t- Extra Trees Boosting\n- Decision Tree 기반 Gradient Boosting 모형\n\t- [Extreme Gradient Boosting](notes/XGB%20Modeling.md)  \n\t- Light Gradient Boosting  \n\t- Categorical Gradient Boosting\n- Natural Gradient Boosting\n\n\u003e [!note] Note  \n\u003e   \n\u003e 일반적으로 **Decision Tree \u0026 Logistic Regression** 이 설명력이 좋아서 자주 쓰인다. 그러나 최근에는 **Gradient Boosting** 계열도 자주 쓰이고 있다.\n\n## ML system 종류의 대표 구분 3가지\n\n**1. 사람의 감독하에 프로그램이 훈련하는 것인지 여부** \n- [지도학습 (Supervised Learning)](notes/지도학습%20(Supervised%20Learning).md)\n\t- 분류 (Classification)\n\t- 회귀 (Regression)\n- [비지도학습 (Un-supervised Learning)](notes/비지도학습%20(Un-supervised%20Learning).md)\n- 준지도학습 (Semi-supervised Learning)\n- 강화학습 (Reinforcement Learning)\n\n**2. 실시간으로 점진적인 학습을 하는지 여부**\n- 온라인 학습 (Online Learning)  \n- 배치 학습 (Batch Learning)\n\n**3. 기존의 데이터와 새로운 데이터를 간단히 비교분석하는 것인지 혹은 훈련 데이터 셋으로부터 패턴을 발견하여 예측 모델을 만드는지 여부** \n- 사례 기반 학습 (Instance-based Learning)\n- 모델 기반 학습 (Model-based Learning)\n\n## ML 문제 해결 순서\n\n\n```mermaid\nstateDiagram-v2\n\tData_distribution --\u003e Models\n\tModels --\u003e Hyper_parameter_tuning\n\tModels --\u003e Cross_validation\n\tHyper_parameter_tuning --\u003e Training\n\tCross_validation --\u003e Training\n\tTraining --\u003e Testing\n```\n\n1. [EDA \u0026 Visualization](notes/EDA%20\u0026%20Visualization.md)\n\t1. 학습 데이터와 테스트 데이터의 분포가 동일한지 여부를 파악한다\n2. 활용할 모델 알고리즘들을 정한다\n3. [Hyper parameter tuning](notes/Hyper%20parameter%20tuning.md)을 수행한다\n4. `Cross Validation` 기반 학습을 진행한다. \n5. 유의미한 결과가 나오는 알고리즘 및 Hyper parameter Case를 기반으로 학습 데이터 전체를 학습한다.\n6. 테스트 결과를 확인한다.\n\n## Examples\n\n","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/On-Cross-Dataset-Generalization-in-Automatic-Detection-of-Online-Abuse":{"title":"On Cross-Dataset Generalization in Automatic Detection of Online Abuse","content":"\n\u003e[!info] Reference  \n\u003e\n\u003eNejadgholi, I., \u0026 Kiritchenko, S. (2020). On cross-dataset generalization in automatic detection of online abuse. _arXiv preprint arXiv:2010.07414_.\n\n---\n\n## Research Questions\n\n\u003e Test and training sets were created for each dataset by performing a stratified split of 20% vs 80%, with the larger part used for training the models. The training sets were further subdivided, keeping 1/8 shares of them as separate validation sets during development and fine-tuning of the hyper-parameters.\n\n- Fine-tuning에서의 일반적인 방법을 말하고 있다. 전체 데이터를 label 분포를 유지한 채로 `train`, `test` 으로 나누고, 이후 `train`에서 `validation`을 다시 나눈다. 특히 `test` 데이터셋은 훈련에 쓰이지 않는데, 이후 학습한 모델의 일반화 성능을 평가할 때 사용한다. 그래서 `test` 데이터셋에서 성능이 잘 나온다면, 해당 모델이 다른 데이터셋에서도 성능이 잘 나올 것이라는 가설을 세울 수 있다.  \n- 그러나 본 논문은 이 가설에 의문을 제기한다. \n\n\u003e (...) the aim here, in contrast, was to see **how well the best models (that may have learnt some dataset-specific biases) performed on other datasets.** **This was done to investigate how well state-of-the-art systems perform in a real-life scenario**, i.e., when exposed to data from other domains, with the hypothesis that a model trained on one dataset that exhibits comparatively reasonable results on other datasets can be expected to generalise well.\n\n- 이 논문의 목적은 그렇게 한 데이터셋을 잘 학습한(아마도 그 데이터셋에 내재한 편향도 잘 학습한) 모델이 다른 데이터셋에 얼마나 성능이 좋은지 보는 것이다. 이건 실제 세계에서의 상황과 유사한데, 모델은 결국 다른 도메인에서 생성된 데이터에 노출될 수 밖에 없기 때문이다. \n- 이를 통해 *'한 데이터셋을 잘 학습하여 좋은 성능을 내는 모델이라면, 다른 데이터셋에도 잘 일반화를 할 수 있을 것*'이라는 가설을 실제로 확인해보는 것이다. \n\n\n## 실험 방법\n\n\u003e To explore how well the Toxic class from the Wiki-dataset generalizes to other types of offensive behaviour, **we train a binary classifier (Toxic vs. Normal) on the Wiki-dataset (combining the train, development and test sets) and test it on the Out-of-Domain Test set.** This classifier is expected to predict a positive (Toxic) label for the instances of classes Founta-Abusive, Founta-Hateful, Waseem-Sexism and Waseem-Racism, and a negative (Normal) label for the tweets in the Founta-Normal class. We fine-tune a BERT-based classifier (Devlin et al., 2019) with a linear prediction layer, the batch size of 16 and the learning rate of 2 × 10−5 for 2 epochs.\n\n- 저자들은 Wiki-dataset으로 훈련한 모델이 다른 데이터셋에 얼마나 잘 일반화하는가를 보기 위해, binary classifier를 wiki-Dataset으로 훈련시키고 *'도메인 외 테스트셋(the Out-of-Domain Test set)'* 에 이를 테스트 했다. 모델은 BERT를 사용했다.\n\n\n## 실험 결과\n\n![Table 3](notes/images/table3.png)\n\n\u003e Results: The overall performance of the classifier on the Out-of-Domain test set is quite high: weighted macro-averaged F1 = 0.90.\n- 저자들의 예상과 달리 전체적인 Out-of-Domain test 성능은 높은 편이었다. 그러나 Waseem 데이터셋의 Sexist, Racist class를 분류하는 데에는 Wiki-Dataset의 Toxic class로 훈련된 모델이 적합하지 않다는 사실을 확인했다. \n\n### Formulation에 대한 논의 \n#key-observation\n\u003e The impact of task formulation: From task formulations described in Section 3, observe that the Wiki-dataset defines the class Toxic in a general way. The class Founta-Abusive is also a general formulation of offensive behaviour. The similarity of these two definitions is reflected clearly in our results.\n- 흥미로운 분석은 Formulation에 대한 것이다. 먼저 Wiki dataset의 Tosic class에 대한 정의는 다음과 같다 : 'The class Toxic comprises rude, hateful, aggressive, disrespectful or unreasonable comments that are likely to make a person leave a conversation'.\n- 그런데 이것이, Waseem 데이터셋의 Sexist, Racist class를 분류하기에는 다소 일반적인 정의라는 것이다. \n\n## Impact of Data Size on Generalizability\n#data-size\n\n\u003e Observe that the average accuracies remain unchanged when the dataset’s size triples at the same class balance ratio. This finding contrasts with the general assumption that more training data results in a higher classification performance.\n\n- 다음으로 저자는 또 흥미로운 포인트를 하나 더 확인했다. \n- 만약 class의 비율이 변하지 않는다면 데이터의 크기가 커지더라도 정확도(`accuracy`)는 변하지 않는다는 것이다. 이는 더 많은 훈련데이터가 항상 높은 분류 성능을 낸다는 general assumption과 반대되는 결과이다.\n\n## Discussion\n\n\u003e In the task of online abuse detection, both False Positive and False Negative errors can lead to significant harm as one threatens the freedom of speech and ruins people’s reputations, and the other ignores hurtful behaviour.\n- False Positive와 False Negative는 표현의 자유를 위협할 수 있다. \n\n\u003e We suggest evaluating each class (both positive and negative) separately taking into account the potential costs of different types of errors.\n- 그리고 저자들은 각 class를 따로 평가하는 것을 제안했다. ","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/Picking-Numbers":{"title":"Picking Numbers","content":"\n\u003e Given an array of integers, find the longest subarray where the absolute difference between any two elements is less than or equal to 1. \n\n**Example**\n\na = [1,1,2,2,4,4,5,5,5]\n\nThere are two subarrays meeting the criterion:  [1,1,2,2,] and [4,4,5,5,5]. The maximum length subarray has 5 elements.\n\n**Function Description**\n\nComplete the _pickingNumbers_ function in the editor below. \n\npickingNumbers has the following parameter(s): \n\n- int a[n] : an array of integers \n\n**Returns**\n\n- int : the length of the longest subarray that meets the criterion \n\n**Input Format**\n\nThe first line contains a single integer _n_, the size of the array _a_.   \nThe second line contains _n_ space-separated integers, each *a[i]* .\n\n**Constraints**\n- 2 \u003c= n \u003c= 100\n- 0 \u003c a[i] \u003c 100\n-   The answer will be =\u003e 2. \n\n**Sample Input 0**\n\n```\n6\n4 6 5 3 3 1\n```\n\n**Sample Output 0**\n\n3\n\n**Explanation 0**\n\nWe choose the following multiset of integers from the array: {4,3,3}. Each pair in the multiset has an absolute difference  \u003c= 1(i.e., |4-1| = 1 and |3-3| = 0), so we print the number of chosen integers, 3, as our answer.\n\n---\n## 나의 풀이\n\n```python\nimport math\n\nimport os\n\nimport random\n\nimport re\n\nimport sys\n\nfrom collections import Counter\n\n#\n\n# Complete the 'pickingNumbers' function below.\n\n#\n\n# The function is expected to return an INTEGER.\n\n# The function accepts INTEGER_ARRAY a as parameter.\n\n#\n\n  \n\ndef pickingNumbers(a):\n\n    # imput is un array of numbers.\n\n    count_nums = Counter(a)\n\n    max_num = 0\n\n    for i in range(1, 100):\n\n        max_num = max(max_num, \n\t\t\t\t  count_nums[i] + count_nums[i+1])\n\n    return max_num\n\nif __name__ == '__main__':\n\n    fptr = open(os.environ['OUTPUT_PATH'], 'w')\n\n  \n\n    n = int(input().strip())\n\n  \n\n    a = list(map(int, input().rstrip().split()))\n\n  \n\n    result = pickingNumbers(a)\n\n  \n\n    fptr.write(str(result) + '\\n')\n\n  \n\n    fptr.close()\n```\n\n\u003e [!note]  Note  \n\u003e   \n\u003e 이 문제는 `Counter()`로 일단 세고 시작하면 간단한다.\n\u003e 개수를 센 이후에는 양 옆의 크기를 더한 것이 가장 큰 경우가 답이므로.\n","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/Studying-Generalisability-Across-Abusive-Language-Detection-Datasets":{"title":"Studying Generalisability Across Abusive Language Detection Datasets","content":"\n\u003e [!note] Reference  \n\u003e \n\u003e Steve Durairaj Swamy, Anupam Jamatia, and Björn Gambäck. 2019. [Studying Generalisability across Abusive Language Detection Datasets](https://aclanthology.org/K19-1088). In _Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)_, pages 940–950, Hong Kong, China. Association for Computational Linguistics.\n\n## Previous Work\n\n\u003e Abusive language detection has served as an umbrella term for a wide variety of subtasks. Research in the field has typically focused on a particular subtask: Hate Speech (Davidson et al., 2017; Founta et al., 2018; Gao and Huang, 2017; Golbeck et al., 2017), Sexism/Racism (Waseem and Hovy, 2016), Cyberbullying (Xu et al., 2012; Dadvar et al., 2013), Trolling and Aggression (Kumar et al., 2018a), and so on. \n\u003e Datasets for these tasks have been collected from various social media platforms, such as Twitter (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018; Burnap and Williams, 2015; Golbeck et al., 2017), Facebook (Kumar et al., 2018a), Instagram (Hosseinmardi et al., 2015; Zhong et al., 2016), Yahoo! (Nobata et al., 2016; Djuric et al., 2015; Warner and Hirschberg, 2012), YouTube (Dinakar et al., 2011), and Wikipedia (Wulczyn et al., 2017), with annotation typically carried out on crowdsourcing platforms such as CrowdFlower (Figure Eight)1 and Amazon Mechanical Turk.\n- Abusive language detection에 대한 해외 연구 레퍼런스가 잘 설명되어 있다.\n\n\u003e In the ‘OffensEval’ shared task (Zampieri et al., 2019b), the use of contextual embeddings such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) exhibited the best results.\n- BERT가 등장하고 탐지 연구에 쓰이기 시작. \n\n#key-observation \n\u003e Generalisability of a model has also come under considerable scrutiny. Works such as Karan and Šnajder (2018) and Gröndahl et al. (2018) have shown that **models trained on one dataset tend to perform well only when tested on the same dataset.**\n- 이 부분이 내 연구의 핵심과 관련이 깊다. \n\n\u003e Additionally, Gröndahl et al. (2018) showed how adversarial methods such as typos and word changes could bypass existing state-of- the-art abusive language detection systems.\n- \"All you need is “love”: Evading hate speech detection\" 논문 저자이다. \n\n\u003e Fortuna et al. (2018) concurred, stating that although models perform better on the data they are trained on, slightly improved performance can be obtained when adding more training data from other social media.\n- Fortuna et al. (2018) 연구도 있었구나. 다른 도메인에서 수집한 데이터를 더 추가해 훈련시키면 더 좋은 성능을 얻게 된다고 했었네. 지금의 일반화 가능성에 대한 논의가 여기에서 출발했다.\n\n## Preliminary Feature and Model Study\n\n\u003e However, fine-tuning was carried out on the mod- els’ hyper-parameters, such as sequence length, drop out, and class weights. Test and training sets were created for each dataset by performing a stratified split of 20% vs 80%, with the larger part used for training the models. The training sets were further subdivided, keeping 1/8 shares of them as separate validation sets during devel- opment and fine-tuning of the hyper-parameters.\n- **그러나 시퀀스 길이, 드롭아웃 및 클래스 가중치와 같은 모델의 초 매개 변수에 대해 미세 조정이 수행되었다. 테스트 및 훈련 세트는 각 데이터 세트에 대해 20% 대 80%의 계층화된 분할을 수행하였고, 이후 더 큰 부분을 모델 훈련에 사용하였다.**  훈련 세트는 더욱 세분화되어 하이퍼 파라미터의 개발 및 미세 조정 중에 1/8 공유를 별도의 검증 세트로 유지하였다.\n- 모델 훈련에 대해서는 이렇게 설명하면 되겠다.\n\n\u003e The best models used a learning rate of e−5 and batch size 32 with varying maximum sequence lengths between 60 and 70. Other parameters worth mentioning are the number of epochs and the Linear Warm-up Proportion.\n- 모델 하이퍼파라미터의 경우에는 이렇게 표현하면 된다.\n- 어라, 그런데 이거 보다보니 [On Cross-Dataset Generalization in Automatic Detection of Online Abuse](notes/On%20Cross-Dataset%20Generalization%20in%20Automatic%20Detection%20of%20Online%20Abuse.md) 이랑 설명이 똑같다. \n\n![Table 1: Overview of the datasets by Davidson et al., Founta et al., Waseem and Hovy, and Zampieri et al.](Datasets-overview.png)\n- 전체 데이터셋의 개요는 위 그림과 같다. \n\n## Cross-Dataset Training and Testing\n\n![Table 4: Cross-dataset test results (accuracy and macro-F1)](Cross-dataset-test-results.png)\n\u003e Considerable performance drops can be observed when going from a large training dataset to a small test set (i.e., Founta et al.’s results when tested on the Waseem and Hovy dataset) and vice versa. This is in line with a similar conclusion by Karan and Šnajder(2018). \n- 큰 데이터셋부터 작은 데이터셋으로 갈 때 퍼포먼스 하락이 보인다. \n\n\u003e The most interesting observation is that **datasets with larger percentages of positive samples tend to** **generalise better than datasets with fewer positive samples**, in particular when **tested against dissimilar datasets**. For example, we see that the models trained on the Davidson et al. dataset, which contains a majority of offensive tags, perform well when tested on the Founta et al. dataset, which contains a majority of non-offensive tags.\n- 가장 흥미로운 관찰은 양성 샘플의 비율이 큰 데이터 세트가 특히 다른 데이터 세트에 대해 테스트할 때 양성 샘플이 적은 데이터 세트보다 더 잘 일반화되는 경향이 있다는 것이다. 예를 들어, 대다수의 공격 태그를 포함하는 Davidson 등 데이터 세트에 대해 훈련된 모델은 대다수의 비공격 태그를 포함하는 Founta 등 데이터 세트에서 테스트될 때 성능이 우수하다는 것을 알 수 있다.\n- 내 연구도 이런 것이 있나 확인해보자. \n\n## Discussion and Conclusion\n\n\u003e Second, experiments showing that datasets with larger percentages of positive samples generalise better than datasets with fewer positive samples when tested against a dissimilar dataset (at least within the same platform, e.g., Twitter), which indicates that a more balanced dataset is healthier for generalisation.\n\n\u003e **An overall conclusion is that the data is more important than the model when tackling Abusive Language Detection.**\n- 데이터, 데이터! 결국은 데이터의 중요성을 말하며 이 논문은 끝을 낸다. ","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/Towards-generalisable-hate-speech-detection":{"title":"Towards generalisable hate speech detection","content":"\n\u003e [!info] Reference  \n\u003e \n\u003e Yin, W., \u0026 Zubiaga, A. (2021). Towards generalisable hate speech detection: a review on obstacles and solutions. _PeerJ Computer Science_, _7_, e598.\n\n---\n\n## Generalisation\n\n\u003e Most if not all proposed hate speech detection models rely on supervised machine learning methods, where the ultimate purpose is for the model to learn the real relationship between features and predictions through training data, which generalises to previously unobserved inputs (Goodfellow, Bengio \u0026 Courville, 2016). The generalisation performance of a model measures how well it fulfils this purpose.\n- 제안된 혐오 발언 탐지 모델은 대부분 supervised 기계 학습 방법에 의존하며, 궁극적인 목적은 모델이 이전에 관찰되지 않은 입력으로 일반화하는 훈련 데이터를 통해 기능과 예측 사이의 실제 관계를 학습하는 것이다(Goodfellow, Bengio \u0026 Courville, 2016). 모델의 일반화 성과는 이 목적을 얼마나 잘 달성하는지 측정한다.\n\n\u003e The ultimate purpose of studying automatic hate speech detection is to facilitate the alleviation of the harms brought by online hate speech. To fulfil this purpose, hate speech detection models need to be able to deal with the constant growth and evolution of hate speech, regardless of its form, target, and speaker.\n- 자동 혐오표현 탐지를 연구하는 궁극적인 목적은 온라인 혐오표현이 가져오는 해악의 완화를 용이하게 하는 것이다. 이러한 목적을 달성하기 위해, 혐오표현 탐지 모델은 형태, 대상 및 화자에 관계없이 혐오 발언의 지속적인 성장과 진화를 처리할 수 있어야 한다.\n\n#key-observation \n\u003eRecent research has raised concerns on the generalisability of existing models (Swamy, Jamatia \u0026 Gambäck, 2019). Despite their impressive performance on their respective test sets, **the performance significantly dropped when the models are applied to a different hate speech dataset.** This means that the assumption that test data of existing datasets represent the distribution of future cases is not true, and that **the generalisation performance of existing models have been severely overestimated** (Arango, Prez \u0026 Poblete, 2020). This lack of generalisability undermines the practical value of these hate speech detection models.\n- 최근 연구는 기존 모델의 일반화 가능성에 대한 우려를 제기했다(Swamy, Jamatia \u0026 Gambeck, 2019 : [Studying Generalisability Across Abusive Language Detection Datasets](notes/Studying%20Generalisability%20Across%20Abusive%20Language%20Detection%20Datasets.md)).  **각각의 테스트 세트에서 인상적인 성능에도 불구하고 모델이 다른 혐오 음성 데이터 세트에 적용될 때 성능이 크게 떨어졌다.** 이는 기존 데이터 세트의 테스트 데이터가 미래의 사례 분포를 나타낸다는 가정이 사실이 아니며, **기존 모델의 일반화 성능이 심각하게 과대 평가되었다는 것을 의미한다**(Arango, Pres \u0026 Poblete, 2020 : [Hate speech detection is not as easy as you may think](notes/Hate%20speech%20detection%20is%20not%20as%20easy%20as%20you%20may%20think.md)). 이러한 일반성의 부족은 이러한 혐오표현 탐지 모델의 실질적인 가치를 훼손한다.\n\n\u003e[!note] Note  \n\u003e\n\u003e이 부분이 내가 하고 있는 연구의 핵심이다! 모델의 일반화 성능이 과대평가되어 있다는 것. 한국어 데이터셋과 모델로도 비슷한 결과가 나오는지 보는 것.\n\n## Data\n\n\u003e [예시 1]\n\u003e For example, in Wiegand, Ruppenhofer \u0026 Kleinbauer (2019)’s study, FastText models (Joulin et al., 2017a) trained on three datasets (Kaggle, Founta, Razavi) achieved F1 scores above 70 when tested on one another, **while models trained or tested on datasets outside this group achieved around 60 or less**.\n- 모델이 훈련된 것과 다른 데이터셋에서는 모델의 성능이 떨어진다는 결과가 있다.\n\n#key-observation \n\u003e Founta and OLID produced models that performed well on each other. The source of such differences are usually traced back to search terms (Swamy, Jamatia \u0026 Gambäck, 2019), topics covered (Nejadgholi \u0026 Kiritchenko, 2020; Pamungkas, Basile \u0026 Patti, 2020), label definitions (Pamungkas \u0026 Patti, 2019; Pamungkas, Basile \u0026 Patti, 2020; Fortuna, Soler-Company \u0026 Wanner, 2021), and data source platforms (Glavaš, Karan \u0026 Vulić, 2020; Karan \u0026 Šnajder, 2018).\n- 서로 테스트 성능이 잘 나오는 데이터셋은 그 근원을 따라가보면 알 수 있는 사실이 있다. 예를 들면,` Founta`와 `OLID` 데이터셋은 서로 비슷한 데이터를 공유하고 있다. \n\n\u003e Fortuna, Soler \u0026 Wanner (2020) used averaged word embeddings (Bojanowski et al., 2017; Mikolov et al., 2018) to compute the representations of classes from different datasets, and compared classes across datasets. **One of their observations is that Davidson’s ‘‘hate speech’’ is very different from Waseem’s ‘‘hate speech’’, ‘‘racism’’, ‘‘sexism’’, while being relatively close to HatEval’s ‘‘hate speech’’ and Kaggle’s ‘‘identity hate’’.** This echoes with experiments that showed poor generalisation of models from Waseem to HatEval (Arango, Prez \u0026 Poblete, 2020) and between Davidson and Waseem (Waseem, Thorne \u0026 Bingel, 2018; Gröndahl et al., 2018).\n- 혐오표현 데이터셋에서 자주 나오는 단어들인 'hate speech', 'racism', 'sexism' 등도 워드 임베딩을 통해 살펴보니 데이터셋마다 그 의미가 다르다는 관찰이 나왔다. 이는 당연히 모델 성능의 일반화에도 악영향을 끼쳤고 말이다. \n\n\u003e In terms of what properties of a dataset lead to more generalisable models, there are frequently mentioned factors (...)\n\n\u003e Biases in the samples are also frequently mentioned. **Wiegand, Ruppenhofer \u0026 Kleinbauer (2019) hold that less biased sampling approaches produce more generalisable models.** This was later reproduced by Razo \u0026 Kübler (2020) and also helps explain their results with the two datasets that have the least positive cases. Similarly, Pamungkas \u0026 Patti (2019) mentioned that a wider coverage of phenomena lead to more generalisable models. \n- `Wiegand, Ruppenhofer \u0026 Kleinbauer (2019)` 연구에서 언급한 바와 같이, 조금이라도 더 일반화가 잘 되는 모델을 만드려면 sampling을 덜 치우치게 해주어야 한다. 훈련시 `sampler`를 잘 만들어야겠다.\n\n\u003e Another way of looking at generalisation and similarity is by comparing differences between individual classes across datasets (Nejadgholi \u0026 Kiritchenko, 2020; Fortuna, Soler \u0026 Wanner, 2020; Fortuna, Soler-Company \u0026 Wanner, 2021), as opposed to comparing datasets as a whole.\n- 이 논문에서도 class 개별로 비교하라고 주장하는구나. [On Cross-Dataset Generalization in Automatic Detection of Online Abuse](notes/On%20Cross-Dataset%20Generalization%20in%20Automatic%20Detection%20of%20Online%20Abuse.md) 에서 주장하는 것과 맞물린다. \n\n## OBSTACLES TO GENERALISABLE HATE SPEECH DETECTION\n\n\u003e Hate speech detection, which is largely focused on social media, shares similar challenges to other social media tasks and has its specific ones, **when it comes to the grammar and vocabulary used.** Such user language style introduces challenges to generalisability at the data source, mainly by making it difficult to utilise common NLP pre-training approaches.\n- 혐오표현 탐지는 문법이나 어휘 관련해서 어려움이 많다는 특징이 있다. \n\n\u003e On social media, syntax use is generally more casual, such as the omission of punctuation (Blodgett \u0026 O’Connor, 2017). Alternative spelling and expressions are also used in dialects (Blodgett \u0026 O’Connor, 2017), to save space, and to provide emotional emphasis (Baziotis, Pelekis \u0026 Doulkeridis, 2017). Sanguinetti et al. (2020) provided extensive guidelines for studying such phenomena syntactically.\n- 그래서 혐오표현 탐지 연구를 할 때는 KcELECTRA가 그나마 괜찮겠구나. 이러한 케이스가 많은 데이터로 사전학습된 모델이니까. 실제로 성능도 가장 괜찮고.\n\n\u003e Qian et al. (2018) found that rare words and implicit expressions are the two main causes of false negatives; Van Aken et al. (2018) compared several models that used pre-trained word embeddings, and found that rare and unknown words were present in 30% of the false negatives of Wikipedia data and 43% of Twitter data.\n- 또한 rare words, implicit expressions는 false negatives를 증가시킨다. 이는 따라서 하나의 도메인에서만 수집한 데이터셋이 가지는 한계일 수 밖에 없겠다. 이를 극복하려면 여러 도메인에서 데이터를 수집해야겠네.\n\n\u003eIndeed, BERT (Devlin et al., 2019) and its variants have demonstrated top performances at hate or abusive speech detection challenges recently (Liu, Li \u0026 Zou, 2019; Mishra \u0026 Mishra, 2019).\n- BERT 계열 모델은 혐오표현 탐지에서도 여전히 top 퍼포먼스를 보인다. \n\n\u003eIt is particularly challenging to acquire labelled data for hate speech detection as knowledge or relevant training is required of the annotators. As a high-level and abstract concept, the judgement of ‘‘hate speech’’ is subjective, needing extra care when processing annotations. Hence, datasets are usually not big in size.\n- 혐오표현 데이터셋은 '혐오표현'을 정의하는 것 자체가 주관적이기 때문에, 주석 처리에 추가적인 힘이 들고 따라서 큰 사이즈로 만들어지기 어렵다.\n\n\u003e Moreover, **different studies are based on varying definitions of ‘‘hate speech’’, as seen in different annotation guidelines** (Table 5). Despite all covering the same two main aspects (directly attack or promote hate towards), datasets vary by their wording, what they consider a target (any group, minority groups, specific minority groups), and their clarifications on edge cases.\n\u003e Davidson and HatEval both distinguished ‘‘hate speech’’ from ‘‘offensive language’’, while ‘‘uses a sexist or racist slur’’ is in Waseem’s guidelines to mark a case positive of hate, **blurring the boundary of offensive and hateful.**\n\u003e Additionally, as both HatEval and Waseem specified the types of hate (towards women and immigrants; racism and sexism), hate speech that fell outside of these specific types were not included in the positive classes, while Founta and Davidson included any type of hate speech.\n- 또한, 다른 주석 지침(표 5)에서 볼 수 있듯이, 다양한 연구는 \"혐오 발언\"의 다양한 정의를 기반으로 한다. 모든 것이 동일한 두 가지 주요 측면을 포함함에도 불구하고 데이터 세트는 표현, 대상으로 간주하는 것(모든 그룹, 소수 그룹, 특정 소수 그룹) 및 엣지 사례에 대한 명확화에 따라 다르다. \n- Davidson과 HatEval은 모두 \"hate speech\"와 \"offensive language\"를 구분했으며, \"성차별적 또는 인종차별적 비방 사용\"은 Waseem의 가이드라인에 hate로 표시하여 offensive와 hate의 경계를 모호하게 한다. \n- 또한, HatEval과 Waseem이 혐오의 유형(여성과 이민자에 대한 것; 인종 차별과 성차별)을 명시함에 따라, 이러한 특정 유형에서 벗어난 혐오 발언은 긍정적인 등급에 포함되지 않았고, 반면 Fonta와 Davidson은 모든 유형의 혐오 발언을 포함시켰다.\n\n","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/Two-Pointers":{"title":"⚙️ Two Pointers","content":"\n**투포인터 알고리즘(Two Pointers Algorithm)** 또는 **슬라이딩 윈도우(Sliding Window)** 라고 부른다.\n\n알고리즘 문제를 풀다 완전탐색으로 해결하면 시간 초과가 나는 문제가 종종 있는데, 이때 사용하면 빠르게 해결할 수 있다. 기본적인 원리는 다음과 같다.\n\n\u003e 1차원 배열이 있고, 이 배열에서 각자 다른 원소를 가리키고 있는 2개의 포인터를 조작해가면서 원하는 것을 얻기\n\nN칸의 1차원 배열이 있을 때, 부분 배열 중 그 원소의 합이 M이 되는 경우의 수를 구한다고 생각해보자. 모든 경우의 수를 다 테스트 해보면 구간 합을 구간 배열로 `O(1)`만에 구한다고 해도 경우의 수는 `O(N^2)`이 된다. 따라서 문제를 풀 수 없다. N의 최대 범위가 너무 크기 때문이다. 그러나 이 경우 각 원소는 자연수이고 M 또한 자연수인데, 따라서 다음과 같이 풀 수 있다.\n\n- 포인터 2개를 준비한다. 시작과 끝을 알 수 있도록 start, end 라고 한다.\n- 맨 처음에는 start = end = 0이며, 항상 start\u003c=end을 만족해야 한다.\n- 2개의 포인터는 현재 부분 배열의 시작과 끝을 가리키는 역할을 한다.\n\ns=e일 경우 그건 크기가 0인, 아무것도 포함하지 않는 부분 배열을 뜻한다. 다음의 과정을 s \u003c N인 동안 반복한다.\n\n1. 만약 현재 부분합이 M 이상이거나, 이미 e = N이면 s++\n2. 그렇지 않다면 e++\n3. 현재 부분합이 M과 같으면 결과 ++ \n\n쉽게 이해하자면, start와 end 를 무조건 증가시키는 방향으로만 변화시켜가면서 도중에 부분 배열의 합이 정확히 M이 되는 경우를 세는 것이다. \n\nEx) M = 5인 경우를 살펴보자.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_56/kks227_1471976777631dvMpe_PNG/1.png?type=w2)\n\n초기 상태이며, 빨간색 포인터 : start, 파란색 포인터 : end이다. S : 합.\n\n**end**가 뒤로 움직일 때는 새로 포함한 원소를 S에 더하고, **start**가 뒤로 움직일 때는 새로 넘긴 원소를 S에서 빼는 식으로 현재 [start, end)의 합 S를 매번 쉽게 구할 수 있다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_196/kks227_1471976777962Qks67_PNG/2.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_274/kks227_1471976778508STsIS_PNG/3.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_118/kks227_1471976778842HkF4H_PNG/4.png?type=w2)\n\n처음에는 이렇게 end만 증가하게 된다. S가 계속 M보다 작기 때문! 마지막엔 S\u003e=M이 되었으므로 아래와 같다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_101/kks227_1471976779156aosTT_PNG/5.png?type=w2)\n\nstart를 한 칸 옮겼는데, 동시에 S = 5인 경우를 만났다. 이때 결과를 1 증가시켜 준다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_186/kks227_1471976779456z8WVP_PNG/6.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_1/kks227_1471976779887ko5yw_PNG/7.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_265/kks227_1471976780291PDw0Y_PNG/8.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_139/kks227_1471976780603hkxD5_PNG/9.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_284/kks227_1471976780877YjQiA_PNG/10.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_101/kks227_1471976781212P3Li0_PNG/11.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_188/kks227_14719767815252r1eQ_PNG/12.png?type=w2)\n\n이런 식으로 포인터들이 움직이게 된다. 여기서 2번째로 S = 5인 지점을 만났으므로 결과를 1 증가시켜 준다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_80/kks227_14719767817475h0eo_PNG/13.png?type=w2)\n\n그 직후, start가 1 증가하면서 start = end인 경우가 나온다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_240/kks227_1471976782107sRHbv_PNG/14.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_240/kks227_14719767826459iErQ_PNG/15.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_192/kks227_1471976782977RS8E6_PNG/16.png?type=w2)\n\n계속 가다 보면 세 번째로 S = 5인 지점을 만난다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_147/kks227_1471976783270H1Bah_PNG/17.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_44/kks227_1471976783607C4F3g_PNG/18.png?type=w2)\n\n그 이후 조건에 맞춰 포인터를 증가시키다 보면, end가 배열 끝을 가리키게 되어 더이상 증가할 수 없는 상태가 된다.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_197/kks227_1471976784071FLqRR_PNG/19.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_87/kks227_14719767845214em80_PNG/20.png?type=w2)\n\n그렇게 되면 그냥 start만 증가시켜 가다가 start 역시 배열 끝에 다다르면 종료해도 되고, 그냥 그 자리에서 루프를 끝내버려도 된다. 이렇게 해서 S = 5인 경우는 3개 발견되었다.\n\n- 시간 복잡도 \n  - 이 알고리즘은 매 루프마다 항상 두 포인터 중 하나는 1씩 증가하고 있고, 각 포인터가 N번 누적 증가해야 알고리즘이 끝난다. 따라서 각각 배열 끝에 다다르는데 `O(N)`이라서 합쳐도 `O(N)`이 된다.\n\n- **추천 문제(백준 기준)**\n  - 2003 : 수들의 합\n  - 1644 : 소수의 연속합\n  - 1806 : 부분합\n  - 2230 : 수 고르기\n  - 1484 : 다이어트\n  - 2038 : 골룽 수열\n  - 2531 : 회전 초밥\n  - 2096 : 내려가기\n  - 2293 : 동전1\n\n---\n### Reference\n- https://github.com/WooVictory/Ready-For-Tech-Interview/blob/master/Algorithm/투포인터%20알고리즘.md\n","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/XGB-Modeling":{"title":"XGB Modeling","content":"\n## 1. XGB explained\n\n### XGBoost의 특징\n\n- 병렬처리 가능\n- GPU지원이 가능\n- 추가적으로 **정규화 기능, Tree pruning 기능, Early Stopping, 내장된 교차검증과 결측치 처리** 등\n\n### XGBoost의 대표적인 파라미터\n\n**다룰 수 있는 파라미터가 많기 때문에 Customizing이 용이하다.**\n\n- n_estimators (int) : 내부에서 생성할 결정 트리의 개수\n- max_depth (int) : 생성할 결정 트리의 높이\n- learning_rate (float) : 훈련량, 학습 시 모델을 얼마나 업데이트할지 결정하는 값\n- colsample_bytree (float) : 열 샘플링에 사용하는 비율\n- subsample (float) : 행 샘플링에 사용하는 비율\n- reg_alpha (float) : L1 정규화 계수\n- reg_lambda (float) : L2 정규화 계수\n- booster (str) : 부스팅 방법 (gblinear / gbtree / dart)\n- random_state (int) : 내부적으로 사용되는 난수값\n- n_jobs (int) : 병렬처리에 사용할 CPU 수\n\n## 2. Usage\n\n### 분류 문제\n```python\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\n# 원래 여기 데이터에는 검증 데이터를 넣어야함 Test 데이터 넣으면 안됨!\n# 검증 데이터 넣어주어서 교차검증 해보도록하기\nevals = [(x_test, y_test)]\nxgb_wrapper = XGBClassifier(n_estimators=100, learning_rate=0.1,\n                           max_depth=3)\n                           \n# eval_metric넣어주면서 검증 데이터로 loss 측정할 때 사용할 metric 지정\nxgb_wrapper.fit(x_train, y_train, early_stopping_rounds=200,\n               eval_set=evals, eval_metric='auc')\n# Prediction 1\nprint('Train Score : {}'.format(xgb_wrapper.score(X_train, y_train)))\nprint('Test Score : {}'.format(xgb_wrapper.score(X_test,y_test)))\n\n# Prediction 2\npreds = xgb_wrapper.predict(x_test)\npreds_proba = xgb_wrapper.predict_proba(x_test)[:, 1]\nprint(preds_proba[:10])\n\n# 모델 평가\nxgb_roc_score = roc_auc_score(\n    y_test,\n    xgb_wrapper.predict_proba(X_test)[:, 1]\n)\n```\n\n### Missing value는?\n\nXGBoost는 누락 값에 대해서 어떻게 대응할 지 알아서 정하도록 설정되어 있다. 따라서 우리가 해줘야 되는 가공 작업은 단순히 **누락값을 전부 0으로 바꿔주는 것**이다. \n\n```python\n# 누락된 값이 있는 행의 개수 = 누락 데이터 개수\nlen(df.loc[df['Total_Charges'] == ' '])\n\n# 누락된 행 직접보기\ndf.loc[df['Total_Charges'] == ' ']\n\n# 직접 바꾸기\ndf.loc[(df['Total_Charges'] == ' '), 'Total_Charges'] = 0\n```\n\n### Categorical values는?\n\n```python\npd.get_dummies(X, columns = ['Payment_Method'])\n```\n\n### 성능 알아보기\n```python\nval_error = mean_squared_error(y_val, y_pred)\nprint(\"XGB's Validation MSE:\", val_error)\n```\n\n### 회귀 문제\n```python\nfrom xgboost import XGBRegressor # XGBRegressor 모델 선언 후 Fitting\nxgbr = XGBRegressor() \nxgbr.fit(x_train, y_train) # Fitting된 모델로x_valid를 통해 예측을 진행 \n\n# Prediction 1\nprint('Train Score : {}'.format(xgbr.score(X_train, y_train)))\nprint('Test Score : {}'.format(xgbr.score(X_test,y_test)))\n\n# Prediction 2\ny_pred = xgbr.predict(x_valid)\n```\n\nXGBoost는 feature별 중요도를 plot 해주는 라이브러리를 개별적으로 제공한다. feature별 중요도를 보기 위해서 간단하게 시각화하는 코드는 다음과 같다.\n\n```python\n# feature별 중요도 시각화하기\nfrom xgboost import plot_importance\n\nfig, ax = plt.subplots(figsize=(9,11))\nplot_importance(xgb_wrapper, ax)\n```\n","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/coding-test":{"title":"👩‍💻 Coding Test","content":"\n## 프로그래머스 문제 모음\n\n### Level 1\n- [숫자 문자열과 영단어](notes/숫자%20문자열과%20영단어.md) #Kakao \n- [키패드 누르기](notes/키패드%20누르기.md)\n\n### Level 2\n- [기능개발](notes/기능개발.md)\n- [올바른 괄호](notes/올바른%20괄호.md)\n- [다음 큰 숫자](notes/다음%20큰%20숫자.md)\n- [영어 끝말잇기](notes/영어%20끝말잇기.md)\n- [구명보트](notes/구명보트.md)\n- [멀리뛰기](notes/멀리뛰기.md)\n- [두 큐 합 같게 만들기](notes/두%20큐%20합%20같게%20만들기.md) #Kakao \n- [H-index](notes/H-index.md)\n- [숫자 카드 나누기](notes/숫자%20카드%20나누기.md)\n- [타겟 넘버](notes/타겟%20넘버.md)\n- [스킬트리](notes/스킬트리.md)\n\n### Level 3\n- [정수 삼각형](notes/정수%20삼각형.md)\n\n\n## HackerRank\n\n### Basic\n- [Picking Numbers](notes/Picking%20Numbers.md)","lastmodified":"2022-11-18T10:45:30.280414132Z","tags":null},"/notes/paper-review":{"title":"📑 Paper Review","content":"\n## Contents\n\n- [📄 GLUCOSE](notes/GLUCOSE.md)\n- [📄 On Cross-Dataset Generalization in Automatic Detection of Online Abuse](notes/On%20Cross-Dataset%20Generalization%20in%20Automatic%20Detection%20of%20Online%20Abuse.md) \n- [📄 Towards generalisable hate speech detection](notes/Towards%20generalisable%20hate%20speech%20detection.md)\n- [📄 Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge](notes/Call%20for%20Customized%20Conversation.md)\n- [📄 Challenges and frontiers in abusive content detect](notes/Challenges%20and%20frontiers%20in%20abusive%20content%20detect.md)\n- [📄 Studying Generalisability Across Abusive Language Detection Datasets](notes/Studying%20Generalisability%20Across%20Abusive%20Language%20Detection%20Datasets.md)\n- [📄 Hate speech detection is not as easy as you may think](notes/Hate%20speech%20detection%20is%20not%20as%20easy%20as%20you%20may%20think.md)","lastmodified":"2022-11-18T10:45:30.284414164Z","tags":null}}