{"/":{"title":"ğŸŒ± Hayul's digital garden","content":"\nì•ˆë…•í•˜ì„¸ìš”?  \nì €ì˜ ê°œë°œ ë¸”ë¡œê·¸, **'Hayul's digital garden'** ì…ë‹ˆë‹¤.  \nì•„ë˜ **Contents**ë¥¼ í†µí•´ í¬ìŠ¤íŒ…ì„ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n\n## Contents\n\n###  [ğŸ‘©â€ğŸ’» Coding Test](notes/coding-test.md)\n###  [ğŸ“‘ Paper Review](notes/paper-review.md)\n### [âš™ï¸ Algorithms](notes/Algorithms.md)\n### [ğŸ¦¾ Machine Learning](notes/Machine%20Learning.md)","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/%EA%B5%AC%EB%AA%85%EB%B3%B4%ED%8A%B8":{"title":"êµ¬ëª…ë³´íŠ¸","content":"ë¬´ì¸ë„ì— ê°‡íŒ ì‚¬ëŒë“¤ì„ êµ¬ëª…ë³´íŠ¸ë¥¼ ì´ìš©í•˜ì—¬ êµ¬ì¶œí•˜ë ¤ê³  í•©ë‹ˆë‹¤. êµ¬ëª…ë³´íŠ¸ëŠ” ì‘ì•„ì„œ í•œ ë²ˆì— ìµœëŒ€Â **2ëª…**ì”© ë°–ì— íƒˆ ìˆ˜ ì—†ê³ , ë¬´ê²Œ ì œí•œë„ ìˆìŠµë‹ˆë‹¤.\n\nì˜ˆë¥¼ ë“¤ì–´, ì‚¬ëŒë“¤ì˜ ëª¸ë¬´ê²Œê°€ [70kg, 50kg, 80kg, 50kg]ì´ê³  êµ¬ëª…ë³´íŠ¸ì˜ ë¬´ê²Œ ì œí•œì´ 100kgì´ë¼ë©´ 2ë²ˆì§¸ ì‚¬ëŒê³¼ 4ë²ˆì§¸ ì‚¬ëŒì€ ê°™ì´ íƒˆ ìˆ˜ ìˆì§€ë§Œ 1ë²ˆì§¸ ì‚¬ëŒê³¼ 3ë²ˆì§¸ ì‚¬ëŒì˜ ë¬´ê²Œì˜ í•©ì€ 150kgì´ë¯€ë¡œ êµ¬ëª…ë³´íŠ¸ì˜ ë¬´ê²Œ ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ê°™ì´ íƒˆ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nêµ¬ëª…ë³´íŠ¸ë¥¼ ìµœëŒ€í•œ ì ê²Œ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì‚¬ëŒì„ êµ¬ì¶œí•˜ë ¤ê³  í•©ë‹ˆë‹¤.\n\nì‚¬ëŒë“¤ì˜ ëª¸ë¬´ê²Œë¥¼ ë‹´ì€ ë°°ì—´ peopleê³¼ êµ¬ëª…ë³´íŠ¸ì˜ ë¬´ê²Œ ì œí•œ limitê°€ ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§ˆ ë•Œ, ëª¨ë“  ì‚¬ëŒì„ êµ¬ì¶œí•˜ê¸° ìœ„í•´ í•„ìš”í•œ êµ¬ëª…ë³´íŠ¸ ê°œìˆ˜ì˜ ìµœì†Ÿê°’ì„ return í•˜ë„ë¡ solution í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n## ì œí•œì‚¬í•­\n\n-   ë¬´ì¸ë„ì— ê°‡íŒ ì‚¬ëŒì€ 1ëª… ì´ìƒ 50,000ëª… ì´í•˜ì…ë‹ˆë‹¤.\n-   ê° ì‚¬ëŒì˜ ëª¸ë¬´ê²ŒëŠ” 40kg ì´ìƒ 240kg ì´í•˜ì…ë‹ˆë‹¤.\n-   êµ¬ëª…ë³´íŠ¸ì˜ ë¬´ê²Œ ì œí•œì€ 40kg ì´ìƒ 240kg ì´í•˜ì…ë‹ˆë‹¤.\n-   êµ¬ëª…ë³´íŠ¸ì˜ ë¬´ê²Œ ì œí•œì€ í•­ìƒ ì‚¬ëŒë“¤ì˜ ëª¸ë¬´ê²Œ ì¤‘ ìµœëŒ“ê°’ë³´ë‹¤ í¬ê²Œ ì£¼ì–´ì§€ë¯€ë¡œ ì‚¬ëŒë“¤ì„ êµ¬ì¶œí•  ìˆ˜ ì—†ëŠ” ê²½ìš°ëŠ” ì—†ìŠµë‹ˆë‹¤.\n\n## ì…ì¶œë ¥ ì˜ˆ\n\n| people           | limit | return |\n| ---------------- | ----- | ------ |\n| [70, 50, 80, 50] | 100   | 3      |\n| [70, 80, 50]     | 100   | 3      |\n\n\n## ë‚˜ì˜ í’€ì´\n\n```python\ndef solution(people, limit) :\n    gone = 0\n    people.sort()\n\n    a = 0\n    b = len(people) - 1\n    \n    while a \u003c b :\n        if people[b] + people[a] \u003c= limit :\n            a += 1\n            gone += 1\n        b -= 1\n    \n    answer = len(people) - gone\n    \n    return answer\n```\n\nì´ ë¬¸ì œì—ì„œ ì‚¬ìš©ëœ  [Two Pointers](notes/Two%20Pointers.md) ì•Œê³ ë¦¬ì¦˜ì€ 1ì°¨ì› ë°°ì—´ì—ì„œ ë‘ ê°œì˜ í¬ì¸í„°ë¥¼ ì¡°ì‘í•˜ì—¬ ê¸°ì¡´ì˜ ë°©ì‹ë³´ë‹¤ ë¹ ë¥´ê²Œ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ì–´ë‚´ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.\n","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null},"/notes/%EA%B8%B0%EB%8A%A5%EA%B0%9C%EB%B0%9C":{"title":"ê¸°ëŠ¥ê°œë°œ","content":"\n-  í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ íŒ€ì—ì„œëŠ” ê¸°ëŠ¥ ê°œì„  ì‘ì—…ì„ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤. ê° ê¸°ëŠ¥ì€ ì§„ë„ê°€ 100%ì¼ ë•Œ ì„œë¹„ìŠ¤ì— ë°˜ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n- ë˜, ê° ê¸°ëŠ¥ì˜ ê°œë°œì†ë„ëŠ” ëª¨ë‘ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ë’¤ì— ìˆëŠ” ê¸°ëŠ¥ì´ ì•ì— ìˆëŠ” ê¸°ëŠ¥ë³´ë‹¤ ë¨¼ì € ê°œë°œë  ìˆ˜ ìˆê³ , ì´ë•Œ ë’¤ì— ìˆëŠ” ê¸°ëŠ¥ì€ ì•ì— ìˆëŠ” ê¸°ëŠ¥ì´ ë°°í¬ë  ë•Œ í•¨ê»˜ ë°°í¬ë©ë‹ˆë‹¤.  \n- ë¨¼ì € ë°°í¬ë˜ì–´ì•¼ í•˜ëŠ” ìˆœì„œëŒ€ë¡œ ì‘ì—…ì˜ ì§„ë„ê°€ ì íŒ ì •ìˆ˜ ë°°ì—´ progressesì™€ ê° ì‘ì—…ì˜ ê°œë°œ ì†ë„ê°€ ì íŒ ì •ìˆ˜ ë°°ì—´ speedsê°€ ì£¼ì–´ì§ˆ ë•Œ ê° ë°°í¬ë§ˆë‹¤ ëª‡ ê°œì˜ ê¸°ëŠ¥ì´ ë°°í¬ë˜ëŠ”ì§€ë¥¼ `return` í•˜ë„ë¡ `solution` í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.  \n\n## ì œí•œ ì‚¬í•­\n\n-   ì‘ì—…ì˜ ê°œìˆ˜(progresses, speedsë°°ì—´ì˜ ê¸¸ì´)ëŠ” 100ê°œ ì´í•˜ì…ë‹ˆë‹¤.\n-   ì‘ì—… ì§„ë„ëŠ” 100 ë¯¸ë§Œì˜ ìì—°ìˆ˜ì…ë‹ˆë‹¤.\n-   ì‘ì—… ì†ë„ëŠ” 100 ì´í•˜ì˜ ìì—°ìˆ˜ì…ë‹ˆë‹¤.\n-   ë°°í¬ëŠ” í•˜ë£¨ì— í•œ ë²ˆë§Œ í•  ìˆ˜ ìˆìœ¼ë©°, í•˜ë£¨ì˜ ëì— ì´ë£¨ì–´ì§„ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì§„ë„ìœ¨ì´ 95%ì¸ ì‘ì—…ì˜ ê°œë°œ ì†ë„ê°€ í•˜ë£¨ì— 4%ë¼ë©´ ë°°í¬ëŠ” 2ì¼ ë’¤ì— ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\n\n## ì…ì¶œë ¥ ì˜ˆ\n\n**ì…ì¶œë ¥ ì˜ˆ #1**\nì²« ë²ˆì§¸ ê¸°ëŠ¥ì€ 93% ì™„ë£Œë˜ì–´ ìˆê³  í•˜ë£¨ì— 1%ì”© ì‘ì—…ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ 7ì¼ê°„ ì‘ì—… í›„ ë°°í¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.ë‘ ë²ˆì§¸ ê¸°ëŠ¥ì€ 30%ê°€ ì™„ë£Œë˜ì–´ ìˆê³  í•˜ë£¨ì— 30%ì”© ì‘ì—…ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ 3ì¼ê°„ ì‘ì—… í›„ ë°°í¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ì „ ì²« ë²ˆì§¸ ê¸°ëŠ¥ì´ ì•„ì§ ì™„ì„±ëœ ìƒíƒœê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì²« ë²ˆì§¸ ê¸°ëŠ¥ì´ ë°°í¬ë˜ëŠ” 7ì¼ì§¸ ë°°í¬ë©ë‹ˆë‹¤.ì„¸ ë²ˆì§¸ ê¸°ëŠ¥ì€ 55%ê°€ ì™„ë£Œë˜ì–´ ìˆê³  í•˜ë£¨ì— 5%ì”© ì‘ì—…ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ 9ì¼ê°„ ì‘ì—… í›„ ë°°í¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\në”°ë¼ì„œ 7ì¼ì§¸ì— 2ê°œì˜ ê¸°ëŠ¥, 9ì¼ì§¸ì— 1ê°œì˜ ê¸°ëŠ¥ì´ ë°°í¬ë©ë‹ˆë‹¤.\n\n**ì…ì¶œë ¥ ì˜ˆ #2**\nëª¨ë“  ê¸°ëŠ¥ì´ í•˜ë£¨ì— 1%ì”© ì‘ì—…ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ, ì‘ì—…ì´ ëë‚˜ê¸°ê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜ëŠ” ê°ê° 5ì¼, 10ì¼, 1ì¼, 1ì¼, 20ì¼, 1ì¼ì…ë‹ˆë‹¤. ì–´ë–¤ ê¸°ëŠ¥ì´ ë¨¼ì € ì™„ì„±ë˜ì—ˆë”ë¼ë„ ì•ì— ìˆëŠ” ëª¨ë“  ê¸°ëŠ¥ì´ ì™„ì„±ë˜ì§€ ì•Šìœ¼ë©´ ë°°í¬ê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\n\në”°ë¼ì„œ 5ì¼ì§¸ì— 1ê°œì˜ ê¸°ëŠ¥, 10ì¼ì§¸ì— 3ê°œì˜ ê¸°ëŠ¥, 20ì¼ì§¸ì— 2ê°œì˜ ê¸°ëŠ¥ì´ ë°°í¬ë©ë‹ˆë‹¤.\n\n## í’€ì´\n\n```python\ndef solution(progresses, speeds):\n    Q=[]\n    for p, s in zip(progresses, speeds):\n        if len(Q)==0 or Q[-1][0]\u003c-((p-100)//s):\n            Q.append([-((p-100)//s),1])\n        else:\n            Q[-1][1]+=1\n    return [q[1] for q in Q]\n```\n","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null},"/notes/%EB%8B%A4%EC%9D%8C-%ED%81%B0-%EC%88%AB%EC%9E%90":{"title":"ë‹¤ìŒ í° ìˆ«ì","content":"\nìì—°ìˆ˜ nì´ ì£¼ì–´ì¡Œì„ ë•Œ, nì˜ ë‹¤ìŒ í° ìˆ«ìëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ í•©ë‹ˆë‹¤.\n\n-   ì¡°ê±´ 1. nì˜ ë‹¤ìŒ í° ìˆ«ìëŠ” në³´ë‹¤ í° ìì—°ìˆ˜ ì…ë‹ˆë‹¤.\n-   ì¡°ê±´ 2. nì˜ ë‹¤ìŒ í° ìˆ«ìì™€ nì€ 2ì§„ìˆ˜ë¡œ ë³€í™˜í–ˆì„ ë•Œ 1ì˜ ê°¯ìˆ˜ê°€ ê°™ìŠµë‹ˆë‹¤.\n-   ì¡°ê±´ 3. nì˜ ë‹¤ìŒ í° ìˆ«ìëŠ” ì¡°ê±´ 1, 2ë¥¼ ë§Œì¡±í•˜ëŠ” ìˆ˜ ì¤‘ ê°€ì¥ ì‘ì€ ìˆ˜ ì…ë‹ˆë‹¤.\n\nì˜ˆë¥¼ ë“¤ì–´ì„œ 78(1001110)ì˜ ë‹¤ìŒ í° ìˆ«ìëŠ” 83(1010011)ì…ë‹ˆë‹¤.\n\n**ìì—°ìˆ˜ nì´ ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§ˆ ë•Œ, nì˜ ë‹¤ìŒ í° ìˆ«ìë¥¼ return í•˜ëŠ” solution í•¨ìˆ˜ë¥¼ ì™„ì„±í•´ì£¼ì„¸ìš”.**\n\n### ì œí•œ ì‚¬í•­\n-   nì€ 1,000,000 ì´í•˜ì˜ ìì—°ìˆ˜ ì…ë‹ˆë‹¤.\n\n### ì…ì¶œë ¥ ì˜ˆ\n| n   | result |\n| --- | ------ |\n| 78  | 83     |\n| 15    |      23  |\n#### ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…\n- ì…ì¶œë ¥ ì˜ˆ#1  \n\t- ë¬¸ì œ ì˜ˆì‹œì™€ ê°™ìŠµë‹ˆë‹¤.  \n- ì…ì¶œë ¥ ì˜ˆ#2  \n\t- 15(1111)ì˜ ë‹¤ìŒ í° ìˆ«ìëŠ” 23(10111)ì…ë‹ˆë‹¤.\n--- \n\n## ì¢‹ì€ í’€ì´\n  \n```python\ndef solution(n):\n    num1 = bin(n).count('1')\n    while True:\n        n = n + 1\n        if num1 == bin(n).count('1'):\n            break\n    return n\n```\n  \n## ë‚˜ì˜ í’€ì´\n  \n```python\nfrom collections import Counter\n\ndef find_one(k):\n    return Counter(list(format(k, 'b')))['1']\n\ndef solution(n):\n    answer = n\n    while True:\n        answer += 1\n        if find_one(n) == find_one(answer):\n            break\n    return answer\n```\n   \n## ë‘˜ì˜ ì°¨ì´ì \nìš°ì„  `Counter()`ë¥¼ ì“¸ ì´ìœ ê°€ ì—†ì—ˆë‹¤. ì™œëƒí•˜ë©´ ë¬¸ìì—´ ë’¤ì—ëŠ”` .count()`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. êµ³ì´ `Counter()`ë¥¼ ì“°ê¸° ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ë„ í•„ìš”ê°€ ì—†ì—ˆë˜ ê²ƒì´ë‹¤. ì¦‰, ì—°ì‚° ë‚­ë¹„ì´ë‹¤. ê¸°ì–µí•˜ì, ë¬¸ìì—´ì—ì„œ íŠ¹ì •í•œ ë¬¸ìì˜ ê°¯ìˆ˜ë¥¼ ì•Œê³  ì‹¶ë‹¤ë©´ `.count()`ë¥¼ ì“°ë©´ ëœë‹¤ëŠ” ê²ƒ. ","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null},"/notes/%EB%91%90-%ED%81%90-%ED%95%A9-%EA%B0%99%EA%B2%8C-%EB%A7%8C%EB%93%A4%EA%B8%B0":{"title":"ë‘ í í•© ê°™ê²Œ ë§Œë“¤ê¸°","content":"\nê¸¸ì´ê°€ ê°™ì€ ë‘ ê°œì˜ íê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤. í•˜ë‚˜ì˜ íë¥¼ ê³¨ë¼ ì›ì†Œë¥¼ ì¶”ì¶œ(pop)í•˜ê³ , ì¶”ì¶œëœ ì›ì†Œë¥¼Â **ë‹¤ë¥¸ í**ì— ì§‘ì–´ë„£ëŠ”(insert) ì‘ì—…ì„ í†µí•´ ê° íì˜ ì›ì†Œ í•©ì´ ê°™ë„ë¡ ë§Œë“¤ë ¤ê³  í•©ë‹ˆë‹¤. ì´ë•Œ í•„ìš”í•œ ì‘ì—…ì˜ ìµœì†Œ íšŸìˆ˜ë¥¼ êµ¬í•˜ê³ ì í•©ë‹ˆë‹¤. í•œ ë²ˆì˜ popê³¼ í•œ ë²ˆì˜ insertë¥¼ í•©ì³ì„œ ì‘ì—…ì„ 1íšŒ ìˆ˜í–‰í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.\n\níëŠ” ë¨¼ì € ì§‘ì–´ë„£ì€ ì›ì†Œê°€ ë¨¼ì € ë‚˜ì˜¤ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤. ì´ ë¬¸ì œì—ì„œëŠ” íë¥¼ ë°°ì—´ë¡œ í‘œí˜„í•˜ë©°, ì›ì†Œê°€ ë°°ì—´ ì•ìª½ì— ìˆì„ìˆ˜ë¡ ë¨¼ì € ì§‘ì–´ë„£ì€ ì›ì†Œì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¦‰, popì„ í•˜ë©´ ë°°ì—´ì˜ ì²« ë²ˆì§¸ ì›ì†Œê°€ ì¶”ì¶œë˜ë©°, insertë¥¼ í•˜ë©´ ë°°ì—´ì˜ ëì— ì›ì†Œê°€ ì¶”ê°€ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ íÂ `[1, 2, 3, 4]`ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, popì„ í•˜ë©´ ë§¨ ì•ì— ìˆëŠ” ì›ì†Œ 1ì´ ì¶”ì¶œë˜ì–´Â `[2, 3, 4]`ê°€ ë˜ë©°, ì´ì–´ì„œ 5ë¥¼ insertí•˜ë©´Â `[2, 3, 4, 5]`ê°€ ë©ë‹ˆë‹¤.\n\në‹¤ìŒì€ ë‘ íë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.\n\n```\nqueue1 = [3, 2, 7, 2]\nqueue2 = [4, 6, 5, 1]\n```\n\në‘ íì— ë‹´ê¸´ ëª¨ë“  ì›ì†Œì˜ í•©ì€ 30ì…ë‹ˆë‹¤. ë”°ë¼ì„œ, ê° íì˜ í•©ì„ 15ë¡œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì´ 2ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.\n\n1.  queue2ì˜ 4, 6, 5ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œí•˜ì—¬ queue1ì— ì¶”ê°€í•œ ë’¤, queue1ì˜ 3, 2, 7, 2ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œí•˜ì—¬ queue2ì— ì¶”ê°€í•©ë‹ˆë‹¤. ê·¸ ê²°ê³¼ queue1ì€ [4, 6, 5], queue2ëŠ” [1, 3, 2, 7, 2]ê°€ ë˜ë©°, ê° íì˜ ì›ì†Œ í•©ì€ 15ë¡œ ê°™ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì‘ì—…ì„ 7ë²ˆ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n2.  queue1ì—ì„œ 3ì„ ì¶”ì¶œí•˜ì—¬ queue2ì— ì¶”ê°€í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  queue2ì—ì„œ 4ë¥¼ ì¶”ì¶œí•˜ì—¬ queue1ì— ì¶”ê°€í•©ë‹ˆë‹¤. ê·¸ ê²°ê³¼ queue1ì€ [2, 7, 2, 4], queue2ëŠ” [6, 5, 1, 3]ê°€ ë˜ë©°, ê° íì˜ ì›ì†Œ í•©ì€ 15ë¡œ ê°™ìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì‘ì—…ì„ 2ë²ˆë§Œ ìˆ˜í–‰í•˜ë©°, ì´ë³´ë‹¤ ì ì€ íšŸìˆ˜ë¡œ ëª©í‘œë¥¼ ë‹¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në”°ë¼ì„œ ê° íì˜ ì›ì†Œ í•©ì„ ê°™ê²Œ ë§Œë“¤ê¸° ìœ„í•´ í•„ìš”í•œ ì‘ì—…ì˜ ìµœì†Œ íšŸìˆ˜ëŠ” 2ì…ë‹ˆë‹¤.\n\nê¸¸ì´ê°€ ê°™ì€ ë‘ ê°œì˜ íë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜ ë°°ì—´Â `queue1`,Â `queue2`ê°€ ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§‘ë‹ˆë‹¤. ê° íì˜ ì›ì†Œ í•©ì„ ê°™ê²Œ ë§Œë“¤ê¸° ìœ„í•´ í•„ìš”í•œ ì‘ì—…ì˜ ìµœì†Œ íšŸìˆ˜ë¥¼ return í•˜ë„ë¡ solution í•¨ìˆ˜ë¥¼ ì™„ì„±í•´ì£¼ì„¸ìš”. ë‹¨, ì–´ë–¤ ë°©ë²•ìœ¼ë¡œë„ ê° íì˜ ì›ì†Œ í•©ì„ ê°™ê²Œ ë§Œë“¤ ìˆ˜ ì—†ëŠ” ê²½ìš°, -1ì„ return í•´ì£¼ì„¸ìš”.\n\n---\n\n### ì œí•œì‚¬í•­\n\n-   1 â‰¤Â `queue1`ì˜ ê¸¸ì´ =Â `queue2`ì˜ ê¸¸ì´ â‰¤ 300,000\n-   1 â‰¤Â `queue1`ì˜ ì›ì†Œ,Â `queue2`ì˜ ì›ì†Œ â‰¤ 10^9\n-   ì£¼ì˜: ì–¸ì–´ì— ë”°ë¼ í•© ê³„ì‚° ê³¼ì • ì¤‘ ì‚°ìˆ  ì˜¤ë²„í”Œë¡œìš° ë°œìƒ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ long type ê³ ë ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n\n---\n\n### ì…ì¶œë ¥ ì˜ˆ\n| queue1       | queue2        | result |\n| ------------ | ------------- | ------ |\n| [3, 2, 7, 2] | [4, 6, 5, 1]  | 2      |\n| [1, 2, 1, 2] | [1, 10, 1, 2] | 7      |\n| [1, 1]             |  [1, 5]             |   -1     |\n\n---\n\n### ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…\n\n**ì…ì¶œë ¥ ì˜ˆ #1**\n\në¬¸ì œ ì˜ˆì‹œì™€ ê°™ìŠµë‹ˆë‹¤.\n\n**ì…ì¶œë ¥ ì˜ˆ #2**\n\në‘ íì— ë‹´ê¸´ ëª¨ë“  ì›ì†Œì˜ í•©ì€ 20ì…ë‹ˆë‹¤. ë”°ë¼ì„œ, ê° íì˜ í•©ì„ 10ìœ¼ë¡œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. queue2ì—ì„œ 1, 10ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œí•˜ì—¬ queue1ì— ì¶”ê°€í•˜ê³ , queue1ì—ì„œ 1, 2, 1, 2ì™€ 1(queue2ìœ¼ë¡œë¶€í„° ë°›ì€ ì›ì†Œ)ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œí•˜ì—¬ queue2ì— ì¶”ê°€í•©ë‹ˆë‹¤. ê·¸ ê²°ê³¼ queue1ì€ [10], queue2ëŠ” [1, 2, 1, 2, 1, 2, 1]ê°€ ë˜ë©°, ê° íì˜ ì›ì†Œ í•©ì€ 10ìœ¼ë¡œ ê°™ìŠµë‹ˆë‹¤. ì´ë•Œ ì‘ì—… íšŸìˆ˜ëŠ” 7íšŒì´ë©°, ì´ë³´ë‹¤ ì ì€ íšŸìˆ˜ë¡œ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ë°©ë²•ì€ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ 7ë¥¼ return í•©ë‹ˆë‹¤.\n\n**ì…ì¶œë ¥ ì˜ˆ #3**\n\nì–´ë–¤ ë°©ë²•ì„ ì“°ë”ë¼ë„ ê° íì˜ ì›ì†Œ í•©ì„ ê°™ê²Œ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ -1ì„ return í•©ë‹ˆë‹¤.\n\n---\n\n### ë‚˜ì˜ í’€ì´\n\n```python\nfrom collections import deque\n\ndef act(queue1, queue2, reverse = None):\n    if reverse == None:\n        gone = queue1[0]\n        queue2.append(queue1.popleft())\n    else:\n        gone = queue2[0]\n        queue1.append(queue2.popleft())\n\n    return queue1, queue2, gone\n\ndef solution(queue1, queue2):\n    answer = 0\n\n    queue1 = deque(queue1)\n    queue2 = deque(queue2)\n\n    #ë¨¼ì € ê¸°ì¤€ì„ ì¡ì•„ë‘ê³  ì‹œì‘í•˜ì.\n    flag =  len(queue1)\n    left = sum(queue1) \n    right = sum(queue2)\n\n    while left != right:\n        if left \u003e right:\n            queue1, queue2, gone = act(queue1, queue2)\n            answer += 1\n            left -= gone\n            right += gone\n        elif left \u003c right:\n            queue1, queue2, gone = act(queue1, queue2,\n\t\t\t\t\t\t\t        reverse = True)\n            answer += 1\n            right -= gone\n            left += gone\n        else:\n            break\n        if answer \u003e flag * 3:\n            return -1\n\n    return answer\n```\n\nì—­ì‹œ `deque`ê°€ ë¹ ë¥´ë‹¤. ê´œíˆ ë¦¬ìŠ¤íŠ¸ë¡œ í’€ë ¤ê³  í–ˆë‹¤ê°€ ì‹œê°„ì´ˆê³¼ê°€ ìê¾¸ ë– ì„œ ì‹œê°„ë§Œ ë‚ ë ¤ë¨¹ì—ˆë„¤...ì´ë ‡ê²Œ ëª…ì‹œì ìœ¼ë¡œ `FIFO` ë†€ì´ë¥¼ ì‹œí‚¨ë‹¤ë©´ ê³ ë¯¼í•˜ì§€ë§ê³   `deque`ë¥¼ ë¶ˆëŸ¬ì˜¤ì. \n\nê·¸ê²ƒê³¼ëŠ” ë³„ê°œë¡œ, ë§ˆì§€ë§‰ `-1` ë¦¬í„´í•˜ëŠ” ì½”ë“œê°€ ë¯¸ì™„ì„±ì´ë‹¤. ì´ê±´ ì†Œìœ„ ì•¼ë§¤ë¡œ í’€ì–´ë²„ë¦°ê±°ë¼...ì´ ì ì€ ë” ìƒê°í•´ë´ì•¼ê² ë‹¤. ê·¸ë˜ë„ `act()` í•¨ìˆ˜ë¥¼ ìƒˆë¡œ ë§Œë“¤ì–´ì„œ í‘¼ ê²ƒì€ ë‚˜ì˜ì§€ ì•Šì€ ì„ íƒì´ì—ˆë˜ ê²ƒ ê°™ë‹¤. ","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null},"/notes/%EB%A9%80%EB%A6%AC%EB%9B%B0%EA%B8%B0":{"title":"ë©€ë¦¬ë›°ê¸°","content":"## ë¬¸ì œ ì„¤ëª…\n\níš¨ì§„ì´ëŠ” ë©€ë¦¬ ë›°ê¸°ë¥¼ ì—°ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤. íš¨ì§„ì´ëŠ” í•œë²ˆì— 1ì¹¸, ë˜ëŠ” 2ì¹¸ì„ ë›¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¹¸ì´ ì´ 4ê°œ ìˆì„ ë•Œ, íš¨ì§„ì´ëŠ”\n\n```\n(1ì¹¸, 1ì¹¸, 1ì¹¸, 1ì¹¸)  \n(1ì¹¸, 2ì¹¸, 1ì¹¸)  \n(1ì¹¸, 1ì¹¸, 2ì¹¸)  \n(2ì¹¸, 1ì¹¸, 1ì¹¸)  \n(2ì¹¸, 2ì¹¸)  \n```\n\nì˜ 5ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë§¨ ë ì¹¸ì— ë„ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n\n**ë©€ë¦¬ë›°ê¸°ì— ì‚¬ìš©ë  ì¹¸ì˜ ìˆ˜ nì´ ì£¼ì–´ì§ˆ ë•Œ, íš¨ì§„ì´ê°€ ëì— ë„ë‹¬í•˜ëŠ” ë°©ë²•ì´ ëª‡ ê°€ì§€ì¸ì§€ ì•Œì•„ë‚´, ì—¬ê¸°ì— 1234567ë¥¼ ë‚˜ëˆˆ ë‚˜ë¨¸ì§€ë¥¼ ë¦¬í„´í•˜ëŠ” í•¨ìˆ˜, solutionì„ ì™„ì„±í•˜ì„¸ìš”.** \n\nì˜ˆë¥¼ ë“¤ì–´ 4ê°€ ì…ë ¥ëœë‹¤ë©´, 5ë¥¼ returní•˜ë©´ ë©ë‹ˆë‹¤.\n\n**ì œí•œ ì‚¬í•­**\n\n-   nì€ 1 ì´ìƒ, 2000 ì´í•˜ì¸ ì •ìˆ˜ì…ë‹ˆë‹¤.\n\n**ì…ì¶œë ¥ ì˜ˆ**\n\n| n   | result |\n| --- | ------ |\n| 4   | 5      |\n| 3   | 3      |\n\n**ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…**\n\nì…ì¶œë ¥ ì˜ˆ #1  \nìœ„ì—ì„œ ì„¤ëª…í•œ ë‚´ìš©ê³¼ ê°™ìŠµë‹ˆë‹¤.\n\nì…ì¶œë ¥ ì˜ˆ #2\n\n(2ì¹¸, 1ì¹¸)  \n(1ì¹¸, 2ì¹¸)  \n(1ì¹¸, 1ì¹¸, 1ì¹¸)  \n\nì´ 3ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë©€ë¦¬ ë›¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## ë¬¸ì œ í’€ì´\n\nì´ ë¬¸ì œëŠ” [Dynamic Programming](notes/Dynamic%20Programming.md)ì„ ì´ìš©í•˜ëŠ” ë¬¸ì œì´ë‹¤. `dp[n]`Â ì€Â `n`ì¹¸ì„ ê°ˆ ìˆ˜ ìˆëŠ” ë°©ë²•ì˜ ìˆ˜ì´ë‹¤. ê·¸ë˜ì„œÂ `dp[1]`ë¶€í„°Â `dp[n]`ê¹Œì§€ ê²½ìš°ì˜ ìˆ˜ë¥¼ ì°¾ì•„ ë”í•´ì¤€ë‹¤. ë‘ ë²ˆì§¸ forë¬¸ì—ì„œëŠ” 1ì¹¸ë§Œìœ¼ë¡œ ê°€ëŠ” ê²½ìš°ì˜ ìˆ˜ë¥¼ ë”í•´ì£¼ê³ , 2ì¹¸ë„ ì‚¬ìš©í•´ì„œ ê°€ëŠ” ê²½ìš°ì˜ ìˆ˜ë¥¼ ë˜ ë”í•´ì¤€ë‹¤.Â \n\nê²°ê³¼ì ìœ¼ë¡œ, ê°ˆ ìˆ˜ ìˆëŠ” ì¹¸ì´Â `[1,2]`Â ì¹¸ì´ê¸° ë•Œë¬¸ì—Â `can-step`ì´ í•­ìƒÂ `can-1`Â ë˜ëŠ”Â `can-2`ê°€ ë˜ì–´ í”¼ë³´ë‚˜ì¹˜ì™€ ê°™ì•„ì§€ê²Œ ëœë‹¤. ì´ ì½”ë“œëŠ” ë§Œì•½Â `[1,2]`Â ì¹¸ì´ ì•„ë‹ˆë¼ ë” ì—¬ëŸ¬ ê°œì˜ ì¹¸ì„ ê°ˆ ìˆ˜ ìˆëŠ” ë¬¸ì œì˜€ì–´ë„ ê·¸ëŒ€ë¡œ ì ìš©í•  ìˆ˜ ìˆë‹¤.Â \n\ní”¼ë³´ë‚˜ì¹˜ë¡œ í‘¸ëŠ” ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n```python\ndef solution(n):\n    dp = [1] + [0] * n\n    dp[0], dp[1] = 1, 1\n    for i in range(2, n+1):\n        dp[i] = (dp[i-1] + dp[i-2]) % 1234567\n    return dp[n]\n```","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null},"/notes/%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5-Un-supervised-Learning":{"title":"ë¹„ì§€ë„í•™ìŠµ (Un-supervised Learning)","content":"\n## ë¹„ì§€ë„í•™ìŠµ ì˜ˆì‹œ\n\n-   **êµ°ì§‘ ( Clustering )**\n\t- K-í‰ê·  ( K-Means )  \n\t- DBSCAN  \n\t- ê³„ì¸µ êµ°ì§‘ ë¶„ì„ ( Hierarchical Cluster Analysis : HCA )\n-   **ì´ìƒì¹˜/íŠ¹ì´ì¹˜íƒì§€( Anomaly / Novelty Detection )**\n\t- One-class SCM\n\t- Isolation Forest\n-   **ì‹œê°í™” ( Visualization ) \u0026 ì°¨ì›ì¶•ì†Œ ( Dimension Reduction )**\n\t- ì£¼ì„±ë¶„ ë¶„ì„ ( Principal Component Analysis : PCA )\n\t- ì»¤ë„ PCA ( Kernel PCA )\n\t- ì§€ì—­ì  ì„ í˜• ì„ë² ë”© ( Locally Linear Embedding : LLE )\n\t- T-SNE ( t-distributed stochastic neighbor embedding )\n-   ì—°ê´€ê·œì¹™í•™ìŠµ ( Association Rule Learning )","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null},"/notes/%EC%88%AB%EC%9E%90-%EB%AC%B8%EC%9E%90%EC%97%B4%EA%B3%BC-%EC%98%81%EB%8B%A8%EC%96%B4":{"title":"ìˆ«ì ë¬¸ìì—´ê³¼ ì˜ë‹¨ì–´","content":"\n![img1.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/d31cb063-4025-4412-8cbc-6ac6909cf93e/img1.png)\n\në„¤ì˜¤ì™€ í”„ë¡œë„ê°€ ìˆ«ìë†€ì´ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë„¤ì˜¤ê°€ í”„ë¡œë„ì—ê²Œ ìˆ«ìë¥¼ ê±´ë„¬ ë•Œ ì¼ë¶€ ìë¦¿ìˆ˜ë¥¼ ì˜ë‹¨ì–´ë¡œ ë°”ê¾¼ ì¹´ë“œë¥¼ ê±´ë„¤ì£¼ë©´ í”„ë¡œë„ëŠ” ì›ë˜ ìˆ«ìë¥¼ ì°¾ëŠ” ê²Œì„ì…ë‹ˆë‹¤.  \n  \në‹¤ìŒì€ ìˆ«ìì˜ ì¼ë¶€ ìë¦¿ìˆ˜ë¥¼ ì˜ë‹¨ì–´ë¡œ ë°”ê¾¸ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.\n\n-   1478 â†’ \"one4seveneight\"\n-   234567 â†’ \"23four5six7\"\n-   10203 â†’ \"1zerotwozero3\"\n\nì´ë ‡ê²Œ ìˆ«ìì˜ ì¼ë¶€ ìë¦¿ìˆ˜ê°€ ì˜ë‹¨ì–´ë¡œ ë°”ë€Œì–´ì¡Œê±°ë‚˜, í˜¹ì€ ë°”ë€Œì§€ ì•Šê³  ê·¸ëŒ€ë¡œì¸ ë¬¸ìì—´Â `s`ê°€ ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§‘ë‹ˆë‹¤.Â `s`ê°€ ì˜ë¯¸í•˜ëŠ” ì›ë˜ ìˆ«ìë¥¼ return í•˜ë„ë¡ solution í•¨ìˆ˜ë¥¼ ì™„ì„±í•´ì£¼ì„¸ìš”.\n\n---\n### ì œí•œì‚¬í•­\n\n-   1 â‰¤Â `s`ì˜ ê¸¸ì´ â‰¤ 50\n-   `s`ê°€ \"zero\" ë˜ëŠ” \"0\"ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ëŠ” ì£¼ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.\n-   return ê°’ì´ 1 ì´ìƒ 2,000,000,000 ì´í•˜ì˜ ì •ìˆ˜ê°€ ë˜ëŠ” ì˜¬ë°”ë¥¸ ì…ë ¥ë§ŒÂ `s`ë¡œ ì£¼ì–´ì§‘ë‹ˆë‹¤.\n\n---\n### ì…ì¶œë ¥ ì˜ˆ\n\n| s                    | result      |\n| -------------------- | ----------- |\n| `\"one4seveneight\"`   | 1478        |\n| `\"23four5six7\"`      | 234567 |\n| `\"2three45sixseven\"` | 234567      |\n|        `\"123\"`              |     123        |\n\n---\n### ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…\n\n**ì…ì¶œë ¥ ì˜ˆ #1**\n\n-   ë¬¸ì œ ì˜ˆì‹œì™€ ê°™ìŠµë‹ˆë‹¤.\n\n**ì…ì¶œë ¥ ì˜ˆ #2**\n\n-   ë¬¸ì œ ì˜ˆì‹œì™€ ê°™ìŠµë‹ˆë‹¤.\n\n**ì…ì¶œë ¥ ì˜ˆ #3**\n\n-   \"three\"ëŠ” 3, \"six\"ëŠ” 6, \"seven\"ì€ 7ì— ëŒ€ì‘ë˜ê¸° ë•Œë¬¸ì— ì •ë‹µì€ ì…ì¶œë ¥ ì˜ˆ 2ì™€ ê°™ì€ 234567ì´ ë©ë‹ˆë‹¤.\n-   ì…ì¶œë ¥ ì˜ˆ 2ì™€ 3ê³¼ ê°™ì´ ê°™ì€ ì •ë‹µì„ ê°€ë¦¬í‚¤ëŠ” ë¬¸ìì—´ì´ ì—¬ëŸ¬ ê°€ì§€ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n**ì…ì¶œë ¥ ì˜ˆ #4**\n\n-   `s`ì—ëŠ” ì˜ë‹¨ì–´ë¡œ ë°”ë€ ë¶€ë¶„ì´ ì—†ìŠµë‹ˆë‹¤.\n---\n### ì œí•œì‹œê°„ ì•ˆë‚´\n-   ì •í™•ì„± í…ŒìŠ¤íŠ¸ : 10ì´ˆ\n***\n\n## ë‚´ í’€ì´\n\n```python\ndef solution(s):\n    result = ''\n    eng = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n    for idx, num in enumerate(eng):\n        if num in s:\n            s = s.replace(num, str(idx))\n        result = s\n            \n    return int(result)\n```\n\n`enumerate` ê°€ ë°˜í™˜í•˜ëŠ” ì¸ë±ìŠ¤ì™€ ì˜ë¬¸ ìˆ«ìê°€ ì¼ì¹˜í•´ì„œ ê°€ëŠ¥í•œ ë°©ë²•ì´ì—ˆë‹¤.\nì£¼ì–´ì§„ ë¬¸ìì—´ì— í•´ë‹¹ ì˜ë¬¸ ìˆ«ìê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ìˆë‹¤ë©´ `replace`ë¡œ ëŒ€ì²´í–ˆë‹¤.\nì´í›„ resultë¥¼ ê³„ì† ì—…ë°ì´íŠ¸í•˜ê³  ìµœì¢…ì ìœ¼ë¡œ `int`ë¥¼ ì”Œì›Œì„œ ë°˜í™˜í•˜ë©´ ëì´ë‹¤.","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null},"/notes/%EC%98%81%EC%96%B4-%EB%81%9D%EB%A7%90%EC%9E%87%EA%B8%B0":{"title":"ì˜ì–´ ëë§ì‡ê¸°","content":"\n1ë¶€í„° nê¹Œì§€ ë²ˆí˜¸ê°€ ë¶™ì–´ìˆëŠ” nëª…ì˜ ì‚¬ëŒì´ ì˜ì–´ ëë§ì‡ê¸°ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì˜ì–´ ëë§ì‡ê¸°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê·œì¹™ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.\n\n1. 1ë²ˆë¶€í„° ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ í•œ ì‚¬ëŒì”© ì°¨ë¡€ëŒ€ë¡œ ë‹¨ì–´ë¥¼ ë§í•©ë‹ˆë‹¤.\n2. ë§ˆì§€ë§‰ ì‚¬ëŒì´ ë‹¨ì–´ë¥¼ ë§í•œ ë‹¤ìŒì—ëŠ” ë‹¤ì‹œ 1ë²ˆë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.\n3. ì•ì‚¬ëŒì´ ë§í•œ ë‹¨ì–´ì˜ ë§ˆì§€ë§‰ ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë¥¼ ë§í•´ì•¼ í•©ë‹ˆë‹¤.\n4. ì´ì „ì— ë“±ì¥í–ˆë˜ ë‹¨ì–´ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n5. í•œ ê¸€ìì¸ ë‹¨ì–´ëŠ” ì¸ì •ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\në‹¤ìŒì€ 3ëª…ì´ ëë§ì‡ê¸°ë¥¼ í•˜ëŠ” ìƒí™©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n\ntank â†’ kick â†’ know â†’ wheel â†’ land â†’ dream â†’ mother â†’ robot â†’ tank\n\nìœ„ ëë§ì‡ê¸°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ë©ë‹ˆë‹¤.\n\n- 1ë²ˆ ì‚¬ëŒì´ ìì‹ ì˜ ì²« ë²ˆì§¸ ì°¨ë¡€ì— tankë¥¼ ë§í•©ë‹ˆë‹¤.\n- 2ë²ˆ ì‚¬ëŒì´ ìì‹ ì˜ ì²« ë²ˆì§¸ ì°¨ë¡€ì— kickì„ ë§í•©ë‹ˆë‹¤.\n- 3ë²ˆ ì‚¬ëŒì´ ìì‹ ì˜ ì²« ë²ˆì§¸ ì°¨ë¡€ì— knowë¥¼ ë§í•©ë‹ˆë‹¤.\n- 1ë²ˆ ì‚¬ëŒì´ ìì‹ ì˜ ë‘ ë²ˆì§¸ ì°¨ë¡€ì— wheelì„ ë§í•©ë‹ˆë‹¤.\n- (ê³„ì† ì§„í–‰)\n\nëë§ì‡ê¸°ë¥¼ ê³„ì† ì§„í–‰í•´ ë‚˜ê°€ë‹¤ ë³´ë©´, 3ë²ˆ ì‚¬ëŒì´ ìì‹ ì˜ ì„¸ ë²ˆì§¸ ì°¨ë¡€ì— ë§í•œ tank ë¼ëŠ” ë‹¨ì–´ëŠ” ì´ì „ì— ë“±ì¥í–ˆë˜ ë‹¨ì–´ì´ë¯€ë¡œ íƒˆë½í•˜ê²Œ ë©ë‹ˆë‹¤.\n\nì‚¬ëŒì˜ ìˆ˜ nê³¼ ì‚¬ëŒë“¤ì´ ìˆœì„œëŒ€ë¡œ ë§í•œ ë‹¨ì–´ words ê°€ ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§ˆ ë•Œ, ê°€ì¥ ë¨¼ì € íƒˆë½í•˜ëŠ” ì‚¬ëŒì˜ ë²ˆí˜¸ì™€ ê·¸ ì‚¬ëŒì´ ìì‹ ì˜ ëª‡ ë²ˆì§¸ ì°¨ë¡€ì— íƒˆë½í•˜ëŠ”ì§€ë¥¼ êµ¬í•´ì„œ return í•˜ë„ë¡ solution í•¨ìˆ˜ë¥¼ ì™„ì„±í•´ì£¼ì„¸ìš”.\n\n#### ì œí•œ ì‚¬í•­\n\n- ëë§ì‡ê¸°ì— ì°¸ì—¬í•˜ëŠ” ì‚¬ëŒì˜ ìˆ˜ nì€ 2 ì´ìƒ 10 ì´í•˜ì˜ ìì—°ìˆ˜ì…ë‹ˆë‹¤.\n- wordsëŠ” ëë§ì‡ê¸°ì— ì‚¬ìš©í•œ ë‹¨ì–´ë“¤ì´ ìˆœì„œëŒ€ë¡œ ë“¤ì–´ìˆëŠ” ë°°ì—´ì´ë©°, ê¸¸ì´ëŠ” n ì´ìƒ 100 ì´í•˜ì…ë‹ˆë‹¤.\n- ë‹¨ì–´ì˜ ê¸¸ì´ëŠ” 2 ì´ìƒ 50 ì´í•˜ì…ë‹ˆë‹¤.\n- ëª¨ë“  ë‹¨ì–´ëŠ” ì•ŒíŒŒë²³ ì†Œë¬¸ìë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n- ëë§ì‡ê¸°ì— ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ì˜ ëœ»(ì˜ë¯¸)ì€ ì‹ ê²½ ì“°ì§€ ì•Šìœ¼ì…”ë„ ë©ë‹ˆë‹¤.\n- ì •ë‹µì€ [ ë²ˆí˜¸, ì°¨ë¡€ ] í˜•íƒœë¡œ return í•´ì£¼ì„¸ìš”.\n- ë§Œì•½ ì£¼ì–´ì§„ ë‹¨ì–´ë“¤ë¡œ íƒˆë½ìê°€ ìƒê¸°ì§€ ì•ŠëŠ”ë‹¤ë©´, [0, 0]ì„ return í•´ì£¼ì„¸ìš”.\n\n----\n\n#### ì…ì¶œë ¥ ì˜ˆ\n\n| **n** | **words**                                                                                                                                                          | **result** |\n| ----- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------- |\n| 3     | [\"tank\", \"kick\", \"know\", \"wheel\", \"land\", \"dream\", \"mother\", \"robot\", \"tank\"]                                                                                      | [3,3]      |\n| 5     | [\"hello\", \"observe\", \"effect\", \"take\", \"either\", \"recognize\", \"encourage\", \"ensure\", \"establish\", \"hang\", \"gather\", \"refer\", \"reference\", \"estimate\", \"executive\"] | [0,0]      |\n| 2     | [\"hello\", \"one\", \"even\", \"never\", \"now\", \"world\", \"draw\"]                                                                                                          | [1,3]      |\n\n#### ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…\n\nì…ì¶œë ¥ ì˜ˆ #1\n\n3ëª…ì˜ ì‚¬ëŒì´ ëë§ì‡ê¸°ì— ì°¸ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n\n- 1ë²ˆ ì‚¬ëŒ : tank, wheel, mother\n- 2ë²ˆ ì‚¬ëŒ : kick, land, robot\n- 3ë²ˆ ì‚¬ëŒ : know, dream, `tank`\n\nì™€ ê°™ì€ ìˆœì„œë¡œ ë§ì„ í•˜ê²Œ ë˜ë©°, 3ë²ˆ ì‚¬ëŒì´ ìì‹ ì˜ ì„¸ ë²ˆì§¸ ì°¨ë¡€ì— ë§í•œ `tank`ë¼ëŠ” ë‹¨ì–´ê°€ 1ë²ˆ ì‚¬ëŒì´ ìì‹ ì˜ ì²« ë²ˆì§¸ ì°¨ë¡€ì— ë§í•œ `tank`ì™€ ê°™ìœ¼ë¯€ë¡œ 3ë²ˆ ì‚¬ëŒì´ ìì‹ ì˜ ì„¸ ë²ˆì§¸ ì°¨ë¡€ë¡œ ë§ì„ í•  ë•Œ ì²˜ìŒ íƒˆë½ìê°€ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤.\n\nì…ì¶œë ¥ ì˜ˆ #2\n\n5ëª…ì˜ ì‚¬ëŒì´ ëë§ì‡ê¸°ì— ì°¸ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n\n- 1ë²ˆ ì‚¬ëŒ : hello, recognize, gather\n- 2ë²ˆ ì‚¬ëŒ : observe, encourage, refer\n- 3ë²ˆ ì‚¬ëŒ : effect, ensure, reference\n- 4ë²ˆ ì‚¬ëŒ : take, establish, estimate\n- 5ë²ˆ ì‚¬ëŒ : either, hang, executive\n\nì™€ ê°™ì€ ìˆœì„œë¡œ ë§ì„ í•˜ê²Œ ë˜ë©°, ì´ ê²½ìš°ëŠ” ì£¼ì–´ì§„ ë‹¨ì–´ë¡œë§Œìœ¼ë¡œëŠ” íƒˆë½ìê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ [0, 0]ì„ returní•˜ë©´ ë©ë‹ˆë‹¤.\n\nì…ì¶œë ¥ ì˜ˆ #3\n\n2ëª…ì˜ ì‚¬ëŒì´ ëë§ì‡ê¸°ì— ì°¸ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n\n- 1ë²ˆ ì‚¬ëŒ : hello, even, `now`, draw\n- 2ë²ˆ ì‚¬ëŒ : one, never, world\n\nì™€ ê°™ì€ ìˆœì„œë¡œ ë§ì„ í•˜ê²Œ ë˜ë©°, 1ë²ˆ ì‚¬ëŒì´ ìì‹ ì˜ ì„¸ ë²ˆì§¸ ì°¨ë¡€ì— 'r'ë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ ëŒ€ì‹ , nìœ¼ë¡œ ì‹œì‘í•˜ëŠ” `now`ë¥¼ ë§í–ˆê¸° ë•Œë¬¸ì— ì´ë•Œ ì²˜ìŒ íƒˆë½ìê°€ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤.\n\n---\n\n## ë‚˜ì˜ í’€ì´\n\n```python\ndef solution(n, words):\n    for p in range(1, len(words)):\n        if words[p-1][-1] != words[p][0] or words[p] in words[:p]:\n            return [(p%n) + 1, (p//n) + 1]\n    else:\n        return([0,0])\n```\n\n### í•´ì„\nì²˜ìŒì—ëŠ” ë‹¨ì–´ì™€ ì‚¬ëŒ ë²ˆí˜¸ë¥¼ ê°™ì´ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•˜ë ¤ê³  í–ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³„ì† ì½”ë“œê°€ ë³µì¡í•´ì§€ì ìƒê°ì„ ë°”ê¾¸ê³  ìœ„ì™€ ê°™ì´ ì½”ë“œë¥¼ ìƒˆë¡œ ì‘ì„±í–ˆë‹¤. ì´ ë¬¸ì œì˜ í•µì‹¬ì€ ë‘ ê°€ì§€ì´ë‹¤.\n\n- **ì´ì „ ë‹¨ì–´ì˜ ë§ˆì§€ë§‰ ê¸€ìì™€ í˜„ì¬ ë‹¨ì–´ì˜ ì²« ê¸€ìê°€ ì¼ì¹˜í•˜ëŠ”ê°€**\n- **ì´ì „ì— ë‚˜ì™”ë˜ ë‹¨ì–´ê°€ ë‹¤ì‹œ ë‚˜ì˜¤ëŠ”ê°€**\n\nì²˜ìŒ ê²ƒì€ ì¸ë±ìŠ¤ë¡œ ê°„ë‹¨í•˜ê²Œ í™•ì¸í•˜ë©´ ë˜ëŠ”ë°, ë‘ë²ˆì§¸ ê²ƒì€ ìŠ¬ë¼ì´ì‹±ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ë°©ë²•ì´ ì‹ ì„ í•œ ì ì´ë‹¤. ê²°êµ­ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í˜„ì¬ ë‹¨ì–´ ì´ì „ì— ë˜‘ê°™ì€ ë‹¨ì–´ê°€ ìˆëŠ”ì§€ë§Œ í™•ì¸í•˜ë©´ ë˜ëŠ” ê²ƒì´ë¯€ë¡œ. `LV-2` ì´ì§€ë§Œ ì§§ê²Œ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë¬¸ì œì˜€ë‹¤.","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null},"/notes/%EC%98%AC%EB%B0%94%EB%A5%B8-%EA%B4%84%ED%98%B8":{"title":"ì˜¬ë°”ë¥¸ ê´„í˜¸","content":"\nê´„í˜¸ê°€ ë°”ë¥´ê²Œ ì§ì§€ì–´ì¡Œë‹¤ëŠ” ê²ƒì€ '(' ë¬¸ìë¡œ ì—´ë ¸ìœ¼ë©´ ë°˜ë“œì‹œ ì§ì§€ì–´ì„œ ')' ë¬¸ìë¡œ ë‹«í˜€ì•¼ í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´\n\n-   \"()()\" ë˜ëŠ” \"(())()\" ëŠ” ì˜¬ë°”ë¥¸ ê´„í˜¸ì…ë‹ˆë‹¤.\n-   \")()(\" ë˜ëŠ” \"(()(\" ëŠ” ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê´„í˜¸ì…ë‹ˆë‹¤.\n\n'(' ë˜ëŠ” ')' ë¡œë§Œ ì´ë£¨ì–´ì§„ ë¬¸ìì—´ `s`ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë¬¸ìì—´ `s`ê°€ ì˜¬ë°”ë¥¸ ê´„í˜¸ì´ë©´ `true`ë¥¼ `return` í•˜ê³ , ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê´„í˜¸ì´ë©´ `false`ë¥¼ `return` í•˜ëŠ” `solution` í•¨ìˆ˜ë¥¼ ì™„ì„±í•´ ì£¼ì„¸ìš”.\n\n## ì œí•œì‚¬í•­\n\n-   ë¬¸ìì—´ `s`ì˜ ê¸¸ì´ : 100,000 ì´í•˜ì˜ ìì—°ìˆ˜\n-   ë¬¸ìì—´ `s`ëŠ” '(' ë˜ëŠ” ')' ë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n\n\n## ì…ì¶œë ¥ ì˜ˆ\n| s        | answer |\n| -------- | ------ |\n| \"()()\"   | true   |\n| \"(())()\" | true   |\n| \")()(\"   | false  |\n| \"(()(\"   | false  |\n\n\n## ë‚˜ì˜ í’€ì´\n\n```python\ndef solution(s):\n    answer = True\n    flag = 0\n    for n in s :\n        if flag \u003c 0 :\n            return False\n        if n == '(' :\n            flag += 1\n        else :\n            flag -= 1\n    return True if flag == 0 else False\n```\n\nì´ ë¬¸ì œëŠ” ìŠ¤íƒì„ ì‚¬ìš©í•˜ëŠ” ë¬¸ì œì´ë‹¤. `flag`Â ìš©ë„ë¡œ ì“¸ ë³€ìˆ˜ë¥¼ ì„ ì–¸í•˜ê³  í•´ë‹¹ ë³€ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ê´„í˜¸ê°€ ì˜¬ë°”ë¥¸ì§€ë¥¼ ì²´í¬í–ˆë‹¤. ë§Œì•½ì—Â `flag`ê°€ ìŒìˆ˜ê°€ ë˜ëŠ” ê²½ìš°ëŠ”Â `)`ê°€ ë¨¼ì € ë‚˜ì˜¤ëŠ” ê²½ìš°ë¼ì„œ ë°”ë¡œ ì˜¬ë°”ë¥´ì§€ ì•Šê¸° ë•Œë¬¸ì—Â `False`ë¥¼ ë¦¬í„´í–ˆê³ ,Â `(`ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ëŠ”Â `flag`ë¥¼ ì¦ê°€ì‹œí‚¤ê³ Â `)`ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°Â `flag`ë¥¼ ê°ì†Œì‹œì¼œì„œ ë§Œì•½ì—Â `0`ì´ ë˜ë©´ ê´„í˜¸ê°€ ì—´ë¦°ë§Œí¼ ë‹«íŒê±°ë¼Â `True`ë¥¼ ë¦¬í„´í–ˆê³ Â `0`ë³´ë‹¤ í¬ë©´Â `(`ê°€ ë” ë§ì´ ë–´ë‹¤ëŠ” ëœ»ìœ¼ë¡œÂ `False`ë¥¼ ë¦¬í„´í–ˆë‹¤.\n","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null},"/notes/%EC%A0%95%EC%88%98-%EC%82%BC%EA%B0%81%ED%98%95":{"title":"ì •ìˆ˜ ì‚¼ê°í˜•","content":"\n![ìŠ¤í¬ë¦°ìƒ· 2018-09-14 ì˜¤í›„ 5.44.19.png](https://grepp-programmers.s3.amazonaws.com/files/production/97ec02cc39/296a0863-a418-431d-9e8c-e57f7a9722ac.png)\n\nìœ„ì™€ ê°™ì€ ì‚¼ê°í˜•ì˜ ê¼­ëŒ€ê¸°ì—ì„œ ë°”ë‹¥ê¹Œì§€ ì´ì–´ì§€ëŠ” ê²½ë¡œ ì¤‘, ê±°ì³ê°„ ìˆ«ìì˜ í•©ì´ ê°€ì¥ í° ê²½ìš°ë¥¼ ì°¾ì•„ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì•„ë˜ ì¹¸ìœ¼ë¡œ ì´ë™í•  ë•ŒëŠ” ëŒ€ê°ì„  ë°©í–¥ìœ¼ë¡œ í•œ ì¹¸ ì˜¤ë¥¸ìª½ ë˜ëŠ” ì™¼ìª½ìœ¼ë¡œë§Œ ì´ë™ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 3ì—ì„œëŠ” ê·¸ ì•„ë˜ì¹¸ì˜ 8 ë˜ëŠ” 1ë¡œë§Œ ì´ë™ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\nì‚¼ê°í˜•ì˜ ì •ë³´ê°€ ë‹´ê¸´ ë°°ì—´ `triangle`ì´ ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§ˆ ë•Œ, ê±°ì³ê°„ ìˆ«ìì˜ ìµœëŒ“ê°’ì„ `return` í•˜ë„ë¡ `solution` í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n\n### ì œí•œì‚¬í•­\n\n-   ì‚¼ê°í˜•ì˜ ë†’ì´ëŠ” 1 ì´ìƒ 500 ì´í•˜ì…ë‹ˆë‹¤.\n-   ì‚¼ê°í˜•ì„ ì´ë£¨ê³  ìˆëŠ” ìˆ«ìëŠ” 0 ì´ìƒ 9,999 ì´í•˜ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤.\n\n### ì…ì¶œë ¥ ì˜ˆ\n\n| triangle                                                  | result |\n| --------------------------------------------------------- | ------ |\n| [\\[7], [3, 8], [8, 1, 0], [2, 7, 4, 4], [4, 5, 2, 6, 5]\\] | 30     |\n\n## ë‚˜ì˜ ì˜¤ë‹µ\n\n```python\ndef solution(triangle):\n    curr_answer = 0\n    k = 0\n    for i in range(len(triangle)-1, 1, -1):\n        if k not in [0, len(triangle[i])-1]:\n            triangle[i-1][k-1] += triangle[i][k]\n            triangle[i-1][k] += triangle[i][k]\n\n        elif k == 0:\n            triangle[i-1][k] += triangle[i][k]\n        else:\n            triangle[i-1][k-1] += triangle[i][k]\n        k += 1\n        if k == len(triangle[i])-1:\n            k = 0\n    left = triangle[0][0] + triangle[1][0]\n    right = triangle[0][0] + triangle[1][1]\n    prev_answer = max(left, right)\n    if prev_answer \u003e curr_answer:\n        curr_answer = prev_answer\n    \n    return curr_answer\n```\n\nì´ê±´ ì˜¤ë‹µì´ë‹¤. \nì•„ë˜ì˜ ì •ë‹µê³¼ ë¹„êµí•˜ì. \n\n## ë‹¤ë¥¸ ì‚¬ëŒì˜ í’€ì´\n\n```python\ndef solution(triangle):\n\n    height = len(triangle)\n\n    while height \u003e 1:\n        for i in range(height - 1):\n            triangle[height-2][i] += max([triangle[height-1][i], triangle[height-1][i+1]])\n        height -= 1\n\n    answer = triangle[0][0]\n    return answer\n```\n\në‚˜ë„ ì´ë ‡ê²Œ **ì‚¼ê°í˜•ì„ ê±°ê¾¸ë¡œ ì‹œì‘í•˜ëŠ” ë°©ë²•**ì„ ìƒê°í–ˆëŠ”ë°...\n`for i in range(len(triangle)-1, 1, -1):` ì´ëŸ° ê±¸ë¡œ í•˜ë‚˜í•˜ë‚˜ ë‚´ë ¤ê°ˆ í•„ìš” ì—†ì´, `while height \u003e 1:`ìœ¼ë¡œ í•œ ë‹¤ìŒ, `height -= 1`ìœ¼ë¡œ ë‚´ë ¤ê°€ë©´ ë˜ì—ˆêµ¬ë‚˜. ê´œíˆ ë³µì¡í•˜ê²Œ ìƒê°í–ˆë‹¤. \n\nê·¸ë¦¬ê³  ë”°ë¡œ [`DP í…Œì´ë¸”`](notes/Dynamic%20Programming.md) ì„ ì‚¬ìš©í•  í•„ìš” ì—†ì´, ì•„ë˜ ì¹¸ì˜ ê°’ì„ í˜„ì¬ ì¹¸ì˜ ì¢Œ ìš°ì˜ ê°’ì˜ í•©ìœ¼ë¡œ ì €ì¥í•´ë²„ë¦¬ë©´ ëœë‹¤. ë‚˜ëŠ” ì—¬ê¸°ì—ì„œ ëº„ì…ˆì„ ì‚¬ìš©í•˜ëŠ” ë°”ëŒì— ì¡°ê±´ë¬¸ì´ ë” ì¶”ê°€ë˜ì–´ ë²„ë ¸ëŠ”ë°, ê·¸ëƒ¥ `i + 1`ë¡œ í•´ë²„ë ¤ë„ ëœë‹¤. \n","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null},"/notes/%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5-Supervised-Learning":{"title":"ì§€ë„í•™ìŠµ (Supervised Learning)","content":"## ì§€ë„í•™ìŠµ ëŒ€í‘œ ì•Œê³ ë¦¬ì¦˜\n\n1.  K-Nearest Neighbors (KNN)\n2. Linear Regression\n3. Logistic Regression\n4. Support Vector Machines (SVM)\n5. Decision Tree / Random Forest\n6. Gradient Boosting Algorithms ([XGB](notes/XGB%20Modeling.md), LGB, CatGB, NGB ë“±)\n7. Neural Networks","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null},"/notes/%ED%82%A4%ED%8C%A8%EB%93%9C-%EB%88%84%EB%A5%B4%EA%B8%B0":{"title":"í‚¤íŒ¨ë“œ ëˆ„ë¥´ê¸°","content":"\nìŠ¤ë§ˆíŠ¸í° ì „í™” í‚¤íŒ¨ë“œì˜ ê° ì¹¸ì— ë‹¤ìŒê³¼ ê°™ì´ ìˆ«ìë“¤ì´ ì í˜€ ìˆìŠµë‹ˆë‹¤.\n\n![kakao_phone1.png](https://grepp-programmers.s3.ap-northeast-2.amazonaws.com/files/production/4b69a271-5f4a-4bf4-9ebf-6ebed5a02d8d/kakao_phone1.png)\n\nì´ ì „í™” í‚¤íŒ¨ë“œì—ì„œ ì™¼ì†ê³¼ ì˜¤ë¥¸ì†ì˜ ì—„ì§€ì†ê°€ë½ë§Œì„ ì´ìš©í•´ì„œ ìˆ«ìë§Œì„ ì…ë ¥í•˜ë ¤ê³  í•©ë‹ˆë‹¤.  \në§¨ ì²˜ìŒ ì™¼ì† ì—„ì§€ì†ê°€ë½ì€Â `*`Â í‚¤íŒ¨ë“œì— ì˜¤ë¥¸ì† ì—„ì§€ì†ê°€ë½ì€Â `#`Â í‚¤íŒ¨ë“œ ìœ„ì¹˜ì—ì„œ ì‹œì‘í•˜ë©°, ì—„ì§€ì†ê°€ë½ì„ ì‚¬ìš©í•˜ëŠ” ê·œì¹™ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n1.  ì—„ì§€ì†ê°€ë½ì€ ìƒí•˜ì¢Œìš° 4ê°€ì§€ ë°©í–¥ìœ¼ë¡œë§Œ ì´ë™í•  ìˆ˜ ìˆìœ¼ë©° í‚¤íŒ¨ë“œ ì´ë™ í•œ ì¹¸ì€ ê±°ë¦¬ë¡œ 1ì— í•´ë‹¹í•©ë‹ˆë‹¤.\n2.  ì™¼ìª½ ì—´ì˜ 3ê°œì˜ ìˆ«ìÂ `1`,Â `4`,Â `7`ì„ ì…ë ¥í•  ë•ŒëŠ” ì™¼ì† ì—„ì§€ì†ê°€ë½ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n3.  ì˜¤ë¥¸ìª½ ì—´ì˜ 3ê°œì˜ ìˆ«ìÂ `3`,Â `6`,Â `9`ë¥¼ ì…ë ¥í•  ë•ŒëŠ” ì˜¤ë¥¸ì† ì—„ì§€ì†ê°€ë½ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n4.  ê°€ìš´ë° ì—´ì˜ 4ê°œì˜ ìˆ«ìÂ `2`,Â `5`,Â `8`,Â `0`ì„ ì…ë ¥í•  ë•ŒëŠ” ë‘ ì—„ì§€ì†ê°€ë½ì˜ í˜„ì¬ í‚¤íŒ¨ë“œì˜ ìœ„ì¹˜ì—ì„œ ë” ê°€ê¹Œìš´ ì—„ì§€ì†ê°€ë½ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.  \n    4-1. ë§Œì•½ ë‘ ì—„ì§€ì†ê°€ë½ì˜ ê±°ë¦¬ê°€ ê°™ë‹¤ë©´, ì˜¤ë¥¸ì†ì¡ì´ëŠ” ì˜¤ë¥¸ì† ì—„ì§€ì†ê°€ë½, ì™¼ì†ì¡ì´ëŠ” ì™¼ì† ì—„ì§€ì†ê°€ë½ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\nìˆœì„œëŒ€ë¡œ ëˆ„ë¥¼ ë²ˆí˜¸ê°€ ë‹´ê¸´ ë°°ì—´ numbers, ì™¼ì†ì¡ì´ì¸ì§€ ì˜¤ë¥¸ì†ì¡ì´ì¸ ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´ handê°€ ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§ˆ ë•Œ, ê° ë²ˆí˜¸ë¥¼ ëˆ„ë¥¸ ì—„ì§€ì†ê°€ë½ì´ ì™¼ì†ì¸ ì§€ ì˜¤ë¥¸ì†ì¸ ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì—°ì†ëœ ë¬¸ìì—´ í˜•íƒœë¡œ return í•˜ë„ë¡ solution í•¨ìˆ˜ë¥¼ ì™„ì„±í•´ì£¼ì„¸ìš”.\n\n## ì œí•œì‚¬í•­\n\n-   numbers ë°°ì—´ì˜ í¬ê¸°ëŠ” 1 ì´ìƒ 1,000 ì´í•˜ì…ë‹ˆë‹¤.\n-   numbers ë°°ì—´ ì›ì†Œì˜ ê°’ì€ 0 ì´ìƒ 9 ì´í•˜ì¸ ì •ìˆ˜ì…ë‹ˆë‹¤.\n-   handëŠ”Â `\"left\"`Â ë˜ëŠ”Â `\"right\"`Â ì…ë‹ˆë‹¤.\n    -   `\"left\"`ëŠ” ì™¼ì†ì¡ì´,Â `\"right\"`ëŠ” ì˜¤ë¥¸ì†ì¡ì´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n-   ì™¼ì† ì—„ì§€ì†ê°€ë½ì„ ì‚¬ìš©í•œ ê²½ìš°ëŠ”Â `L`, ì˜¤ë¥¸ì† ì—„ì§€ì†ê°€ë½ì„ ì‚¬ìš©í•œ ê²½ìš°ëŠ”Â `R`ì„ ìˆœì„œëŒ€ë¡œ ì´ì–´ë¶™ì—¬ ë¬¸ìì—´ í˜•íƒœë¡œ return í•´ì£¼ì„¸ìš”.\n\n## ì…ì¶œë ¥ ì˜ˆ\n\n| numbers                           | hand      | result          |\n| --------------------------------- | --------- | --------------- |\n| [1, 3, 4, 5, 8, 2, 1, 4, 5, 9, 5] | `\"right\"` | `\"LRLLLRLLRRL\"` |\n| [7, 0, 8, 2, 8, 3, 1, 5, 7, 6, 2] | `\"left\"`  | `\"LRLLRRLLLRR\"` |\n| [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]    | `\"right\"` | `\"LLRLLRLLRL\"`  |\n\n## ë‚˜ì˜ í’€ì´\n\n```python\ndef solution(numbers, hand):\n    pad = {\n        1:(0,0), 2:(0,1), 3:(0,2),\n        4:(1,0), 5:(1,1), 6:(1,2),\n        7:(2,0), 8:(2,1), 9:(2,2),\n        '*':(3,0), 0:(3,1), '#':(3,2)\n    }\n    L_prev_key = pad['*']\n    R_prev_key = pad['#']\n    answer = ''\n\n    for key in numbers:\n        if key in [1,4,7]:\n            answer += 'L'\n            L_prev_key = pad[key]\n        elif key in [3,6,9]:\n            answer += 'R'\n            R_prev_key = pad[key]\n        else:\n            L_distance = abs(L_prev_key[0] - pad[key][0]) + abs(L_prev_key[1] - pad[key][1])\n            R_distance = abs(R_prev_key[0] - pad[key][0]) + abs(R_prev_key[1] - pad[key][1])\n            if L_distance \u003c R_distance:\n                answer += 'L'\n                L_prev_key = pad[key]\n            elif L_distance \u003e R_distance:\n                answer += 'R'\n                R_prev_key = pad[key]\n            else:\n                if hand == 'right':\n                    answer += 'R'\n                    R_prev_key = pad[key]\n                else:\n                    answer += 'L'\n                    L_prev_key = pad[key]\n    return answer\n```\n\nì´ ë¬¸ì œëŠ” ì¢Œí‘œê°€ í•„ìš”í•œ ë¬¸ì œì´ë‹¤. ì¢Œí‘œ íŠœí”Œì„ ê° ë²ˆí˜¸ì— í•´ë‹¹í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•˜ê³  ì‹œì‘í•˜ë©´ ëœë‹¤. ","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null},"/notes/Algorithms":{"title":"âš™ï¸ Algorithms","content":"- [âš™ï¸ Two Pointers](notes/Two%20Pointers.md)\n- [âš™ï¸ Dynamic Programming](notes/Dynamic%20Programming.md)","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/Call-for-Customized-Conversation":{"title":"Call for Customized Conversation-Customized Conversation Grounding Persona and Knowledge","content":"\n## Abstarct\n\u003e Humans usually have conversations by making use of prior knowledge about a topic and background information of the people whom they are talking to. **However, existing conversational agents and datasets do not consider such comprehensive information, and thus they have a limitation in generating the utterances where the knowledge and persona are fused properly.** To address this issue, we introduce a call For Customized conversation **(FoCus) dataset** where the customized answers are built with the userâ€™s persona and Wikipedia knowledge. (...)\n- í˜„ì¬ ìƒí™©ê³¼ í•œê³„ë¥¼ ì°¾ì•„ë³´ì. \n\t- ì‚¬ëŒì€ ëŒ€í™”ì˜ ì£¼ì œë‚˜, ìê¸°ì™€ ë§í•˜ê³  ìˆëŠ” ì‚¬ëŒì˜ ë°°ê²½ ì •ë³´ ë“±ì— ê¸°ëŒ€ì–´ ëŒ€í™”ë¥¼ ì´ì–´ë‚˜ê°„ë‹¤. ê·¸ëŸ¬ë‚˜ í˜„ì¬ ëŒ€í™” ì‹œìŠ¤í…œì´ë‚˜ ë°ì´í„°ì…‹ì€ ì´ëŸ¬í•œ ì •ë³´ë“¤ì„ ì´í•´í•˜ì§€ ëª»í•˜ê³  ìˆìœ¼ë©°, ë”°ë¼ì„œ ì§€ì‹ì´ë‚˜ ì„±ê²©ì´ í•¨ê»˜ ì ì ˆíˆ ìœµí•©ëœ ë°œí™”ë¥¼ ìƒì„±í•˜ëŠ” ë°ì— í•œê³„ê°€ ìˆë‹¤. \n- ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì˜ ëª©ì ì€?\n\t- ì´ì— ë³¸ ë…¼ë¬¸ì€ ì´ìš©ìì˜ ì„±ê²©(persona)ì™€ Wikipediaê³¼ í•¨ê»˜ êµ¬ì¶•ëœ FoCus ë°ì´í„°ì…‹ì„ ë°œí‘œí–ˆë‹¤.\n\n\u003e We examine whether the model reflects adequate persona and knowledge with our proposed two sub-tasks, **persona grounding (PG) and knowledge grounding (KG)**.\n- ê·¸ë˜ì„œ ê·¸ê±¸ë¡œ ë­˜ í•œ ê±´ë°?\n\t- ëª¨ë¸ì´ ì ì ˆíˆ í˜ë¥´ì†Œë‚˜ì™€ ì§€ì‹ì„ ë°˜ì˜í•˜ëŠ”ì§€ í™•ì¸í•´ë³¼ ê²ƒì´ë‹¤. \n\t- `Persona grounding (PG)` ì™€ `Knowledge grounding (KG)`ë¼ê³  í•˜ëŠ” ì„œë¸Œ íƒœìŠ¤í¬ë¥¼ í†µí•´ì„œ!\n\n## FoCus Dataset\n\n![Figure 2: Example dialog between Human and Machine in FoCus dataset](Example-dialog.png)\n- ê·¸ë˜ì„œ ì´ëŸ° ëŒ€í™”ê°€ ê°€ëŠ¥í•˜ë„ë¡ í•˜ê³  ì‹¶ë‹¤ëŠ” ê²ƒì´ë‹¤. \n- ì´ìš©ìì™€ ëŒ€í™”ë¥¼ ì§„í–‰í•˜ëŠ” ì‹œìŠ¤í…œì´ ì‹¤ì œ ì§€ì‹ì— ê¸°ë°˜í•˜ë©´ì„œ('This place is called Sentosa'), ë˜í•œ ì´ìš©ìì˜ í˜ë¥´ì†Œë‚˜ì— ê·¼ê±°í•˜ì—¬ ë§ì„ ì´ì–´ë‚˜ê°€ëŠ” ê²ƒ('I believe you wish to visit Singapore.')!\n\n## Model\n\n![](Overview-of-model.png)\n\u003e We introduce the baseline models trained on our FoCus dataset, consisting of a `retrieval module` and a `dialog module`. The `retrieval module` retrieves the knowledge paragraphs related to a question, and the `dialog module` generates utterances of the machine by taking the retrieved knowledge paragraphs, humanâ€™s persona, and previous utterances as inputs.\n- ê·¸ë˜ì„œ ëª¨ë¸ ì•„í‚¤í…ì³ì—ëŠ” ë‘ ê°œì˜ ëª¨ë“ˆì´ ë“¤ì–´ê°„ë‹¤. `retrieval module` ê³¼ `dialog module` ì´ë‹¤. `retrieval module` ì€ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì§€ì‹ íŒŒë¼ê·¸ë˜í”„ë¥¼ ê²€ìƒ‰í•˜ê³ , `dialog module` ì€ ì´ê²ƒê³¼ ì´ìš©ìì˜ í˜ë¥´ì†Œë‚˜ ì •ë³´ì— ê·¼ê±°í•´ì„œ ëª¨ë¸ì˜ ë°œí™”ë¥¼ ìƒì„±í•´ë‚´ëŠ” ê²ƒì´ë‹¤.\n\n![Table 3: Experimental results](Experimental-results.png)\n- ì‹¤í—˜ ê²°ê³¼, PG, KGì— ëª¨ë‘ í›ˆë ¨ëœ BARTì™€ GPT-2ê°€ generationì—ì„œëŠ” ì¡°ê¸ˆ ì„±ëŠ¥ì´ ë‚®ì§€ë§Œ ì „ë°˜ì ìœ¼ë¡œ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, Grouding ì„œë¸Œ íƒœìŠ¤í¬ì—ì„œëŠ” ê°€ì¥ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤. \n\n## Conclusion\n\u003e We hope that the researches aim to make dialog agents more attractive and knowledgeable with grounding abilities to be explored.\n- ìš°ë¦¬ëŠ” ì—°êµ¬ìë“¤ì´ ë” ìˆì„ ì‹¤ì œì  ëŠ¥ë ¥ë“¤ê³¼ í•¨ê»˜, ëŒ€í™” ì—ì´ì „íŠ¸ë¥¼ ë”ìš± ë§¤ë ¥ì ì´ê³  ì§€ì‹ì„ í’ë¶€íˆ ê°€ì§€ë„ë¡ ë§Œë“œëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•˜ê¸°ë¥¼ í¬ë§í•œë‹¤. \n- ëë§ºìŒì€ í›ˆí›ˆí•˜ê²Œ í•˜ëŠ”êµ¬ë‚˜. ëŒ€í™” ì‹œìŠ¤í…œì€ ë”ìš± ë§¤ë ¥ì ì´ê²Œ ë  ìˆ˜ ìˆë‹¤. **ê°ì • ë¿ë§Œ ì•„ë‹ˆë¼ ì§€ì‹, í˜ë¥´ì†Œë‚˜ë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ëŒ€í™” ì‹œìŠ¤í…œì€ ì–´ë–¤ ëª¨ìŠµì´ ë ê¹Œ?**","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/Challenges-and-frontiers-in-abusive-content-detect":{"title":"Challenges and frontiers in abusive content detect","content":"\n\u003e [!note] Note  \n\u003e \n\u003e ì´ë²ˆì— ì •ë¦¬í•˜ëŠ” ë…¼ë¬¸ì€ \"abusive content detect\" ê³¼ì œê°€ ì–´ë–»ê²Œ ë°œì „í–ˆê³ , ì§€ê¸ˆ ë§ˆì£¼í•˜ê³  ìˆëŠ” ì–´ë ¤ì›€ì€ ë¬´ì—‡ì¸ì§€ ì†Œê°œí•˜ëŠ” ë…¼ë¬¸ì´ë‹¤. \n\u003e í˜ì˜¤í‘œí˜„ íƒì§€ ê³¼ì œì™€ (ì™„ì „íˆëŠ” ì•„ë‹ˆì§€ë§Œ) ë¹„ìŠ·í•œ ê³¼ì œì´ê¸°ì— ë¹„ìŠ·í•œ ì–´ë ¤ì›€ë“¤ì„ ê³µìœ í•˜ê³  ìˆë‹¤. ì£¼ì„ ì‘ì—…ì˜ ì–´ë ¤ì›€, ê°œë… ì •ì˜ì˜ ì •í™•ì„± ë“±...\n\u003e ê°œì¸ì ìœ¼ë¡œ ì¸ìƒ ê¹Šì€ ê²ƒì€ 'í›ˆë ¨ ë°ì´í„°ì™€ ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œ í…ŒìŠ¤íŠ¸ê°€ ì´ë£¨ì–´ì§€ë©´ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë³€í• ì§€'ë¥¼ ì†Œê°œí•˜ëŠ” íŒŒíŠ¸ì´ë‹¤.\n\n***\n\n\u003e Developing robust systems to detect abuse is a crucial part of online content moderation and plays a fundamental role in creating an open, safe and accessible Internet.\n-  ìš•ì„¤(abuse)ì„ íƒì§€í•˜ê¸° ìœ„í•œ ê°•ë ¥í•œ ì‹œìŠ¤í…œì„ ê°œë°œí•˜ëŠ” ê²ƒì€ ì˜¨ë¼ì¸ ì½˜í…ì¸  ì¡°ì •ì˜ ì¤‘ìš”í•œ ë¶€ë¶„ì´ë©° **ê°œë°©ì ì´ê³  ì•ˆì „í•˜ë©° ì ‘ê·¼ ê°€ëŠ¥í•œ ì¸í„°ë„·ì„ ë§Œë“œëŠ” ë° ê·¼ë³¸ì ì¸ ì—­í• ì„ í•œë‹¤**.\n\n\u003e Advances in machine learning and NLP have led to marked improvements in abusive content detection systemsâ€™ performance (Fortuna \u0026 Nunes, 2018; Schmidt \u0026 Wiegand, 2017). For instance, in 2018 Pitsilis et al. trained a classification system on Waseem and Hovyâ€™s 16,000 tweet dataset and achieved an F-Score of 0.932, compared against Waseem and Hovyâ€™s original 0.739; a 20-point increase (Pitsilis, Ramampiaro, \u0026 Langseth, 2018; Waseem \u0026 Hovy, 2016).\n-  **ë¨¸ì‹  ëŸ¬ë‹ê³¼ NLPì˜ ë°œì „ì€ ëª¨ìš• ì½˜í…ì¸  ê°ì§€ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í˜„ì €í•˜ê²Œ í–¥ìƒì‹œì¼°ë‹¤(Fortuna \u0026 Nunes, 2018; Schmidt \u0026 Wiegand, 2017).** ì˜ˆë¥¼ ë“¤ì–´, 2018ë…„ Pitsilis ë“±ì€ Wasemê³¼ Hovyì˜ 16,000ê°œì˜ íŠ¸ìœ— ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ë¶„ë¥˜ ì‹œìŠ¤í…œì„ í›ˆë ¨í•˜ê³  Wasemê³¼ Hovyì˜ ì›ë˜ 0.739ì™€ ë¹„êµí•˜ì—¬ 0.932ì˜ F-Scoreë¥¼ ë‹¬ì„±í–ˆë‹¤(Pitsilis, Ramampiaro, \u0026 Langseth, 2018; Wasem \u0026 Hovy, 2016).\n\n\u003e Researchers have also addressed numerous tasks beyond binary abusive content classification, including identifying the target of abuse and its strength as well as automatically moderating content (Burnap \u0026 Williams, 2016; Davidson, Warmsley, Macy, \u0026 Weber, 2017; Santos, Melnyk, \u0026 Padhi, 2018).\n- ì—°êµ¬ìë“¤ì€ ë˜í•œ í•™ëŒ€ ëŒ€ìƒì„ ì‹ë³„í•˜ëŠ” ê²ƒë¿ë§Œ ì•„ë‹ˆë¼ ì½˜í…ì¸  ìë™ ì¡°ì ˆì„ í¬í•¨í•˜ì—¬ ì´ì§„ ìš•ì„¤ ì½˜í…ì¸  ë¶„ë¥˜ë¥¼ ë„˜ì–´ ìˆ˜ë§ì€ ê³¼ì œë¥¼ í•´ê²°í–ˆë‹¤(Burnap \u0026 Williams, 2016; Davidson, Warmsley, Macy, Weber, 2017; Santos, Melnyk, \u0026 Padhi, 2018). \n\n\u003e (â€¦) what type of abusive content it is identified as. This is a social and theoretical task: **there is no objectively â€˜correctâ€™ definition** or single set of pre-established criteria which can be applied.\n- (...) ì–´ë–¤ ìœ í˜•ì˜ ìš•ì„¤ ì½˜í…ì¸ ë¡œ ì‹ë³„ë˜ëŠ”ì§€ë„ ì¤‘ìš”í•˜ë‹¤. ì´ëŸ¬í•œ ë¶„ë¥˜ëŠ” ì‚¬íšŒì ì´ê³  ì´ë¡ ì ì¸ ê³¼ì œì´ê¸°ì—, **ê°ê´€ì ìœ¼ë¡œ 'ì˜¬ë°”ë¥¸' ì •ì˜ë‚˜ ì ìš©í•  ìˆ˜ ìˆëŠ” ì‚¬ì „ ì„¤ì •ëœ ê¸°ì¤€ì˜ ë‹¨ì¼ ì§‘í•©ì€ ì—†ë‹¤.**\n\n\u003e Detecting abusive content generically is an important aspiration for the field. **However, it is very difficult because abusive content is so varied.** Research which purports to address the generic task of detecting abuse is typically actually addressing something much more specific. This can often be discerned from the datasets, which may contain systematic biases towards certain types and targets of abuse. For instance, the dataset by Davidson et al. is used widely for tasks described generically as abusive content detection yet it is highly skewed towards racism and sexism (Davidson et al., 2017).\n- ìš•ì„¤ ì½˜í…ì¸ ë¥¼ ì „ë°˜ì ìœ¼ë¡œ ê°ì§€í•˜ëŠ” ê²ƒì€ í˜„ì¥ì˜ ì¤‘ìš”í•œ í¬ë¶€ë‹¤. í•˜ì§€ë§Œ ì´ëŠ” ìš•ì„¤ì˜ ë‚´ìš©ì´ ë‹¤ì–‘í•˜ê¸° ë•Œë¬¸ì— ë§¤ìš° ì–´ë µë‹¤. **ìš•ì„¤ì„ íƒì§€í•˜ëŠ” ì¼ë°˜ì ì¸ ê³¼ì œë¥¼ í•´ê²°í•œë‹¤ê³  ì£¼ì¥í•˜ëŠ” ì—°êµ¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ í›¨ì”¬ ë” êµ¬ì²´ì ì¸ ê²ƒ(specific)ì„ ë‹¤ë£¨ê³  ìˆë‹¤.** ì´ê²ƒì€ ì¢…ì¢… ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì‹ë³„ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” íŠ¹ì • ìœ í˜• ë° ìš•ì„¤ ëŒ€ìƒì— ëŒ€í•œ ì²´ê³„ì ì¸ í¸ê²¬ì„ í¬í•¨í•  ìˆ˜ ìˆë‹¤. **ì˜ˆë¥¼ ë“¤ì–´, Davidson ë“±ì˜ ë°ì´í„° ì„¸íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìš•ì„¤ ì½˜í…ì¸  íƒì§€ë¡œ ì„¤ëª…ë˜ëŠ” ì‘ì—…ì— ë„ë¦¬ ì‚¬ìš©ë˜ì§€ë§Œ ì¸ì¢…ì°¨ë³„ê³¼ ì„±ì°¨ë³„ ìª½ìœ¼ë¡œ í¬ê²Œ ì¹˜ìš°ì³ ìˆë‹¤(Davidson ë“±, 2017).** \n\n\u003e Waseem et al. suggest that one of the main differences between subtasks is whether content is â€˜directed towards a specific entity or is directed towards a generalized groupâ€™ (Waseem et al., 2017).\n- `Waseem et al.`ì€ í•˜ìœ„ ì‘ì—… ê°„ì˜ ì£¼ìš” ì°¨ì´ì  ì¤‘ í•˜ë‚˜ëŠ” ì½˜í…ì¸ ê°€ 'íŠ¹ì • ì—”í‹°í‹°ë¥¼ ì§€í–¥í•˜ëŠ”ì§€, ì•„ë‹ˆë©´ ì¼ë°˜í™”ëœ ê·¸ë£¹ì„ ì§€í–¥í•˜ëŠ”ì§€'ë¼ê³  ì œì•ˆí•œë‹¤(Wasee et al., 2017).\n\n\n\u003e A key distinction is whether abuse is explicit or implicit (Waseem et al., 2017; Zampieri et al., 2019).\n\n\n\u003e Some of the main problems are (1) researchers use terms which are not well-defined, (2) different concepts and terms are used across the field for similar work, and (3) the terms which are used are theoretically problematic.\n- ì£¼ìš” ë¬¸ì œ ì¤‘ ì¼ë¶€ëŠ” (1) ì—°êµ¬ìë“¤ì´ ì˜ ì •ì˜ë˜ì§€ ì•Šì€ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ê³ , (2) ìœ ì‚¬í•œ ì‘ì—…ì— ëŒ€í•´ í˜„ì¥ ì „ë°˜ì— ê±¸ì³ ë‹¤ë¥¸ ê°œë…ê³¼ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ë©°, (3) ì‚¬ìš©ë˜ëŠ” ìš©ì–´ê°€ ì´ë¡ ì ìœ¼ë¡œ ë¬¸ì œê°€ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n\u003e **Annotation.** \n\u003e Annotation is a notoriously difficult task, reflected in the low levels of inter-annotator agreement reported by most publications, particularly on more complex multi-class tasks (Sanguinetti, Poletto, Bosco, Patti, \u0026 Stranisci, 2018). Noticeably, van Aken suggests that Davidson et al.â€™s widely used hate and offensive language dataset has up to 10% of its data mislabeled (van Aken et al., 2018).\n- ì£¼ì„ì€ ì•…ëª…ë†’ê²Œ ì–´ë ¤ìš´ ì‘ì—…ìœ¼ë¡œ, íŠ¹íˆ ë” ë³µì¡í•œ ë‹¤ì¤‘ í´ë˜ìŠ¤ ì‘ì—…ì— ëŒ€í•´ ëŒ€ë¶€ë¶„ì˜ ì—°êµ¬ë“¤ì—ì„œ ë³´ê³ í•œ ì£¼ì„ ê°„ í•©ì˜(inter-annotator agreement)ì˜ ë‚®ì€ ìˆ˜ì¤€ì— ë°˜ì˜ëœë‹¤(Sanguinetti, Poletto, Bosco, Patti, \u0026 Stranisci, 2018). ë…íŠ¹í•˜ê²Œ,  **ë°˜ ì—ì´ì¼„ì€ ë°ì´ë¹„ë“œìŠ¨ ë“±ì˜ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í˜ì˜¤ ë° ê³µê²©ì ì¸ ì–¸ì–´ ë°ì´í„° ì„¸íŠ¸ê°€ ìµœëŒ€ 10%ì˜ ë°ì´í„° ë ˆì´ë¸”ì´ ì˜ëª» ì§€ì •ë˜ì—ˆìŒì„ ì‹œì‚¬í•œë‹¤(ë°˜ ì—ì´ì¼„ ì™¸, 2018).**\n\n\u003e Few publications provide details of their annotation process or annotation guidelines. Providing such information is the norm in social scientific research and is viewed as an integral part of verifying othersâ€™ findings and robustness (Bucy \u0026 Holbert, 2013). In line with the recommendations of Sabou et al., we advocate that annotation guidelines and processes are shared where possible (Sabou, Bontcheva, Derczynski, \u0026 Scharl, 2014) and that the field also works to develop best practices.\n- ì£¼ì„ í”„ë¡œì„¸ìŠ¤ ë˜ëŠ” ì£¼ì„ ì§€ì¹¨ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì„ ì œê³µí•˜ëŠ” ì—°êµ¬ë“¤ì€ ê±°ì˜ ì—†ë‹¤. **ì´ëŸ¬í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì€ ì‚¬íšŒê³¼í•™ ì—°êµ¬ì˜ í‘œì¤€ì´ë©°**, íƒ€ì¸ì˜ ë°œê²¬ê³¼ ê²¬ê³ ì„±ì„ ê²€ì¦í•˜ëŠ” ë° í•„ìˆ˜ì ì¸ ë¶€ë¶„ìœ¼ë¡œ ê°„ì£¼ëœë‹¤(Bucy \u0026 Holbert, 2013). **Sabou ë“±ì˜ ê¶Œê³ ì— ë”°ë¼, ìš°ë¦¬ëŠ” ì£¼ì„ ì§€ì¹¨ê³¼ í”„ë¡œì„¸ìŠ¤ê°€ ê°€ëŠ¥í•œ ê³³ì—ì„œ ê³µìœ ë˜ê³ (Sabou, Bontcheva, Derczynski, \u0026 Scharl, 2014) ì´ ë¶„ì•¼ë„ ëª¨ë²” ì‚¬ë¡€ë¥¼ ê°œë°œí•˜ê¸° ìœ„í•´ ë…¸ë ¥í•´ì•¼ í•œë‹¤ê³  ì£¼ì¥í•œë‹¤.**\n\n\u003e Ensuring that abusive content detection systems can be applied across different domains is one of the most difficult but also important frontiers in existing research. Thus far, efforts to address this has been unsuccessful. Burnap and Williams train systems on one type of hate speech (e.g. racism) and apply them to another (e.g. sexism) and find that performance drops considerably (Burnap \u0026 Williams, 2016)\n- ìš•ì„¤ ì½˜í…ì¸  íƒì§€ ì‹œìŠ¤í…œì´ ì„œë¡œ ë‹¤ë¥¸ ì˜ì—­ì— ê±¸ì³ ì ìš©ë  ìˆ˜ ìˆë„ë¡ ë³´ì¥í•˜ëŠ” ê²ƒì€ ê¸°ì¡´ ì—°êµ¬ì—ì„œ ê°€ì¥ ì–´ë µì§€ë§Œ ì¤‘ìš”í•œ ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì´ë‹¤. ì§€ê¸ˆê¹Œì§€, ì´ê²ƒì„ í•´ê²°í•˜ë ¤ëŠ” ë…¸ë ¥ì€ ì„±ê³µí•˜ì§€ ëª»í–ˆë‹¤. **Burnapê³¼ WilliamsëŠ” í•œ ìœ í˜•ì˜ í˜ì˜¤ ë°œì–¸(ì˜ˆ: ì¸ì¢…ì°¨ë³„)ì— ëŒ€í•´ ì‹œìŠ¤í…œì„ í›ˆë ¨ì‹œí‚¤ê³  ë‹¤ë¥¸ ìœ í˜•ì˜ í˜ì˜¤ ë°œì–¸(ì˜ˆ: ì„±ì°¨ë³„)ì— ì ìš©í•˜ë©° ì„±ëŠ¥ì´ ìƒë‹¹íˆ ë–¨ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•œë‹¤(Burnap \u0026 Williams, 2016).**\n\n\n\u003e Karan and Å najder use a simple methodology to show the huge differences in performance when applying classifiers on different datasets without domain-specific tuning (Karan \u0026 Å najder, 2018). Noticeably, in the EVALITA hate speech detection shared task, participants were asked to (1) train and test a system on Twitter data, (2) on Facebook data and (3) to train on Twitter and test on Facebook (and vice versa). **Even the best performing teams reported their systems scored around 10 to 15 F1 points fewer on the cross-domain task.**\n- Karanê³¼ ShnajderëŠ” ë„ë©”ì¸ë³„ íŠœë‹ ì—†ì´ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì„¸íŠ¸ì— ë¶„ë¥˜ê¸°ë¥¼ ì ìš©í•  ë•Œ ì„±ëŠ¥ì—ì„œ í° ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ê°„ë‹¨í•œ ë°©ë²•ë¡ ì„ ì‚¬ìš©í•œë‹¤(Karan \u0026 Shnajder, 2018). ëˆˆì— ë„ê²Œ, EVALITA í˜ì˜¤ ë°œì–¸ íƒì§€ ê³µìœ  ì‘ì—…ì—ì„œ, **ì°¸ê°€ìë“¤ì—ê²Œ (1) íŠ¸ìœ„í„° ë°ì´í„°ì— ëŒ€í•œ ì‹œìŠ¤í…œ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸, (2) í˜ì´ìŠ¤ë¶ ë°ì´í„°ì— ëŒ€í•œ ì‹œìŠ¤í…œ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸, (3) íŠ¸ìœ„í„°ì— ëŒ€í•œ í›ˆë ¨ ë° í˜ì´ìŠ¤ë¶ì—ì„œ í…ŒìŠ¤íŠ¸(ë° ê·¸ ë°˜ëŒ€)ë¥¼ ìš”ì²­í•˜ì˜€ë‹¤.** ìµœê³ ì˜ ì„±ê³¼ë¥¼ ê±°ë‘” íŒ€ë“¤ì¡°ì°¨ ê·¸ë“¤ì˜ ì‹œìŠ¤í…œì´ êµì°¨ ë„ë©”ì¸ ì‘ì—…ì—ì„œ ì•½ 10-15ì˜ F1 ì ìˆ˜ë¥¼ ë” ì ê²Œ ë°›ì•˜ë‹¤ê³  ë³´ê³ í–ˆë‹¤.\n\n***\n\n\u003e [!info] Reference  \n\u003e \n\u003e Bertie Vidgen, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott Hale, and Helen Margetts. 2019.Â [Challenges and frontiers in abusive content detection](https://aclanthology.org/W19-3509). InÂ _Proceedings of the Third Workshop on Abusive Language Online_, pages 80â€“93, Florence, Italy. Association for Computational Linguistics.\n\n","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/Dynamic-Programming":{"title":"âš™ï¸ Dynamic Programming","content":"\n\u003e ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ”Â _\"í•œ ë²ˆ ê³„ì‚°í•œ ë¬¸ì œëŠ” ë‹¤ì‹œ ê³„ì‚°í•˜ì§€ ì•Šë„ë¡ í•œë‹¤!\"_Â ëŠ”Â **ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°(Dynamic Programming, ë™ì  ê³„íšë²•ì´ë¼ê³ ë„ í•¨)**ì— ëŒ€í•´ì„œ ì†Œê°œí•´ë³´ê³  ì´ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì.\n\n\në‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°ì€Â **ë©”ëª¨ë¦¬ ê³µê°„ì„ ì•½ê°„ ë” ì‚¬ìš©**í•´ì„œ ì—°ì‚° ì†ë„ë¥¼ ë¹„ì•½ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤. ìš°ì„  ë‹¤ìŒê³¼ ê°™ì€ 2ê°€ì§€ ì¡°ê±´ì„ ë§Œì¡±í•  ë•Œ ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n\n1.  **í° ë¬¸ì œë¥¼ ì‘ì€ ë¬¸ì œë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.**\n2.  **ì‘ì€ ë¬¸ì œì—ì„œ êµ¬í•œ ì •ë‹µì€ ê·¸ê²ƒì„ í¬í•¨í•˜ëŠ” í° ë¬¸ì œì—ì„œë„ ë™ì¼**í•˜ë‹¤.\n\nìœ„ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëŒ€í‘œì ì¸ ë¬¸ì œê°€ **í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´** ë¬¸ì œì´ë‹¤. í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì€ ë‹¤ìŒê³¼ ê°™ì€ ì í™”ì‹ì„ ë§Œì¡±í•˜ëŠ” ìˆ˜ì—´ì´ë‹¤.\n\n$$\na_n=a_{nâˆ’1} + a_{nâˆ’2}, a_1=1 ,Â a_2=1\n$$\n\në‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°ì˜ í¬ì¸íŠ¸ëŠ” ë°”ë¡œ í•œ ë²ˆ ê²°ê³¼ë¥¼ ìˆ˜í–‰í•œ ê²ƒì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•´ ë†“ê³  ë‹¤ìŒì— ë˜‘ê°™ì€ ê²°ê³¼ê°€ í•„ìš”í•˜ë©´ ê·¸ ë•Œ ë‹¤ì‹œ ì—°ì‚°í•˜ì§€ ì•Šê³  ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ê·¸ ê°’ì„ ê°€ì ¸ì™€ ì“°ëŠ” ê²ƒì´ë‹¤.\n\nì´ëŸ¬í•œ ê²ƒì„Â **ë©”ëª¨ì œì´ì…˜(ìºì‹±) ê¸°ë²•**ì´ë¼ê³ ë„ í•œë‹¤. ë‹¤ìŒì€ **ì¬ê·€í•¨ìˆ˜**ë¥¼ ì‚¬ìš©í•œ ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ êµ¬í˜„í•œ ì½”ë“œì´ë‹¤.\n\n```python\nimport time\n\ndp = [0] * 50\n\ndef fibo(x):\n    if x == 1 or x == 2:\n        return 1\n    if dp[x] != 0:\n        return dp[x]\n    dp[x] = fibo(x-1) + fibo(x-2)\n    return dp[x]\n\nfor num in range(5, 40, 10):\n    start = time.time()\n    res = fibo(num)\n    print(res, '-\u003e ëŸ¬ë‹íƒ€ì„:', round(time.time() - start, 2), 'ì´ˆ')\n```\n\nì´ë ‡ê²Œ ì¬ê·€í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ êµ¬í˜„í•˜ëŠ” ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë° ë°©ë²•ì€ ë©”ëª¨ì œì´ì…˜ ê¸°ë²•ì„ í™œìš©í•œ `Top-Down` ë°©ì‹ì´ë¼ê³  í•œë‹¤.\n\nì¦‰, í° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‘ì€ ë¬¸ì œë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒì´ë‹¤.Â \n\në°˜ë©´ì— ì¬ê·€í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ Â **ë‹¨ìˆœ ë°˜ë³µë¬¸**ì„ ì‚¬ìš©í•´ ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. í•˜ë‹¨ì˜ ì½”ë“œë¥¼ ì‚´í´ë³´ì.\n\n```python\ndp = [0] * 100\n\ndp[1] = 1 # ì²« ë²ˆì§¸ í•­\ndp[2] = 1 # ë‘ ë²ˆì§¸ í•­\nN = 99   # í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì˜ 99ë²ˆì§¸ ìˆ«ìëŠ”?\n\nfor i in range(3, N+1):\n    dp[i] = dp[i-1] + dp[i-2]\n\nprint(dp[N])\n```\n\nìœ„ì™€ ê°™ì€ ë°©ì‹ì€ ì‘ì€ ë¬¸ì œë¶€í„° ì°¨ê·¼ì°¨ê·¼ ë‹µì„ ë„ì¶œí•´ì„œ í° ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤ê³  í•˜ì—¬ `Bottom-Up` ë°©ì‹ì´ë¼ê³  í•œë‹¤. ì°¸ê³ ë¡œ Top-Down ë°©ì‹ì—ì„œëŠ” ì´ë¯¸ ìˆ˜í–‰í•œ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ê²ƒì„ `ë©”ëª¨ì œì´ì…˜`, Bottom-Up ë°©ì‹ì—ì„œëŠ” `DP í…Œì´ë¸”`ì´ë¼ê³  í•œë‹¤.\n\nì¼ë°˜ì ìœ¼ë¡  ë‹¨ìˆœ ë°˜ë³µë¬¸ì„ í™œìš©í•˜ëŠ” **Bottom-Up ë°©ì‹ìœ¼ë¡œ ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë° ë°©ë²•ì„ í•´ê²°**í•˜ë¼ê³  ê¶Œì¥í•œë‹¤. ë§Œì•½ ì¬ê·€í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” Top-Down ë°©ì‹ì„ ì‚¬ìš©í•˜ë‹¤ ë³´ë©´ ì¬ê·€ íšŸìˆ˜ ì œí•œ ì˜¤ë¥˜ê°€ ê±¸ë¦´ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì´ë‹¤.","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/GLUCOSE":{"title":"GLUCOSE:Â GeneraLized andÂ COntextualized Story Explanations","content":"\u003e[!info] Reference\n\u003eNasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon, David Buchanan, Lauren Berkowitz, Or Biran, and Jennifer Chu-Carroll. 2020.Â [GLUCOSE: GeneraLized and COntextualized Story Explanations](https://aclanthology.org/2020.emnlp-main.370). InÂ _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4569â€“4586, Online. Association for Computational Linguistics.\n\nì´ ë…¼ë¬¸ì€ Elemental Cognitionì´ë¼ëŠ” AIê¸°ì—… ì—°êµ¬ìë“¤ì´ ì—¬ëŸ¿ ì°¸ì—¬í•˜ì˜€ìœ¼ë©° 2020 EMNLP ì»¨í¼ëŸ°ìŠ¤ì—ì„œ Honourable Mention Papersì— ì˜¤ë¥¸ ë…¼ë¬¸ì´ë‹¤. ë¸”ë¡œê·¸ì— ì²˜ìŒ í¬ìŠ¤íŒ…í•˜ëŠ” ë…¼ë¬¸ìœ¼ë¡œ ì´ ë…¼ë¬¸ì„ ì •í•œ ê¹Œë‹­ì€ **ìƒì‹ ì¶”ë¡  ë°ì´í„°ì…‹** ì„ ìƒë‹¹íˆ í¥ë¯¸ë¡œìš´ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì§‘í•œ ì—°êµ¬ì´ê¸° ë•Œë¬¸ì´ë‹¤. ì¸ê°„ì˜ ì¸ì§€ ì‹¬ë¦¬í•™ì— ì˜í–¥ì„ ë°›ì•„ ì‚¬ê±´ì˜ ì¸ê³¼ ê´€ê³„ë¥¼ 10ì°¨ì›ìœ¼ë¡œ ì •ì˜í•˜ê³ , ì´ì— ë§ì¶° ìƒì‹ ì¶”ë¡  ë°ì´í„°ì…‹ì„ ë§Œë“¤ì—ˆë‹¤ë‹ˆ...ì—­ì‹œ EMNLPì—ëŠ” ì•„ë¬´ë‚˜ íˆ¬ê³ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤.  \n\n## Motivtion\n- ì‚¬ëŒì€ ë¬´ì–¸ê°€ë¥¼ ì½ê±°ë‚˜ ë“¤ì„ ë•Œ, **ì•”ì‹œì ì¸ ìƒì‹ ì¶”ë¡ (implicit commonsense inferences)**ì„ ë§Œë“¤ì–´ ë¬´ì—‡ì´ ì¼ì–´ë‚¬ê³  ì™œ ì¼ì–´ë‚¬ëŠ”ì§€ë¥¼ ì´í•´í•œë‹¤.\n- ê·¸ëŸ¬ë‚˜ AI systemì€ reading comprehensionì´ë‚˜ dialogueê³¼ ê°™ì€ íƒœìŠ¤í¬ì— ìˆì–´ì„œ ì—¬ì „íˆ ì¸ê°„ê³¼ ê°™ì€ ìƒì‹ ì¶”ë¡  ëŠ¥ë ¥ì„ ë³´ì´ì§€ ëª»í•˜ê³  ìˆë‹¤.\n- ê·¸ ì´ìœ ëŠ”...\n\t- â€˜ìƒì‹(commonsense knowledge)â€™ì„ ëŒ€ê·œëª¨ë¡œ íœ™ë“í•  ë°©ë²•ì´ ì—†ê¸° ë•Œë¬¸ì—\n\t- ê·¸ëŸ¬í•œ ì§€ì‹ë“¤ì„ ìµœì‹ ì˜ AI ì‹œìŠ¤í…œì— í†µí•©ì‹œí‚¬ ë°©ë²•ì´ ì—†ê¸° ë•Œë¬¸ì—\n\n## Claims\n- GlUCOSE ìƒì‹ ì¶”ë¡  í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ì´ëŸ¬í•œ ë³´í‹€ë„¥ì„ í•´ê²°í•  ìˆ˜ ìˆë‹¤.\n- GLUCOSE ë°ì´í„°ì…‹ì€ ì‚¬ê±´ì˜ ì¸ê³¼ ì„¤ëª…ì„ 10ê°€ì§€ ì°¨ì›ì„ ì œê³µí•˜ë©°, ë˜í•œ êµ¬ì²´ì ì¸ ìŠ¤í† ë¦¬ì™€ ì¼ë°˜í™”ëœ ë²•ì¹™ì„ ì œê³µí•œë‹¤.\n\n## Significance\n- ì•”ì‹œì ì¸ ìƒì‹(Commonsense knowledge)ë¥¼ ëŒ€ê·œëª¨ë¡œ ìˆ˜ì§‘í•  ë°©ë²•ì„ ìˆ˜ë¦½í–ˆë‹¤.\n- ê¸°ì¡´ì˜ ì‚¬ì „í•™ìŠµ ì–¸ì–´ëª¨ë¸ì„ GLUCOSEì— í›ˆë ¨ì‹œí‚¤ë©´ ì²˜ìŒ ë³´ëŠ” ì´ì•¼ê¸°ì—ë„ ì¸ê°„ê³¼ ë¹„ìŠ·í•œ ì •ë„ì˜ ìƒì‹ ì¶”ë¡ ì´ ê°€ëŠ¥í•´ì§„ë‹¤.\n\n  \n## Introduction\n\në‹¤ìŒê°™ì€ ìƒí™©ì„ ê°€ì •í•´ë³´ì.\n\n- í•œ ì•„ì´ ì•ì—ì„œ ì°¨ê°€ ë°©í–¥ì„ í‹€ì—ˆë‹¤.\n- ê·¸ ì•„ì´ê°€ ì¬ë¹¨ë¦¬ ìì „ê±°ë¥¼ ëŒë ¸ë‹¤.\n- ê·¸ ì•„ì´ê°€ ìì „ê±°ì—ì„œ ë–¨ì–´ì¡Œë‹¤.\n- ê·¸ ì•„ì´ì˜ ë¬´ë¦ì´ ê¹Œì¡Œë‹¤.\n\nì´ ì´ì•¼ê¸°ë¥¼ ì½ìœ¼ë©° ì‚¬ëŒì€ ë¬¸ì¥ê°„ì˜ **'ì¸ê³¼ ì¶”ë¡ '** ì„ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´,\n\n- 'í•œ ì•„ì´ ì•ì—ì„œ ì°¨ê°€ ë°©í–¥ì„ í‹€ì—ˆë‹¤.'\n- **ê·¸ë¦¬ê³  ê·¸ê±´** 'ê·¸ ì•„ì´ê°€ ì¬ë¹¨ë¦¬ ìì „ê±°ë¥¼ ëŒë ¸ë‹¤.'ë¥¼ ì´ˆë˜í•˜ê³ \n- **ê·¸ë¦¬ê³  ê·¸ê±´** 'ê·¸ ì•„ì´ê°€ ìì „ê±°ì—ì„œ ë–¨ì–´ì¡Œë‹¤.' ë¥¼ ì´ˆë˜í•˜ê³ \n- **ê·¸ë¦¬ê³  ê·¸ê±´** 'ê·¸ ì•„ì´ì˜ ë¬´ë¦ì´ ê¹Œì¡Œë‹¤.'ë¥¼ ì´ˆë˜í–ˆë‹¤.\n\nì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ë§ì´ë‹¤. **ì‚¬ëŒì€ ì´ì²˜ëŸ¼ ì–´ë–»ê²Œ ì´ì•¼ê¸° ì† íŠ¹ì •í•œ ì‚¬ê±´ì´ íŠ¹ì •í•œ ê²°ê³¼ë¥¼ ì´ëŒì—ˆëŠ”ì§€ ë¬˜ì‚¬í•˜ëŠ” 'ì¸ê³¼ì  ì‚¬ìŠ¬'ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤**\n\nê·¸ëŸ¬ë‚˜ AI systemì€ reading comprehensionì´ë‚˜ dialogueê³¼ ê°™ì€ íƒœìŠ¤í¬ì— ìˆì–´ì„œ ì—¬ì „íˆ ì¸ê°„ê³¼ ê°™ì€ ìƒì‹ ì¶”ë¡  ëŠ¥ë ¥ì„ ë³´ì´ì§€ ëª»í•˜ê³  ìˆë‹¤. ì´ëŠ” ë‘ ê°€ì§€ ì´ìœ ê°€ ìˆëŠ”ë°, ì²«ì§¸ë¡œ **ì•”ì‹œì ì¸ ìƒì‹ì„ ëŒ€ê·œëª¨ë¡œ íšë“í•  ê¸¸ì´ ì—†ìœ¼ë©°**, ë‘˜ì§¸ë¡œ ê·¸ëŸ¬í•œ ì§€ì‹ì„ **ìµœì‹ ì˜ AI ì‹œìŠ¤í…œì— ìœµí•©ì‹œí‚¬ ê¸¸ì´ ì—†ê¸°** ë•Œë¬¸ì´ë‹¤.\n\nì´ëŸ¬í•œ ìƒí™©ì„ í•´ê²°í•˜ê³  ë‚˜ì˜¨ ê²ƒì´ ë°”ë¡œ GLUCOSEì´ë‹¤. GLUCOSEë¼ëŠ” ì´ë¦„ë„ ë°”ë¡œ ì´ í”„ë ˆì„ì›Œí¬ê°€ AIë¥¼ ìœ„í•´ í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ë¹„ìœ ì ìœ¼ë¡œ ë³´ì—¬ì£¼ê³  ìˆëŠ”ë°, ì‚¬ëŒì˜ ì§€ì‹ í™œë™ì´ ë‡Œ ì†ì˜ ê¸€ë£¨ì½”ìŠ¤ ìš©ëŸ‰ì— ë”°ë¼ ì¢Œìš°ë˜ëŠ” ê²ƒì²˜ëŸ¼, AI ì‹œìŠ¤í…œì´ ê¸°ë³¸ì  ì‚¬ê³ ë¥¼ í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì—°ë£Œê°€ ë˜ë¼ëŠ” ì˜ë¯¸ì—ì„œ ê¸€ë£¨ì½”ìŠ¤ë¼ê³  ì§€ì—ˆë‹¤ê³  í•œë‹¤. ì¢‹ì€ ë…¼ë¬¸ì€ ì—­ì‹œ ì´ë¦„ë„ ì˜ ì§“ëŠ”ë‹¤.\n\nì•ì—ì„œë„ ì–¸ê¸‰í–ˆì§€ë§Œ, GLUCOSE ë°ì´í„°ëŠ” ì•„ì£¼ í¥ë¯¸ë¡œìš´ ê·œì¹™ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆë‹¤. ë…¼ë¬¸ì˜ í‘œí˜„ì„ ê°€ì ¸ì˜¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n  \n\u003e Së¼ëŠ” ì§§ì€ ì´ì•¼ê¸°ì˜ Xë¼ëŠ” ì„ íƒëœ ë¬¸ì¥ì´ ì£¼ì–´ì§€ë©´, GLUCOSEëŠ” Xì™€ ê´€ë ¨ëœ, ì¸ê°„ì˜ ì¸ì§€ ì‹¬ë¦¬í•™ì— ì˜í–¥ì„ ë°›ì€ 10ê°€ì§€ ì°¨ì›ì˜ commonsense causal explanationsì„ ì •ì˜í•œë‹¤.\n\në¿ë§Œ ì•„ë‹ˆë¼, GLUCOSEëŠ” commonsense knowledgeë¥¼ ì„¸ìƒì— ê´€í•œ â€˜ë¯¸ë‹ˆ ì´ë¡ â€™ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” **â€˜ë°˜ì •í˜• ì¶”ë¡  ë²•ì¹™(semi-structured inference rules)'** ì˜ í˜•íƒœë¡œ ì¸ì½”ë”©í•˜ê³ , ê°ê°ì€ êµ¬ì²´ì ì¸ ì´ì•¼ê¸°ì— ê·¼ê±°í•œë‹¤ëŠ” íŠ¹ì§•ë„ ìˆë‹¤.","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/H-index":{"title":"H-index","content":"H-IndexëŠ” ê³¼í•™ìì˜ ìƒì‚°ì„±ê³¼ ì˜í–¥ë ¥ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì–´ëŠ ê³¼í•™ìì˜ H-Indexë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ì¸ hë¥¼ êµ¬í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ìœ„í‚¤ë°±ê³¼[1](https://school.programmers.co.kr/learn/courses/30/lessons/42747/solution_groups?language=python3\u0026type=my#fn1)ì— ë”°ë¥´ë©´, H-IndexëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•©ë‹ˆë‹¤.\n\nì–´ë–¤ ê³¼í•™ìê°€ ë°œí‘œí•œ ë…¼ë¬¸Â `n`í¸ ì¤‘,Â `h`ë²ˆ ì´ìƒ ì¸ìš©ëœ ë…¼ë¬¸ì´Â `h`í¸ ì´ìƒì´ê³  ë‚˜ë¨¸ì§€ ë…¼ë¬¸ì´ hë²ˆ ì´í•˜ ì¸ìš©ë˜ì—ˆë‹¤ë©´Â `h`ì˜ ìµœëŒ“ê°’ì´ ì´ ê³¼í•™ìì˜ H-Indexì…ë‹ˆë‹¤.\n\nì–´ë–¤ ê³¼í•™ìê°€ ë°œí‘œí•œ ë…¼ë¬¸ì˜ ì¸ìš© íšŸìˆ˜ë¥¼ ë‹´ì€ ë°°ì—´ `citationsê°€` ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì–´ì§ˆ ë•Œ, ì´ ê³¼í•™ìì˜ H-Indexë¥¼ `return` í•˜ë„ë¡ `solution` í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n### ì œí•œì‚¬í•­\n\n-   ê³¼í•™ìê°€ ë°œí‘œí•œ ë…¼ë¬¸ì˜ ìˆ˜ëŠ” 1í¸ ì´ìƒ 1,000í¸ ì´í•˜ì…ë‹ˆë‹¤.\n-   ë…¼ë¬¸ë³„ ì¸ìš© íšŸìˆ˜ëŠ” 0íšŒ ì´ìƒ 10,000íšŒ ì´í•˜ì…ë‹ˆë‹¤.\n\n### ì…ì¶œë ¥ ì˜ˆ\n\n| citations | return |\n| --------- | ------ |\n| [3, 0, 6, 1, 5]          |     3   |\n\n### ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…\n\nì´ ê³¼í•™ìê°€ ë°œí‘œí•œ ë…¼ë¬¸ì˜ ìˆ˜ëŠ” 5í¸ì´ê³ , ê·¸ì¤‘ 3í¸ì˜ ë…¼ë¬¸ì€ 3íšŒ ì´ìƒ ì¸ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‚˜ë¨¸ì§€ 2í¸ì˜ ë…¼ë¬¸ì€ 3íšŒ ì´í•˜ ì¸ìš©ë˜ì—ˆê¸° ë•Œë¬¸ì— ì´ ê³¼í•™ìì˜ H-IndexëŠ” 3ì…ë‹ˆë‹¤.\n\n## ë‚˜ì˜ í’€ì´\n\n```python\ndef solution(citations):\n\n    citations.sort()\n\n    h = [len(citations[x:])-1 for x in range(len(citations)) if len(citations[x:]) \u003e= citations[x]]\n    if len(h) != 0:\n       return h[-1]\n    else:\n        return len(citations)\n```\n\n\u003e[!warning]   \n\u003e\n\u003eì´ ë¬¸ì œì—ì„œ hëŠ” ê¼­ citations ì•ˆì— ìˆì§€ ì•Šë‹¤! \n\u003eì˜ˆë¥¼ ë“¤ì–´, citations = [6, 5, 5, 5, 3, 2, 1, 0] ì´ë©´, h = 4ì´ë‹¤.\n\në¬¸ì œ ì •ë…ê³¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ì¤‘ìš”í•¨ì„ ëŠë¼ì‹­ì‹œì˜¤...\nhê°€ ë°˜ë“œì‹œ ì£¼ì–´ì§€ëŠ” ì¤„ ì•Œê³  í•œì°¸ì„ í—¤ë§¸ë‹¤. ì´ëŸ¬ë©´ ì‹¤ì „ì—ì„œëŠ” ì™„ì „ ë‚˜ê°€ë¦¬ë‹¤. \nì†”ì§íˆ ë¬¸ì œê°€ í—·ê°ˆë¦¬ë„ë¡ ë‚˜ì™”ë‹¤ê³  ìƒê°í•˜ëŠ”ë°, \nê·¸ê²ƒê³¼ëŠ” ë³„ê°œë¡œ ë¬¸ì œë¥¼ ì˜ ì½ì. \nê·¸ë¦¬ê³  ë°˜ë¡€ë¥¼ í•­ìƒ ì˜ ë– ì˜¬ë¦¬ì. \n\n## ë‹¤ë¥¸ ì‚¬ëŒì˜ ì½”ë“œ\n\n```python\ndef solution(citations):\n    citations = sorted(citations)\n    l = len(citations)\n    for i in range(l):\n        if citations[i] \u003e= l-i:\n            return l-i\n    return 0\n```\n\nì´ê²Œ ë” íš¨ìœ¨ì ì¸ ì½”ë“œë¡œ ë³´ì¸ë‹¤. ê´œíˆ `h list`ë¥¼ ë§Œë“¤ í•„ìš”ê°€ ì—†ë‹¤. ","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/Hate-speech-detection-and-racial-bias-mitigation-in-social-media-based-on-BERT-model":{"title":"Hate speech detection and racial bias mitigation in social media based on BERT model","content":"\n\u003e To define automated methods with a promising performance for hate speech detection in social media, Natural Language Processing (NLP) has been used jointly with classic Machine Learning (ML) [2â€“4] and Deep Learning (DL) techniques [6, 15, 16]. The majority of contributions in classic supervised machine learning-based methods, for hate speech detection, rely on different text mining-based features or user-based and platform-based metadata [4, 17, 18], which require them to define an applicable feature extraction method and prevent them to generalize their approach to new datasets and platforms. **However, recent advancements in deep neural networks and transfer learning approaches allow the research community to address these limitations.**\n- ì†Œì…œ ë¯¸ë””ì–´ì—ì„œ í˜ì˜¤ ë°œì–¸ íƒì§€ë¥¼ ìœ„í•œ ìœ ë§í•œ ì„±ëŠ¥ì„ ê°€ì§„ ìë™í™”ëœ ë°©ë²•ì„ ì •ì˜í•˜ê¸° ìœ„í•´, ìì—°ì–´ ì²˜ë¦¬(NLP)ëŠ” ê³ ì „ì ì¸ ë¨¸ì‹  ëŸ¬ë‹(ML)[2â€“4] ë° ë”¥ ëŸ¬ë‹(DL) ê¸°ìˆ [6, 15, 16]ê³¼ í•¨ê»˜ ì‚¬ìš©ë˜ì–´ ì™”ë‹¤. í˜ì˜¤ ë°œì–¸ ê°ì§€ë¥¼ ìœ„í•œ ê³ ì „ì ì¸ ì§€ë„ ê¸°ê³„ í•™ìŠµ ê¸°ë°˜ ë°©ë²•ì˜ ê¸°ì—¬ì˜ ëŒ€ë¶€ë¶„ì€ ì„œë¡œ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ê¸°ë°˜ features ë˜ëŠ” ì‚¬ìš©ì ê¸°ë°˜ ë° í”Œë«í¼ ê¸°ë°˜ ë©”íƒ€ë°ì´í„°[4, 17, 18]ì— ì˜ì¡´í•˜ë©°, ì´ëŠ” í•´ë‹¹ ê¸°ëŠ¥ ì¶”ì¶œ ë°©ë²•ì„ ì •ì˜í•˜ê³  ìƒˆë¡œìš´ ë°ì´í„° ì„¸íŠ¸ ë° ê³„íšì— ëŒ€í•œ ì ‘ê·¼ ë°©ì‹ì„ ì¼ë°˜í™”í•˜ëŠ” ê²ƒì„ ì–´ë µê²Œ í•œë‹¤. **ê·¸ëŸ¬ë‚˜ ìµœê·¼ ì‹¬ì¸µ ì‹ ê²½ë§ê³¼ ì „ì´ í•™ìŠµ ì ‘ê·¼ë²•ì˜ ë°œì „ì€ ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ê°€ ì´ëŸ¬í•œ í•œê³„ë¥¼ í•´ê²°í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.**\n- ìì—°ì–´ ì²˜ë¦¬ â† ë¨¸ì‹  ëŸ¬ë‹ \u0026 ë”¥ëŸ¬ë‹ â† ë¨¸ì‹  ëŸ¬ë‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•œ ë”¥ëŸ¬ë‹\n- ì´ëŸ¬í•œ ìˆœì„œê°€ ìì—°ìŠ¤ëŸ½ë„¤. ","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/Hate-speech-detection-is-not-as-easy-as-you-may-think":{"title":"Hate speech detection is not as easy as you may think","content":"\n\u003e[!info] Reference  \n\u003e\n\u003eAymÃ© Arango, Jorge PÃ©rez, and Barbara Poblete. 2019. Hate Speech Detection is Not as Easy as You May Think: A Closer Look at Model Validation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19). Association for Computing Machinery, New York, NY, USA, 45â€“54. https://doi.org/10.1145/3331184.3331262\n\n---\n\n## Introduction\n\n\u003e Despite the apparent difficulty of the hate speech detection problem evidenced by social-media providers, current state-of- the-art approaches reported in the literature show near-perfect performance. Within-dataset experiments on labeled hate speech datasets using supervised learning achieve F1 scores above 93% [3, 7â€“9]. Nevertheless, there are only a few studies towards determining how generalizable the resulting models are, beyond the data collection upon which they were built on, nor on the factors that may affect this property [10]. Furthermore, recent literature that surveys current work also views the state-of-the-art under a more conservative and cautious light [3,10].\n- ì†Œì…œ ë¯¸ë””ì–´ ì œê³µìê°€ ì¦ëª…í•˜ëŠ” í˜ì˜¤ ë°œì–¸ íƒì§€ ë¬¸ì œì˜ ëª…ë°±í•œ ì–´ë ¤ì›€ì—ë„ ë¶ˆêµ¬í•˜ê³ , ë¬¸í—Œì— ë³´ê³ ëœ í˜„ì¬ ìµœì²¨ë‹¨ ì ‘ê·¼ ë°©ì‹ì€ ê±°ì˜ ì™„ë²½í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. ì§€ë„ í•™ìŠµì„ ì‚¬ìš©í•˜ì—¬ ë ˆì´ë¸”ë§ëœ í˜ì˜¤ ë°œì–¸ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ë°ì´í„° ì„¸íŠ¸ ë‚´ ì‹¤í—˜ì€ 93% ì´ìƒì˜ F1 ì ìˆ˜ë¥¼ ë‹¬ì„±í•œë‹¤. \n- **ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ê²°ê³¼ ëª¨ë¸ì´ êµ¬ì¶•ëœ ë°ì´í„° ìˆ˜ì§‘ì„ ë„˜ì–´, ê²°ê³¼ ëª¨ë¸ì˜ ì¼ë°˜í™” ì •ë„ë¥¼ ê²°ì •í•˜ëŠ” ë°ì—ëŠ” ëª‡ ê°€ì§€ ì—°êµ¬ë§Œì´ ìˆë‹¤.** ë˜í•œ, í˜„ì¬ ì‘ì—…ì„ ì¡°ì‚¬í•˜ëŠ” ìµœê·¼ ë¬¸í—Œì—ì„œë„ ìµœì²¨ë‹¨ ê¸°ìˆ ì„ ë³´ë‹¤ ë³´ìˆ˜ì ì´ê³  ì‹ ì¤‘í•œ ê´€ì ì—ì„œ ë°”ë¼ë³¸ë‹¤.\n\n\u003e [!note] Aim of this paper  \n\u003e \"(...) measure how these models would perform on similar yet different datasets.\"\n\n- ë¹„ìŠ·í•˜ì§€ë§Œ ë‹¤ë¥¸ ë°ì´í„°ì…‹. ê±°ê¸°ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë³´ëŠ” ê²ƒ. ì´ê²Œ í•µì‹¬ì´ë‹¤.\n\n## Related work\n\n\u003e **There is some recent work on testing the generalization of the state-of-the-art methods to other datasets and domains [8â€“10].** Most of this work has been focused on Deep Learning methods. \n\u003e Agrawal and Awekar [8] test the performance of models trained on tweets [11] classifying on Wikpedia data [33] and Formspring data [34]. The authors show that transfer learning from Twitter to the two other domains performs poorly achieving less than 10% F1. \n\u003e In a similar study, Dadvar and Eckert [9] perform transfer learning from Twitter to a dataset of Youtube comments [35] showing a performance of 15% F1. \n\u003e GroÌˆndahl et al. [10] present a comprehensive study reproducing several state-of-the-art models. \n\u003e Specially important for us is the experiment transferring Badjatiya et al.â€™s model [7] trained on the Waseem and Hovyâ€™s dataset [11] to two other similarly labeled tweet datasets [16,17]. Even in this case the performance drops significantly, obtaining 33% and 47% F1 in those sets. This is a 40+% drop from the 93% F1 reported by Badjatiya et al. [7]. \n\u003e From these results, **GroÌˆndahl et al. [10] draw as a conclusion that model architecture is less important than the type of data and labeling criteria being used.** \n\u003e In this paper our results are coherent with those of GroÌˆndahl et al. [10]. However, we take our research a step further by investigating why this issue occurs.\n- í›Œë¥­í•œ ê°œê´„ì´ë‹¤. ì´ë ‡ê²Œ ì¨ì•¼ í•˜ëŠ”ë°. ","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/Machine-Learning":{"title":"Machine Learning","content":"\n## ML ëŒ€í‘œ ì•Œê³ ë¦¬ì¦˜\n\n-   Logistic Regression\n-   Decision Tree\n-   NaiÌˆve Bayes\n-   Support Vector Machine\n-   K Nearest Neighbors\n-   Decision Tree ê¸°ë°˜ Ensemble ëª¨í˜•\n\t- Random Forest\n\t- Extra Trees Boosting\n- Decision Tree ê¸°ë°˜ Gradient Boosting ëª¨í˜•\n\t- [Extreme Gradient Boosting](notes/XGB%20Modeling.md)  \n\t- Light Gradient Boosting  \n\t- Categorical Gradient Boosting\n- Natural Gradient Boosting\n\n\u003e [!note]\n\u003e ì¼ë°˜ì ìœ¼ë¡œ **Decision Tree \u0026 Logistic Regression** ì´ ì„¤ëª…ë ¥ì´ ì¢‹ì•„ì„œ ìì£¼ ì“°ì¸ë‹¤. ê·¸ëŸ¬ë‚˜ ìµœê·¼ì—ëŠ” **Gradient Boosting** ê³„ì—´ë„ ìì£¼ ì“°ì´ê³  ìˆë‹¤.\n\n## ML system ì¢…ë¥˜ì˜ ëŒ€í‘œ êµ¬ë¶„ 3ê°€ì§€\n\n**1. ì‚¬ëŒì˜ ê°ë…í•˜ì— í”„ë¡œê·¸ë¨ì´ í›ˆë ¨í•˜ëŠ” ê²ƒì¸ì§€ ì—¬ë¶€** \n- [ì§€ë„í•™ìŠµ (Supervised Learning)](notes/ì§€ë„í•™ìŠµ%20(Supervised%20Learning).md)\n\t- ë¶„ë¥˜ (Classification)\n\t- íšŒê·€ (Regression)\n- [ë¹„ì§€ë„í•™ìŠµ (Un-supervised Learning)](notes/ë¹„ì§€ë„í•™ìŠµ%20(Un-supervised%20Learning).md)\n- ì¤€ì§€ë„í•™ìŠµ (Semi-supervised Learning)\n- ê°•í™”í•™ìŠµ (Reinforcement Learning)\n\n**2. ì‹¤ì‹œê°„ìœ¼ë¡œ ì ì§„ì ì¸ í•™ìŠµì„ í•˜ëŠ”ì§€ ì—¬ë¶€**\n- ì˜¨ë¼ì¸ í•™ìŠµ (Online Learning)  \n- ë°°ì¹˜ í•™ìŠµ (Batch Learning)\n\n**3. ê¸°ì¡´ì˜ ë°ì´í„°ì™€ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ê°„ë‹¨íˆ ë¹„êµë¶„ì„í•˜ëŠ” ê²ƒì¸ì§€ í˜¹ì€ í›ˆë ¨ ë°ì´í„° ì…‹ìœ¼ë¡œë¶€í„° íŒ¨í„´ì„ ë°œê²¬í•˜ì—¬ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“œëŠ”ì§€ ì—¬ë¶€** \n- ì‚¬ë¡€ ê¸°ë°˜ í•™ìŠµ (Instance-based Learning)\n- ëª¨ë¸ ê¸°ë°˜ í•™ìŠµ (Model-based Learning)\n\n## ML ë¬¸ì œ í•´ê²° ìˆœì„œ\n\n1. í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¶„í¬ê°€ ë™ì¼í•œì§€ ì—¬ë¶€ë¥¼ íŒŒì•…í•œë‹¤\n2. í™œìš©í•  ëª¨ë¸ ì•Œê³ ë¦¬ì¦˜ë“¤ì„ ì •í•œë‹¤\n3. Hyperparameter Tuning Caseë¥¼ ì •ì˜í•œë‹¤\n4. `Cross Validation`ê¸°ë°˜ í•™ìŠµì„ ì§„í–‰í•œë‹¤ ( ëª¨ë“  í•™ìŠµ ë°ì´í„°ë¥¼ í•™ìŠµê³¼ ê²€ì¦ì— ë‹¤ í™œìš© )\n5. ìœ ì˜ë¯¸í•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” ì•Œê³ ë¦¬ì¦˜ ë° Hyperparameter Caseë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ì „ì²´ë¥¼ í•™ìŠµí•œë‹¤\n6. í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í™•ì¸í•œë‹¤\n\n## [Model tuning](notes/Model%20tuning.md)","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/Model-tuning":{"title":"Model tuning","content":"\n## ê·¸ë¦¬ë“œ íƒìƒ‰\n-   ì•Œê³ ë¦¬ì¦˜ ë‚´ íš¨ê³¼ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì°¾ì„ ë•Œê¹Œì§€, íƒìƒ‰í•˜ê³ ì í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ê·¸ì— í•´ë‹¹í•˜ëŠ” ì‹œë„í•´ë³¼ ê°’ë“¤ì„ ì§€ì •í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì§„í–‰í•˜ëŠ” ê²ƒ\n-  ì‚¬ì´í‚·ëŸ°ì˜ `GridSearchCV` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬, ë¯¸ë¦¬ ì„¤ì •í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì— ëŒ€í•´ êµì°¨ê²€ì¦ì„ ì§„í–‰í•˜ê³ , ì´ë¥¼ í†µí•´ í‰ê°€ë¥¼ í•  ìˆ˜ ìˆë‹¤\n- `RandomForestRegressor`ì— `GridSearchCV` ì ìš©í•˜ê¸°\n\t-   3X4ê°œì˜ Hyperparameter Tuning\n\t-   2X3ê°œì˜ Hyperparameter Tuning\n\t-   5íšŒ Cross Validation ì§„í–‰\n\n```python\nfrom sklearn.model_selection import GridSearchCV\n\n  \n\nparam_grid = [\n\n# try 12 (3Ã—4) combinations of hyperparameters\n\n{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n\n# then try 6 (2Ã—3) combinations with bootstrap set as False\n\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n\n]\n\n  \n\nforest_reg = RandomForestRegressor(random_state=42)\n\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n\nscoring='neg_mean_squared_error',\n\nreturn_train_score=True)\n\ngrid_search.fit(housing_prepared, housing_labels)\n\n```\n\n`RandomForestRegressor`ì— `GridSearchCV `ì ìš© í›„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³„ í‰ê°€ ì ìˆ˜ í™•ì¸\n\n```python\ncvres = grid_search.cv_results_\n\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n\nprint(np.sqrt(-mean_score), params)\n\n\u003e\u003e\u003e\n65005.182970763315 {'max_features': 2, 'n_estimators': 3} 55582.91015494046 {'max_features': 2, 'n_estimators': 10} 52745.33887865031 {'max_features': 2, 'n_estimators': 30} 60451.18914812725 {'max_features': 4, 'n_estimators': 3} 53062.818497303946 {'max_features': 4, 'n_estimators': 10} 50663.79774079741 {'max_features': 4, 'n_estimators': 30} 57998.07162873506 {'max_features': 6, 'n_estimators': 3} 52042.04702364244 {'max_features': 6, 'n_estimators': 10} 50028.060190761295 {'max_features': 6, 'n_estimators': 30} 58308.44501796401 {'max_features': 8, 'n_estimators': 3} 52082.74313186547 {'max_features': 8, 'n_estimators': 10} 50165.81805010987 {'max_features': 8, 'n_estimators': 30} 62709.54311517104 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3} 54062.01766032325 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10} 60613.541905953585 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3} 53742.988651846914 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10} 59387.46561811065 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3} 52826.41762121993 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n```\n\n## ëœë¤ íƒìƒ‰\n- `GridSearchCV`ë¥¼ ì§„í–‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì˜ ìˆ˜ê°€ ë„ˆë¬´ ë§ì„ ë•Œ, `RandomizedSearchCV`ë¥¼ í™œìš©í•˜ë©´ ì¡°í•© ë‚´ì—ì„œ ì„ì˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ëŒ€ì…í•˜ì—¬ ì§€ì •í•œ íšŸìˆ˜ë§Œí¼ í‰ê°€í•˜ê²Œ ë¨  \n- ì£¼ìš” ì¥ì  2ê°€ì§€\n\t1.  ëœë¤ íƒìƒ‰ì„ 1,000íšŒ ë°˜ë³µí•˜ë©´, ê° í•˜ì´í¼íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ê°ê¸° ë‹¤ë¥¸ 1,000ê°œ ê²½ìš°ë¥¼ íƒìƒ‰í•  ìˆ˜ ìˆìŒ\n\t2. ë°˜ë³µ íšŸìˆ˜ë¥¼ ë‹¨ìˆœíˆ ì¡°ì ˆí•˜ëŠ” ê²ƒ ë§Œìœ¼ë¡œë„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— íˆ¬ì…í•  ì»´í“¨íŒ… ìì›ì„ ì œì–´í•  ìˆ˜ ìˆìŒ\n\n```python\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom scipy.stats import randint\n\n  \n\nparam_distribs = {\n\n'n_estimators': randint(low=1, high=200),\n\n'max_features': randint(low=1, high=8),\n\n}\n\n  \n\nforest_reg = RandomForestRegressor(random_state=42)\n\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n\nn_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n\nrnd_search.fit(housing_prepared, housing_labels)\n```\n\n```python\ncvres = rnd_search.cv_results_\n\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n\nprint(np.sqrt(-mean_score), params)\n\n\u003e\u003e\u003e\n49462.596134607906 {'max_features': 7, 'n_estimators': 180} 51676.97211565583 {'max_features': 5, 'n_estimators': 15} 50827.83871022729 {'max_features': 3, 'n_estimators': 72} 51117.698297994146 {'max_features': 5, 'n_estimators': 21} 49585.185219390754 {'max_features': 7, 'n_estimators': 122} 50836.040148806715 {'max_features': 3, 'n_estimators': 75} 50746.890270152086 {'max_features': 3, 'n_estimators': 88} 49788.190631507045 {'max_features': 5, 'n_estimators': 100} 50574.565725719985 {'max_features': 3, 'n_estimators': 150} 65153.787556165735 {'max_features': 5, 'n_estimators': 2}\n```\n\nìœ„ì™€ ê°™ì´ ëœë¤ íƒìƒ‰ì„ ì´ìš©í•˜ëŠ” ê²Œ ë” ë‚˜ì€ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤. \n\n## í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ë¡œ ì‹œìŠ¤í…œ í‰ê°€í•˜ê¸°\n\ní…ŒìŠ¤íŠ¸ ë°ì´í„°ì„¸íŠ¸ì—ì„œ ì„¤ëª…ë³€ìˆ˜ì™€ íƒ€ê²Ÿë³€ìˆ˜ë¥¼ ì–»ì€ í›„, ê¸°ì¡´ì˜ ë³€í™˜ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ transformì„ ì§„í–‰ **( í…ŒìŠ¤íŠ¸ì˜ ê²½ìš° fit_transformì™€ ê°™ì€ í•™ìŠµ ê³¼ì •ì´ í¬í•¨ëœ ë³€í™˜ì€ ì§„í–‰í•˜ì§€ ì•ŠìŒ )**\n\n```python\nfinal_model = grid_search.best_estimator_\n\n  \n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\n\ny_test = strat_test_set[\"median_house_value\"].copy()\n\n  \n\nX_test_prepared = full_pipeline.transform(X_test)\n\nfinal_predictions = final_model.predict(X_test_prepared)\n\n  \n\nfinal_mse = mean_squared_error(y_test, final_predictions)\n\nfinal_rmse = np.sqrt(final_mse)\n```\n\n```python\nfinal_rmse\n\u003e\u003e 47362.98158022501\n```\n\nì˜¤ì°¨ ê°’ì„ ë‹¨ì¼ ì¶”ì •ê°’ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì–¼ë§ˆë‚˜ ì •í™•í•œì§€ë¥¼ `Scipy.stats.t.interval()` ê¸°ë°˜ 95% ì‹ ë¢° êµ¬ê°„ ê³„ì‚°ì„ í†µí•´ ì ê²€í•´ë³¼ ìˆ˜ ìˆë‹¤.\n\n```python\nfrom scipy import stats\n\n  \n\nconfidence = 0.95\n\nsquared_errors = (final_predictions - y_test) ** 2\n\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n\nloc=squared_errors.mean(),\n\nscale=stats.sem(squared_errors)))\n```\n\n```python\n\u003e\u003e\u003e array([45397.61151846, 49249.98392646])\n```","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/On-Cross-Dataset-Generalization-in-Automatic-Detection-of-Online-Abuse":{"title":"On Cross-Dataset Generalization in Automatic Detection of Online Abuse","content":"\n\u003e[!info] Reference  \n\u003e\n\u003eNejadgholi, I., \u0026 Kiritchenko, S. (2020). On cross-dataset generalization in automatic detection of online abuse.Â _arXiv preprint arXiv:2010.07414_.\n\n---\n\n## Research Questions\n\n\u003e Test and training sets were created for each dataset by performing a stratified split of 20% vs 80%, with the larger part used for training the models. The training sets were further subdivided, keeping 1/8 shares of them as separate validation sets during development and fine-tuning of the hyper-parameters.\n\n- Fine-tuningì—ì„œì˜ ì¼ë°˜ì ì¸ ë°©ë²•ì„ ë§í•˜ê³  ìˆë‹¤. ì „ì²´ ë°ì´í„°ë¥¼ label ë¶„í¬ë¥¼ ìœ ì§€í•œ ì±„ë¡œ `train`, `test` ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ì´í›„ `train`ì—ì„œ `validation`ì„ ë‹¤ì‹œ ë‚˜ëˆˆë‹¤. íŠ¹íˆ `test` ë°ì´í„°ì…‹ì€ í›ˆë ¨ì— ì“°ì´ì§€ ì•ŠëŠ”ë°, ì´í›„ í•™ìŠµí•œ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•  ë•Œ ì‚¬ìš©í•œë‹¤. ê·¸ë˜ì„œ `test` ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¨ë‹¤ë©´, í•´ë‹¹ ëª¨ë¸ì´ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œë„ ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¬ ê²ƒì´ë¼ëŠ” ê°€ì„¤ì„ ì„¸ìš¸ ìˆ˜ ìˆë‹¤.  \n- ê·¸ëŸ¬ë‚˜ ë³¸ ë…¼ë¬¸ì€ ì´ ê°€ì„¤ì— ì˜ë¬¸ì„ ì œê¸°í•œë‹¤. \n\n\u003e (...) the aim here, in contrast, was to see **how well the best models (that may have learnt some dataset-specific biases) performed on other datasets.** **This was done to investigate how well state-of-the-art systems perform in a real-life scenario**, i.e., when exposed to data from other domains, with the hypothesis that a model trained on one dataset that exhibits comparatively reasonable results on other datasets can be expected to generalise well.\n\n- ì´ ë…¼ë¬¸ì˜ ëª©ì ì€ ê·¸ë ‡ê²Œ í•œ ë°ì´í„°ì…‹ì„ ì˜ í•™ìŠµí•œ(ì•„ë§ˆë„ ê·¸ ë°ì´í„°ì…‹ì— ë‚´ì¬í•œ í¸í–¥ë„ ì˜ í•™ìŠµí•œ) ëª¨ë¸ì´ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì— ì–¼ë§ˆë‚˜ ì„±ëŠ¥ì´ ì¢‹ì€ì§€ ë³´ëŠ” ê²ƒì´ë‹¤. ì´ê±´ ì‹¤ì œ ì„¸ê³„ì—ì„œì˜ ìƒí™©ê³¼ ìœ ì‚¬í•œë°, ëª¨ë¸ì€ ê²°êµ­ ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œ ìƒì„±ëœ ë°ì´í„°ì— ë…¸ì¶œë  ìˆ˜ ë°–ì— ì—†ê¸° ë•Œë¬¸ì´ë‹¤. \n- ì´ë¥¼ í†µí•´ *'í•œ ë°ì´í„°ì…‹ì„ ì˜ í•™ìŠµí•˜ì—¬ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ì´ë¼ë©´, ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ë„ ì˜ ì¼ë°˜í™”ë¥¼ í•  ìˆ˜ ìˆì„ ê²ƒ*'ì´ë¼ëŠ” ê°€ì„¤ì„ ì‹¤ì œë¡œ í™•ì¸í•´ë³´ëŠ” ê²ƒì´ë‹¤. \n\n\n## ì‹¤í—˜ ë°©ë²•\n\n\u003e To explore how well the Toxic class from the Wiki-dataset generalizes to other types of offensive behaviour, **we train a binary classifier (Toxic vs. Normal) on the Wiki-dataset (combining the train, development and test sets) and test it on the Out-of-Domain Test set.** This classifier is expected to predict a positive (Toxic) label for the instances of classes Founta-Abusive, Founta-Hateful, Waseem-Sexism and Waseem-Racism, and a negative (Normal) label for the tweets in the Founta-Normal class. We fine-tune a BERT-based classifier (Devlin et al., 2019) with a linear prediction layer, the batch size of 16 and the learning rate of 2 Ã— 10âˆ’5 for 2 epochs.\n\n- ì €ìë“¤ì€ Wiki-datasetìœ¼ë¡œ í›ˆë ¨í•œ ëª¨ë¸ì´ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì— ì–¼ë§ˆë‚˜ ì˜ ì¼ë°˜í™”í•˜ëŠ”ê°€ë¥¼ ë³´ê¸° ìœ„í•´, binary classifierë¥¼ wiki-Datasetìœ¼ë¡œ í›ˆë ¨ì‹œí‚¤ê³  *'ë„ë©”ì¸ ì™¸ í…ŒìŠ¤íŠ¸ì…‹(the Out-of-Domain Test set)'* ì— ì´ë¥¼ í…ŒìŠ¤íŠ¸ í–ˆë‹¤. ëª¨ë¸ì€ BERTë¥¼ ì‚¬ìš©í–ˆë‹¤.\n\n\n## ì‹¤í—˜ ê²°ê³¼\n\n![Table 3](notes/images/table3.png)\n\n\u003e Results: The overall performance of the classifier on the Out-of-Domain test set is quite high: weighted macro-averaged F1 = 0.90.\n- ì €ìë“¤ì˜ ì˜ˆìƒê³¼ ë‹¬ë¦¬ ì „ì²´ì ì¸ Out-of-Domain test ì„±ëŠ¥ì€ ë†’ì€ í¸ì´ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ Waseem ë°ì´í„°ì…‹ì˜ Sexist, Racist classë¥¼ ë¶„ë¥˜í•˜ëŠ” ë°ì—ëŠ” Wiki-Datasetì˜ Toxic classë¡œ í›ˆë ¨ëœ ëª¨ë¸ì´ ì í•©í•˜ì§€ ì•Šë‹¤ëŠ” ì‚¬ì‹¤ì„ í™•ì¸í–ˆë‹¤. \n\n### Formulationì— ëŒ€í•œ ë…¼ì˜ \n#key-observation\n\u003e The impact of task formulation: From task formulations described in Section 3, observe that the Wiki-dataset defines the class Toxic in a general way. The class Founta-Abusive is also a general formulation of offensive behaviour. The similarity of these two definitions is reflected clearly in our results.\n- í¥ë¯¸ë¡œìš´ ë¶„ì„ì€ Formulationì— ëŒ€í•œ ê²ƒì´ë‹¤. ë¨¼ì € Wiki datasetì˜ Tosic classì— ëŒ€í•œ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ : 'The class Toxic comprises rude, hateful, aggressive, disrespectful or unreasonable comments that are likely to make a person leave a conversation'.\n- ê·¸ëŸ°ë° ì´ê²ƒì´, Waseem ë°ì´í„°ì…‹ì˜ Sexist, Racist classë¥¼ ë¶„ë¥˜í•˜ê¸°ì—ëŠ” ë‹¤ì†Œ ì¼ë°˜ì ì¸ ì •ì˜ë¼ëŠ” ê²ƒì´ë‹¤. \n\n## Impact of Data Size on Generalizability\n#data-size\n\n\u003e Observe that the average accuracies remain unchanged when the datasetâ€™s size triples at the same class balance ratio. This finding contrasts with the general assumption that more training data results in a higher classification performance.\n\n- ë‹¤ìŒìœ¼ë¡œ ì €ìëŠ” ë˜ í¥ë¯¸ë¡œìš´ í¬ì¸íŠ¸ë¥¼ í•˜ë‚˜ ë” í™•ì¸í–ˆë‹¤. \n- ë§Œì•½ classì˜ ë¹„ìœ¨ì´ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë°ì´í„°ì˜ í¬ê¸°ê°€ ì»¤ì§€ë”ë¼ë„ ì •í™•ë„(`accuracy`)ëŠ” ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” ë” ë§ì€ í›ˆë ¨ë°ì´í„°ê°€ í•­ìƒ ë†’ì€ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë‚¸ë‹¤ëŠ” general assumptionê³¼ ë°˜ëŒ€ë˜ëŠ” ê²°ê³¼ì´ë‹¤.\n\n## Discussion\n\n\u003e In the task of online abuse detection, both False Positive and False Negative errors can lead to significant harm as one threatens the freedom of speech and ruins peopleâ€™s reputations, and the other ignores hurtful behaviour.\n- False Positiveì™€ False NegativeëŠ” í‘œí˜„ì˜ ììœ ë¥¼ ìœ„í˜‘í•  ìˆ˜ ìˆë‹¤. \n\n\u003e We suggest evaluating each class (both positive and negative) separately taking into account the potential costs of different types of errors.\n- ê·¸ë¦¬ê³  ì €ìë“¤ì€ ê° classë¥¼ ë”°ë¡œ í‰ê°€í•˜ëŠ” ê²ƒì„ ì œì•ˆí–ˆë‹¤. ","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/Picking-Numbers":{"title":"Picking Numbers","content":"\n\u003e Given an array of integers, find the longest subarray where the absolute difference between any two elements is less than or equal toÂ 1.Â \n\n**Example**\n\na = [1,1,2,2,4,4,5,5,5]\n\nThere are two subarrays meeting the criterion:Â Â [1,1,2,2,] andÂ [4,4,5,5,5]. The maximum length subarray hasÂ 5Â elements.\n\n**Function Description**\n\nComplete theÂ _pickingNumbers_Â function in the editor below.Â \n\npickingNumbers has the following parameter(s):Â \n\n- int a[n] :Â an array of integersÂ \n\n**Returns**\n\n- int :Â the length of the longest subarray that meets the criterionÂ \n\n**Input Format**\n\nThe first line contains a single integerÂ _n_, the size of the arrayÂ _a_.Â   \nThe second line containsÂ _n_Â space-separated integers, each *a[i]*Â .\n\n**Constraints**\n- 2 \u003c= n \u003c= 100\n- 0 \u003c a[i] \u003c 100\n-   The answer will beÂ =\u003e 2.Â \n\n**Sample Input 0**\n\n```\n6\n4 6 5 3 3 1\n```\n\n**Sample Output 0**\n\n3\n\n**Explanation 0**\n\nWe choose the following multiset of integers from the array:Â {4,3,3}. Each pair in the multiset has an absolute differenceÂ Â \u003c= 1(i.e.,Â |4-1| = 1 and |3-3| = 0), so we print the number of chosen integers,Â 3, as our answer.\n\n---\n## ë‚˜ì˜ í’€ì´\n\n```python\nimportÂ math\n\nimportÂ os\n\nimportÂ random\n\nimportÂ re\n\nimportÂ sys\n\nfromÂ collectionsÂ importÂ Counter\n\n#\n\n#Â CompleteÂ theÂ 'pickingNumbers'Â functionÂ below.\n\n#\n\n#Â TheÂ functionÂ isÂ expectedÂ toÂ returnÂ anÂ INTEGER.\n\n#Â TheÂ functionÂ acceptsÂ INTEGER_ARRAYÂ aÂ asÂ parameter.\n\n#\n\n  \n\ndefÂ pickingNumbers(a):\n\nÂ Â Â Â #Â imputÂ isÂ unÂ arrayÂ ofÂ numbers.\n\nÂ Â Â Â count_numsÂ =Â Counter(a)\n\nÂ Â Â Â max_numÂ =Â 0\n\nÂ Â Â Â forÂ iÂ inÂ range(1,Â 100):\n\nÂ Â Â Â Â Â Â Â max_numÂ =Â max(max_num,Â \n\t\t\t\tÂ Â count_nums[i]Â +Â count_nums[i+1])\n\nÂ Â Â Â returnÂ max_num\n\nifÂ __name__Â ==Â '__main__':\n\nÂ Â Â Â fptrÂ =Â open(os.environ['OUTPUT_PATH'],Â 'w')\n\n  \n\nÂ Â Â Â nÂ =Â int(input().strip())\n\n  \n\nÂ Â Â Â aÂ =Â list(map(int,Â input().rstrip().split()))\n\n  \n\nÂ Â Â Â resultÂ =Â pickingNumbers(a)\n\n  \n\nÂ Â Â Â fptr.write(str(result)Â +Â '\\n')\n\n  \n\nÂ Â Â Â fptr.close()\n```\n\n\u003e [!note]  \n\u003e \n\u003e ì´ ë¬¸ì œëŠ” `Counter()`ë¡œ ì¼ë‹¨ ì„¸ê³  ì‹œì‘í•˜ë©´ ê°„ë‹¨í•œë‹¤.\n\u003e ê°œìˆ˜ë¥¼ ì„¼ ì´í›„ì—ëŠ” ì–‘ ì˜†ì˜ í¬ê¸°ë¥¼ ë”í•œ ê²ƒì´ ê°€ì¥ í° ê²½ìš°ê°€ ë‹µì´ë¯€ë¡œ.\n","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/Studying-Generalisability-Across-Abusive-Language-Detection-Datasets":{"title":"Studying Generalisability Across Abusive Language Detection Datasets","content":"\n\u003e [!note] Reference  \n\u003e \n\u003e Steve Durairaj Swamy, Anupam Jamatia, and BjÃ¶rn GambÃ¤ck. 2019.Â [Studying Generalisability across Abusive Language Detection Datasets](https://aclanthology.org/K19-1088). InÂ _Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)_, pages 940â€“950, Hong Kong, China. Association for Computational Linguistics.\n\n## Previous Work\n\n\u003e Abusive language detection has served as an umbrella term for a wide variety of subtasks. Research in the field has typically focused on a particular subtask: Hate Speech (Davidson et al., 2017; Founta et al., 2018; Gao and Huang, 2017; Golbeck et al., 2017), Sexism/Racism (Waseem and Hovy, 2016), Cyberbullying (Xu et al., 2012; Dadvar et al., 2013), Trolling and Aggression (Kumar et al., 2018a), and so on. \n\u003e Datasets for these tasks have been collected from various social media platforms, such as Twitter (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018; Burnap and Williams, 2015; Golbeck et al., 2017), Facebook (Kumar et al., 2018a), Instagram (Hosseinmardi et al., 2015; Zhong et al., 2016), Yahoo! (Nobata et al., 2016; Djuric et al., 2015; Warner and Hirschberg, 2012), YouTube (Dinakar et al., 2011), and Wikipedia (Wulczyn et al., 2017), with annotation typically carried out on crowdsourcing platforms such as CrowdFlower (Figure Eight)1 and Amazon Mechanical Turk.\n- Abusive language detectionì— ëŒ€í•œ í•´ì™¸ ì—°êµ¬ ë ˆí¼ëŸ°ìŠ¤ê°€ ì˜ ì„¤ëª…ë˜ì–´ ìˆë‹¤.\n\n\u003e In the â€˜OffensEvalâ€™ shared task (Zampieri et al., 2019b), the use of contextual embeddings such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) exhibited the best results.\n- BERTê°€ ë“±ì¥í•˜ê³  íƒì§€ ì—°êµ¬ì— ì“°ì´ê¸° ì‹œì‘. \n\n#key-observation \n\u003e Generalisability of a model has also come under considerable scrutiny. Works such as Karan and Å najder (2018) and GrÃ¶ndahl et al. (2018) have shown that **models trained on one dataset tend to perform well only when tested on the same dataset.**\n- ì´ ë¶€ë¶„ì´ ë‚´ ì—°êµ¬ì˜ í•µì‹¬ê³¼ ê´€ë ¨ì´ ê¹Šë‹¤. \n\n\u003e Additionally, GrÃ¶ndahl et al. (2018) showed how adversarial methods such as typos and word changes could bypass existing state-of- the-art abusive language detection systems.\n- \"All you need is â€œloveâ€: Evading hate speech detection\" ë…¼ë¬¸ ì €ìì´ë‹¤. \n\n\u003e Fortuna et al. (2018) concurred, stating that although models perform better on the data they are trained on, slightly improved performance can be obtained when adding more training data from other social media.\n- Fortuna et al. (2018) ì—°êµ¬ë„ ìˆì—ˆêµ¬ë‚˜. ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ë” ì¶”ê°€í•´ í›ˆë ¨ì‹œí‚¤ë©´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ê²Œ ëœë‹¤ê³  í–ˆì—ˆë„¤. ì§€ê¸ˆì˜ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì— ëŒ€í•œ ë…¼ì˜ê°€ ì—¬ê¸°ì—ì„œ ì¶œë°œí–ˆë‹¤.\n\n## Preliminary Feature and Model Study\n\n\u003e However, fine-tuning was carried out on the mod- elsâ€™ hyper-parameters, such as sequence length, drop out, and class weights. Test and training sets were created for each dataset by performing a stratified split of 20% vs 80%, with the larger part used for training the models. The training sets were further subdivided, keeping 1/8 shares of them as separate validation sets during devel- opment and fine-tuning of the hyper-parameters.\n- **ê·¸ëŸ¬ë‚˜ ì‹œí€€ìŠ¤ ê¸¸ì´, ë“œë¡­ì•„ì›ƒ ë° í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ì™€ ê°™ì€ ëª¨ë¸ì˜ ì´ˆ ë§¤ê°œ ë³€ìˆ˜ì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •ì´ ìˆ˜í–‰ë˜ì—ˆë‹¤. í…ŒìŠ¤íŠ¸ ë° í›ˆë ¨ ì„¸íŠ¸ëŠ” ê° ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ 20% ëŒ€ 80%ì˜ ê³„ì¸µí™”ëœ ë¶„í• ì„ ìˆ˜í–‰í•˜ì˜€ê³ , ì´í›„ ë” í° ë¶€ë¶„ì„ ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©í•˜ì˜€ë‹¤.**  í›ˆë ¨ ì„¸íŠ¸ëŠ” ë”ìš± ì„¸ë¶„í™”ë˜ì–´ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ê°œë°œ ë° ë¯¸ì„¸ ì¡°ì • ì¤‘ì— 1/8 ê³µìœ ë¥¼ ë³„ë„ì˜ ê²€ì¦ ì„¸íŠ¸ë¡œ ìœ ì§€í•˜ì˜€ë‹¤.\n- ëª¨ë¸ í›ˆë ¨ì— ëŒ€í•´ì„œëŠ” ì´ë ‡ê²Œ ì„¤ëª…í•˜ë©´ ë˜ê² ë‹¤.\n\n\u003e The best models used a learning rate of eâˆ’5 and batch size 32 with varying maximum sequence lengths between 60 and 70. Other parameters worth mentioning are the number of epochs and the Linear Warm-up Proportion.\n- ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ê²½ìš°ì—ëŠ” ì´ë ‡ê²Œ í‘œí˜„í•˜ë©´ ëœë‹¤.\n- ì–´ë¼, ê·¸ëŸ°ë° ì´ê±° ë³´ë‹¤ë³´ë‹ˆ [On Cross-Dataset Generalization in Automatic Detection of Online Abuse](notes/On%20Cross-Dataset%20Generalization%20in%20Automatic%20Detection%20of%20Online%20Abuse.md) ì´ë‘ ì„¤ëª…ì´ ë˜‘ê°™ë‹¤. \n\n![Table 1: Overview of the datasets by Davidson et al., Founta et al., Waseem and Hovy, and Zampieri et al.](Datasets-overview.png)\n- ì „ì²´ ë°ì´í„°ì…‹ì˜ ê°œìš”ëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. \n\n## Cross-Dataset Training and Testing\n\n![Table 4: Cross-dataset test results (accuracy and macro-F1)](Cross-dataset-test-results.png)\n\u003e Considerable performance drops can be observed when going from a large training dataset to a small test set (i.e., Founta et al.â€™s results when tested on the Waseem and Hovy dataset) and vice versa. This is in line with a similar conclusion by Karan and Å najder(2018). \n- í° ë°ì´í„°ì…‹ë¶€í„° ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ê°ˆ ë•Œ í¼í¬ë¨¼ìŠ¤ í•˜ë½ì´ ë³´ì¸ë‹¤. \n\n\u003e The most interesting observation is that **datasets with larger percentages of positive samples tend to** **generalise better than datasets with fewer positive samples**, in particular when **tested against dissimilar datasets**. For example, we see that the models trained on the Davidson et al. dataset, which contains a majority of offensive tags, perform well when tested on the Founta et al. dataset, which contains a majority of non-offensive tags.\n- ê°€ì¥ í¥ë¯¸ë¡œìš´ ê´€ì°°ì€ ì–‘ì„± ìƒ˜í”Œì˜ ë¹„ìœ¨ì´ í° ë°ì´í„° ì„¸íŠ¸ê°€ íŠ¹íˆ ë‹¤ë¥¸ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸í•  ë•Œ ì–‘ì„± ìƒ˜í”Œì´ ì ì€ ë°ì´í„° ì„¸íŠ¸ë³´ë‹¤ ë” ì˜ ì¼ë°˜í™”ë˜ëŠ” ê²½í–¥ì´ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ëŒ€ë‹¤ìˆ˜ì˜ ê³µê²© íƒœê·¸ë¥¼ í¬í•¨í•˜ëŠ” Davidson ë“± ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ í›ˆë ¨ëœ ëª¨ë¸ì€ ëŒ€ë‹¤ìˆ˜ì˜ ë¹„ê³µê²© íƒœê·¸ë¥¼ í¬í•¨í•˜ëŠ” Founta ë“± ë°ì´í„° ì„¸íŠ¸ì—ì„œ í…ŒìŠ¤íŠ¸ë  ë•Œ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n- ë‚´ ì—°êµ¬ë„ ì´ëŸ° ê²ƒì´ ìˆë‚˜ í™•ì¸í•´ë³´ì. \n\n## Discussion and Conclusion\n\n\u003e Second, experiments showing that datasets with larger percentages of positive samples generalise better than datasets with fewer positive samples when tested against a dissimilar dataset (at least within the same platform, e.g., Twitter), which indicates that a more balanced dataset is healthier for generalisation.\n\n\u003e **An overall conclusion is that the data is more important than the model when tackling Abusive Language Detection.**\n- ë°ì´í„°, ë°ì´í„°! ê²°êµ­ì€ ë°ì´í„°ì˜ ì¤‘ìš”ì„±ì„ ë§í•˜ë©° ì´ ë…¼ë¬¸ì€ ëì„ ë‚¸ë‹¤. ","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/Towards-generalisable-hate-speech-detection":{"title":"Towards generalisable hate speech detection","content":"\n\u003e [!info] Reference  \n\u003e \n\u003e Yin, W., \u0026 Zubiaga, A. (2021). Towards generalisable hate speech detection: a review on obstacles and solutions.Â _PeerJ Computer Science_,Â _7_, e598.\n\n---\n\n## Generalisation\n\n\u003e Most if not all proposed hate speech detection models rely on supervised machine learning methods, where the ultimate purpose is for the model to learn the real relationship between features and predictions through training data, which generalises to previously unobserved inputs (Goodfellow, Bengio \u0026 Courville, 2016). The generalisation performance of a model measures how well it fulfils this purpose.\n- ì œì•ˆëœ í˜ì˜¤ ë°œì–¸ íƒì§€ ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ supervised ê¸°ê³„ í•™ìŠµ ë°©ë²•ì— ì˜ì¡´í•˜ë©°, ê¶ê·¹ì ì¸ ëª©ì ì€ ëª¨ë¸ì´ ì´ì „ì— ê´€ì°°ë˜ì§€ ì•Šì€ ì…ë ¥ìœ¼ë¡œ ì¼ë°˜í™”í•˜ëŠ” í›ˆë ¨ ë°ì´í„°ë¥¼ í†µí•´ ê¸°ëŠ¥ê³¼ ì˜ˆì¸¡ ì‚¬ì´ì˜ ì‹¤ì œ ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤(Goodfellow, Bengio \u0026 Courville, 2016). ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ê³¼ëŠ” ì´ ëª©ì ì„ ì–¼ë§ˆë‚˜ ì˜ ë‹¬ì„±í•˜ëŠ”ì§€ ì¸¡ì •í•œë‹¤.\n\n\u003e The ultimate purpose of studying automatic hate speech detection is to facilitate the alleviation of the harms brought by online hate speech. To fulfil this purpose, hate speech detection models need to be able to deal with the constant growth and evolution of hate speech, regardless of its form, target, and speaker.\n- ìë™ í˜ì˜¤í‘œí˜„ íƒì§€ë¥¼ ì—°êµ¬í•˜ëŠ” ê¶ê·¹ì ì¸ ëª©ì ì€ ì˜¨ë¼ì¸ í˜ì˜¤í‘œí˜„ì´ ê°€ì ¸ì˜¤ëŠ” í•´ì•…ì˜ ì™„í™”ë¥¼ ìš©ì´í•˜ê²Œ í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ëª©ì ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´, í˜ì˜¤í‘œí˜„ íƒì§€ ëª¨ë¸ì€ í˜•íƒœ, ëŒ€ìƒ ë° í™”ìì— ê´€ê³„ì—†ì´ í˜ì˜¤ ë°œì–¸ì˜ ì§€ì†ì ì¸ ì„±ì¥ê³¼ ì§„í™”ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.\n\n#key-observation \n\u003eRecent research has raised concerns on the generalisability of existing models (Swamy, Jamatia \u0026 GambaÌˆck, 2019). Despite their impressive performance on their respective test sets, **the performance significantly dropped when the models are applied to a different hate speech dataset.** This means that the assumption that test data of existing datasets represent the distribution of future cases is not true, and that **the generalisation performance of existing models have been severely overestimated** (Arango, Prez \u0026 Poblete, 2020). This lack of generalisability undermines the practical value of these hate speech detection models.\n- ìµœê·¼ ì—°êµ¬ëŠ” ê¸°ì¡´ ëª¨ë¸ì˜ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì— ëŒ€í•œ ìš°ë ¤ë¥¼ ì œê¸°í–ˆë‹¤(Swamy, Jamatia \u0026 Gambeck, 2019 : [Studying Generalisability Across Abusive Language Detection Datasets](notes/Studying%20Generalisability%20Across%20Abusive%20Language%20Detection%20Datasets.md)).  **ê°ê°ì˜ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ì¸ìƒì ì¸ ì„±ëŠ¥ì—ë„ ë¶ˆêµ¬í•˜ê³  ëª¨ë¸ì´ ë‹¤ë¥¸ í˜ì˜¤ ìŒì„± ë°ì´í„° ì„¸íŠ¸ì— ì ìš©ë  ë•Œ ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì¡Œë‹¤.** ì´ëŠ” ê¸°ì¡´ ë°ì´í„° ì„¸íŠ¸ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¯¸ë˜ì˜ ì‚¬ë¡€ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ëŠ” ê°€ì •ì´ ì‚¬ì‹¤ì´ ì•„ë‹ˆë©°, **ê¸°ì¡´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì‹¬ê°í•˜ê²Œ ê³¼ëŒ€ í‰ê°€ë˜ì—ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤**(Arango, Pres \u0026 Poblete, 2020 : [Hate speech detection is not as easy as you may think](notes/Hate%20speech%20detection%20is%20not%20as%20easy%20as%20you%20may%20think.md)). ì´ëŸ¬í•œ ì¼ë°˜ì„±ì˜ ë¶€ì¡±ì€ ì´ëŸ¬í•œ í˜ì˜¤í‘œí˜„ íƒì§€ ëª¨ë¸ì˜ ì‹¤ì§ˆì ì¸ ê°€ì¹˜ë¥¼ í›¼ì†í•œë‹¤.\n\n\u003e[!note] Note  \n\u003e\n\u003eì´ ë¶€ë¶„ì´ ë‚´ê°€ í•˜ê³  ìˆëŠ” ì—°êµ¬ì˜ í•µì‹¬ì´ë‹¤! ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì´ ê³¼ëŒ€í‰ê°€ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒ. í•œêµ­ì–´ ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ë¡œë„ ë¹„ìŠ·í•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ”ì§€ ë³´ëŠ” ê²ƒ.\n\n## Data\n\n\u003e [ì˜ˆì‹œ 1]\n\u003e For example, in Wiegand, Ruppenhofer \u0026 Kleinbauer (2019)â€™s study, FastText models (Joulin et al., 2017a) trained on three datasets (Kaggle, Founta, Razavi) achieved F1 scores above 70 when tested on one another, **while models trained or tested on datasets outside this group achieved around 60 or less**.\n- ëª¨ë¸ì´ í›ˆë ¨ëœ ê²ƒê³¼ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤ëŠ” ê²°ê³¼ê°€ ìˆë‹¤.\n\n#key-observation \n\u003e Founta and OLID produced models that performed well on each other. The source of such differences are usually traced back to search terms (Swamy, Jamatia \u0026 GambÃ¤ck, 2019), topics covered (Nejadgholi \u0026 Kiritchenko, 2020; Pamungkas, Basile \u0026 Patti, 2020), label definitions (Pamungkas \u0026 Patti, 2019; Pamungkas, Basile \u0026 Patti, 2020; Fortuna, Soler-Company \u0026 Wanner, 2021), and data source platforms (GlavaÅ¡, Karan \u0026 VuliÄ‡, 2020; Karan \u0026 Å najder, 2018).\n- ì„œë¡œ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¤ëŠ” ë°ì´í„°ì…‹ì€ ê·¸ ê·¼ì›ì„ ë”°ë¼ê°€ë³´ë©´ ì•Œ ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ì´ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´,` Founta`ì™€ `OLID` ë°ì´í„°ì…‹ì€ ì„œë¡œ ë¹„ìŠ·í•œ ë°ì´í„°ë¥¼ ê³µìœ í•˜ê³  ìˆë‹¤. \n\n\u003e Fortuna, Soler \u0026 Wanner (2020) used averaged word embeddings (Bojanowski et al., 2017; Mikolov et al., 2018) to compute the representations of classes from different datasets, and compared classes across datasets. **One of their observations is that Davidsonâ€™s â€˜â€˜hate speechâ€™â€™ is very different from Waseemâ€™s â€˜â€˜hate speechâ€™â€™, â€˜â€˜racismâ€™â€™, â€˜â€˜sexismâ€™â€™, while being relatively close to HatEvalâ€™s â€˜â€˜hate speechâ€™â€™ and Kaggleâ€™s â€˜â€˜identity hateâ€™â€™.** This echoes with experiments that showed poor generalisation of models from Waseem to HatEval (Arango, Prez \u0026 Poblete, 2020) and between Davidson and Waseem (Waseem, Thorne \u0026 Bingel, 2018; GroÌˆndahl et al., 2018).\n- í˜ì˜¤í‘œí˜„ ë°ì´í„°ì…‹ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì¸ 'hate speech', 'racism', 'sexism' ë“±ë„ ì›Œë“œ ì„ë² ë”©ì„ í†µí•´ ì‚´í´ë³´ë‹ˆ ë°ì´í„°ì…‹ë§ˆë‹¤ ê·¸ ì˜ë¯¸ê°€ ë‹¤ë¥´ë‹¤ëŠ” ê´€ì°°ì´ ë‚˜ì™”ë‹¤. ì´ëŠ” ë‹¹ì—°íˆ ëª¨ë¸ ì„±ëŠ¥ì˜ ì¼ë°˜í™”ì—ë„ ì•…ì˜í–¥ì„ ë¼ì³¤ê³  ë§ì´ë‹¤. \n\n\u003e In terms of what properties of a dataset lead to more generalisable models, there are frequently mentioned factors (...)\n\n\u003e Biases in the samples are also frequently mentioned. **Wiegand, Ruppenhofer \u0026 Kleinbauer (2019) hold that less biased sampling approaches produce more generalisable models.** This was later reproduced by Razo \u0026 KÃ¼bler (2020) and also helps explain their results with the two datasets that have the least positive cases. Similarly, Pamungkas \u0026 Patti (2019) mentioned that a wider coverage of phenomena lead to more generalisable models. \n- `Wiegand, Ruppenhofer \u0026 Kleinbauer (2019)` ì—°êµ¬ì—ì„œ ì–¸ê¸‰í•œ ë°”ì™€ ê°™ì´, ì¡°ê¸ˆì´ë¼ë„ ë” ì¼ë°˜í™”ê°€ ì˜ ë˜ëŠ” ëª¨ë¸ì„ ë§Œë“œë ¤ë©´ samplingì„ ëœ ì¹˜ìš°ì¹˜ê²Œ í•´ì£¼ì–´ì•¼ í•œë‹¤. í›ˆë ¨ì‹œ `sampler`ë¥¼ ì˜ ë§Œë“¤ì–´ì•¼ê² ë‹¤.\n\n\u003e Another way of looking at generalisation and similarity is by comparing differences between individual classes across datasets (Nejadgholi \u0026 Kiritchenko, 2020; Fortuna, Soler \u0026 Wanner, 2020; Fortuna, Soler-Company \u0026 Wanner, 2021), as opposed to comparing datasets as a whole.\n- ì´ ë…¼ë¬¸ì—ì„œë„ class ê°œë³„ë¡œ ë¹„êµí•˜ë¼ê³  ì£¼ì¥í•˜ëŠ”êµ¬ë‚˜. [On Cross-Dataset Generalization in Automatic Detection of Online Abuse](notes/On%20Cross-Dataset%20Generalization%20in%20Automatic%20Detection%20of%20Online%20Abuse.md) ì—ì„œ ì£¼ì¥í•˜ëŠ” ê²ƒê³¼ ë§ë¬¼ë¦°ë‹¤. \n\n## OBSTACLES TO GENERALISABLE HATE SPEECH DETECTION\n\n\u003e Hate speech detection, which is largely focused on social media, shares similar challenges to other social media tasks and has its specific ones, **when it comes to the grammar and vocabulary used.** Such user language style introduces challenges to generalisability at the data source, mainly by making it difficult to utilise common NLP pre-training approaches.\n- í˜ì˜¤í‘œí˜„ íƒì§€ëŠ” ë¬¸ë²•ì´ë‚˜ ì–´íœ˜ ê´€ë ¨í•´ì„œ ì–´ë ¤ì›€ì´ ë§ë‹¤ëŠ” íŠ¹ì§•ì´ ìˆë‹¤. \n\n\u003e On social media, syntax use is generally more casual, such as the omission of punctuation (Blodgett \u0026 Oâ€™Connor, 2017). Alternative spelling and expressions are also used in dialects (Blodgett \u0026 Oâ€™Connor, 2017), to save space, and to provide emotional emphasis (Baziotis, Pelekis \u0026 Doulkeridis, 2017). Sanguinetti et al. (2020) provided extensive guidelines for studying such phenomena syntactically.\n- ê·¸ë˜ì„œ í˜ì˜¤í‘œí˜„ íƒì§€ ì—°êµ¬ë¥¼ í•  ë•ŒëŠ” KcELECTRAê°€ ê·¸ë‚˜ë§ˆ ê´œì°®ê² êµ¬ë‚˜. ì´ëŸ¬í•œ ì¼€ì´ìŠ¤ê°€ ë§ì€ ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì´ë‹ˆê¹Œ. ì‹¤ì œë¡œ ì„±ëŠ¥ë„ ê°€ì¥ ê´œì°®ê³ .\n\n\u003e Qian et al. (2018) found that rare words and implicit expressions are the two main causes of false negatives; Van Aken et al. (2018) compared several models that used pre-trained word embeddings, and found that rare and unknown words were present in 30% of the false negatives of Wikipedia data and 43% of Twitter data.\n- ë˜í•œ rare words, implicit expressionsëŠ” false negativesë¥¼ ì¦ê°€ì‹œí‚¨ë‹¤. ì´ëŠ” ë”°ë¼ì„œ í•˜ë‚˜ì˜ ë„ë©”ì¸ì—ì„œë§Œ ìˆ˜ì§‘í•œ ë°ì´í„°ì…‹ì´ ê°€ì§€ëŠ” í•œê³„ì¼ ìˆ˜ ë°–ì— ì—†ê² ë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ë ¤ë©´ ì—¬ëŸ¬ ë„ë©”ì¸ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì•¼ê² ë„¤.\n\n\u003eIndeed, BERT (Devlin et al., 2019) and its variants have demonstrated top performances at hate or abusive speech detection challenges recently (Liu, Li \u0026 Zou, 2019; Mishra \u0026 Mishra, 2019).\n- BERT ê³„ì—´ ëª¨ë¸ì€ í˜ì˜¤í‘œí˜„ íƒì§€ì—ì„œë„ ì—¬ì „íˆ top í¼í¬ë¨¼ìŠ¤ë¥¼ ë³´ì¸ë‹¤. \n\n\u003eIt is particularly challenging to acquire labelled data for hate speech detection as knowledge or relevant training is required of the annotators. As a high-level and abstract concept, the judgement of â€˜â€˜hate speechâ€™â€™ is subjective, needing extra care when processing annotations. Hence, datasets are usually not big in size.\n- í˜ì˜¤í‘œí˜„ ë°ì´í„°ì…‹ì€ 'í˜ì˜¤í‘œí˜„'ì„ ì •ì˜í•˜ëŠ” ê²ƒ ìì²´ê°€ ì£¼ê´€ì ì´ê¸° ë•Œë¬¸ì—, ì£¼ì„ ì²˜ë¦¬ì— ì¶”ê°€ì ì¸ í˜ì´ ë“¤ê³  ë”°ë¼ì„œ í° ì‚¬ì´ì¦ˆë¡œ ë§Œë“¤ì–´ì§€ê¸° ì–´ë µë‹¤.\n\n\u003e Moreover, **different studies are based on varying definitions of â€˜â€˜hate speechâ€™â€™, as seen in different annotation guidelines** (Table 5). Despite all covering the same two main aspects (directly attack or promote hate towards), datasets vary by their wording, what they consider a target (any group, minority groups, specific minority groups), and their clarifications on edge cases.\n\u003e Davidson and HatEval both distinguished â€˜â€˜hate speechâ€™â€™ from â€˜â€˜offensive languageâ€™â€™, while â€˜â€˜uses a sexist or racist slurâ€™â€™ is in Waseemâ€™s guidelines to mark a case positive of hate, **blurring the boundary of offensive and hateful.**\n\u003e Additionally, as both HatEval and Waseem specified the types of hate (towards women and immigrants; racism and sexism), hate speech that fell outside of these specific types were not included in the positive classes, while Founta and Davidson included any type of hate speech.\n- ë˜í•œ, ë‹¤ë¥¸ ì£¼ì„ ì§€ì¹¨(í‘œ 5)ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ë‹¤ì–‘í•œ ì—°êµ¬ëŠ” \"í˜ì˜¤ ë°œì–¸\"ì˜ ë‹¤ì–‘í•œ ì •ì˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤. ëª¨ë“  ê²ƒì´ ë™ì¼í•œ ë‘ ê°€ì§€ ì£¼ìš” ì¸¡ë©´ì„ í¬í•¨í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ë°ì´í„° ì„¸íŠ¸ëŠ” í‘œí˜„, ëŒ€ìƒìœ¼ë¡œ ê°„ì£¼í•˜ëŠ” ê²ƒ(ëª¨ë“  ê·¸ë£¹, ì†Œìˆ˜ ê·¸ë£¹, íŠ¹ì • ì†Œìˆ˜ ê·¸ë£¹) ë° ì—£ì§€ ì‚¬ë¡€ì— ëŒ€í•œ ëª…í™•í™”ì— ë”°ë¼ ë‹¤ë¥´ë‹¤. \n- Davidsonê³¼ HatEvalì€ ëª¨ë‘ \"hate speech\"ì™€ \"offensive language\"ë¥¼ êµ¬ë¶„í–ˆìœ¼ë©°, \"ì„±ì°¨ë³„ì  ë˜ëŠ” ì¸ì¢…ì°¨ë³„ì  ë¹„ë°© ì‚¬ìš©\"ì€ Waseemì˜ ê°€ì´ë“œë¼ì¸ì— hateë¡œ í‘œì‹œí•˜ì—¬ offensiveì™€ hateì˜ ê²½ê³„ë¥¼ ëª¨í˜¸í•˜ê²Œ í•œë‹¤. \n- ë˜í•œ, HatEvalê³¼ Waseemì´ í˜ì˜¤ì˜ ìœ í˜•(ì—¬ì„±ê³¼ ì´ë¯¼ìì— ëŒ€í•œ ê²ƒ; ì¸ì¢… ì°¨ë³„ê³¼ ì„±ì°¨ë³„)ì„ ëª…ì‹œí•¨ì— ë”°ë¼, ì´ëŸ¬í•œ íŠ¹ì • ìœ í˜•ì—ì„œ ë²—ì–´ë‚œ í˜ì˜¤ ë°œì–¸ì€ ê¸ì •ì ì¸ ë“±ê¸‰ì— í¬í•¨ë˜ì§€ ì•Šì•˜ê³ , ë°˜ë©´ Fontaì™€ Davidsonì€ ëª¨ë“  ìœ í˜•ì˜ í˜ì˜¤ ë°œì–¸ì„ í¬í•¨ì‹œì¼°ë‹¤.\n\n","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/Two-Pointers":{"title":"âš™ï¸ Two Pointers","content":"\n**íˆ¬í¬ì¸í„° ì•Œê³ ë¦¬ì¦˜(Two Pointers Algorithm)** ë˜ëŠ” **ìŠ¬ë¼ì´ë”© ìœˆë„ìš°(Sliding Window)** ë¼ê³  ë¶€ë¥¸ë‹¤.\n\nì•Œê³ ë¦¬ì¦˜ ë¬¸ì œë¥¼ í’€ë‹¤ ì™„ì „íƒìƒ‰ìœ¼ë¡œ í•´ê²°í•˜ë©´ ì‹œê°„ ì´ˆê³¼ê°€ ë‚˜ëŠ” ë¬¸ì œê°€ ì¢…ì¢… ìˆëŠ”ë°, ì´ë•Œ ì‚¬ìš©í•˜ë©´ ë¹ ë¥´ê²Œ í•´ê²°í•  ìˆ˜ ìˆë‹¤. ê¸°ë³¸ì ì¸ ì›ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n\u003e 1ì°¨ì› ë°°ì—´ì´ ìˆê³ , ì´ ë°°ì—´ì—ì„œ ê°ì ë‹¤ë¥¸ ì›ì†Œë¥¼ ê°€ë¦¬í‚¤ê³  ìˆëŠ” 2ê°œì˜ í¬ì¸í„°ë¥¼ ì¡°ì‘í•´ê°€ë©´ì„œ ì›í•˜ëŠ” ê²ƒì„ ì–»ê¸°\n\nNì¹¸ì˜ 1ì°¨ì› ë°°ì—´ì´ ìˆì„ ë•Œ, ë¶€ë¶„ ë°°ì—´ ì¤‘ ê·¸ ì›ì†Œì˜ í•©ì´ Mì´ ë˜ëŠ” ê²½ìš°ì˜ ìˆ˜ë¥¼ êµ¬í•œë‹¤ê³  ìƒê°í•´ë³´ì. ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¥¼ ë‹¤ í…ŒìŠ¤íŠ¸ í•´ë³´ë©´ êµ¬ê°„ í•©ì„ êµ¬ê°„ ë°°ì—´ë¡œ `O(1)`ë§Œì— êµ¬í•œë‹¤ê³  í•´ë„ ê²½ìš°ì˜ ìˆ˜ëŠ” `O(N^2)`ì´ ëœë‹¤. ë”°ë¼ì„œ ë¬¸ì œë¥¼ í’€ ìˆ˜ ì—†ë‹¤. Nì˜ ìµœëŒ€ ë²”ìœ„ê°€ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ê²½ìš° ê° ì›ì†ŒëŠ” ìì—°ìˆ˜ì´ê³  M ë˜í•œ ìì—°ìˆ˜ì¸ë°, ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì´ í’€ ìˆ˜ ìˆë‹¤.\n\n- í¬ì¸í„° 2ê°œë¥¼ ì¤€ë¹„í•œë‹¤. ì‹œì‘ê³¼ ëì„ ì•Œ ìˆ˜ ìˆë„ë¡ start, end ë¼ê³  í•œë‹¤.\n- ë§¨ ì²˜ìŒì—ëŠ” start = end = 0ì´ë©°, í•­ìƒ start\u003c=endì„ ë§Œì¡±í•´ì•¼ í•œë‹¤.\n- 2ê°œì˜ í¬ì¸í„°ëŠ” í˜„ì¬ ë¶€ë¶„ ë°°ì—´ì˜ ì‹œì‘ê³¼ ëì„ ê°€ë¦¬í‚¤ëŠ” ì—­í• ì„ í•œë‹¤.\n\ns=eì¼ ê²½ìš° ê·¸ê±´ í¬ê¸°ê°€ 0ì¸, ì•„ë¬´ê²ƒë„ í¬í•¨í•˜ì§€ ì•ŠëŠ” ë¶€ë¶„ ë°°ì—´ì„ ëœ»í•œë‹¤. ë‹¤ìŒì˜ ê³¼ì •ì„ s \u003c Nì¸ ë™ì•ˆ ë°˜ë³µí•œë‹¤.\n\n1. ë§Œì•½ í˜„ì¬ ë¶€ë¶„í•©ì´ M ì´ìƒì´ê±°ë‚˜, ì´ë¯¸ e = Nì´ë©´ s++\n2. ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ e++\n3. í˜„ì¬ ë¶€ë¶„í•©ì´ Mê³¼ ê°™ìœ¼ë©´ ê²°ê³¼ ++ \n\nì‰½ê²Œ ì´í•´í•˜ìë©´, startì™€ end ë¥¼ ë¬´ì¡°ê±´ ì¦ê°€ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œë§Œ ë³€í™”ì‹œì¼œê°€ë©´ì„œ ë„ì¤‘ì— ë¶€ë¶„ ë°°ì—´ì˜ í•©ì´ ì •í™•íˆ Mì´ ë˜ëŠ” ê²½ìš°ë¥¼ ì„¸ëŠ” ê²ƒì´ë‹¤. \n\nEx) M = 5ì¸ ê²½ìš°ë¥¼ ì‚´í´ë³´ì.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_56/kks227_1471976777631dvMpe_PNG/1.png?type=w2)\n\nì´ˆê¸° ìƒíƒœì´ë©°, ë¹¨ê°„ìƒ‰ í¬ì¸í„° : start, íŒŒë€ìƒ‰ í¬ì¸í„° : endì´ë‹¤. S : í•©.\n\n**end**ê°€ ë’¤ë¡œ ì›€ì§ì¼ ë•ŒëŠ” ìƒˆë¡œ í¬í•¨í•œ ì›ì†Œë¥¼ Sì— ë”í•˜ê³ , **start**ê°€ ë’¤ë¡œ ì›€ì§ì¼ ë•ŒëŠ” ìƒˆë¡œ ë„˜ê¸´ ì›ì†Œë¥¼ Sì—ì„œ ë¹¼ëŠ” ì‹ìœ¼ë¡œ í˜„ì¬ [start, end)ì˜ í•© Së¥¼ ë§¤ë²ˆ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_196/kks227_1471976777962Qks67_PNG/2.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_274/kks227_1471976778508STsIS_PNG/3.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_118/kks227_1471976778842HkF4H_PNG/4.png?type=w2)\n\nì²˜ìŒì—ëŠ” ì´ë ‡ê²Œ endë§Œ ì¦ê°€í•˜ê²Œ ëœë‹¤. Sê°€ ê³„ì† Më³´ë‹¤ ì‘ê¸° ë•Œë¬¸! ë§ˆì§€ë§‰ì—” S\u003e=Mì´ ë˜ì—ˆìœ¼ë¯€ë¡œ ì•„ë˜ì™€ ê°™ë‹¤.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_101/kks227_1471976779156aosTT_PNG/5.png?type=w2)\n\nstartë¥¼ í•œ ì¹¸ ì˜®ê²¼ëŠ”ë°, ë™ì‹œì— S = 5ì¸ ê²½ìš°ë¥¼ ë§Œë‚¬ë‹¤. ì´ë•Œ ê²°ê³¼ë¥¼ 1 ì¦ê°€ì‹œì¼œ ì¤€ë‹¤.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_186/kks227_1471976779456z8WVP_PNG/6.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_1/kks227_1471976779887ko5yw_PNG/7.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_265/kks227_1471976780291PDw0Y_PNG/8.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_139/kks227_1471976780603hkxD5_PNG/9.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_284/kks227_1471976780877YjQiA_PNG/10.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_101/kks227_1471976781212P3Li0_PNG/11.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_188/kks227_14719767815252r1eQ_PNG/12.png?type=w2)\n\nì´ëŸ° ì‹ìœ¼ë¡œ í¬ì¸í„°ë“¤ì´ ì›€ì§ì´ê²Œ ëœë‹¤. ì—¬ê¸°ì„œ 2ë²ˆì§¸ë¡œ S = 5ì¸ ì§€ì ì„ ë§Œë‚¬ìœ¼ë¯€ë¡œ ê²°ê³¼ë¥¼ 1 ì¦ê°€ì‹œì¼œ ì¤€ë‹¤.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_80/kks227_14719767817475h0eo_PNG/13.png?type=w2)\n\nê·¸ ì§í›„, startê°€ 1 ì¦ê°€í•˜ë©´ì„œ start = endì¸ ê²½ìš°ê°€ ë‚˜ì˜¨ë‹¤.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_240/kks227_1471976782107sRHbv_PNG/14.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_240/kks227_14719767826459iErQ_PNG/15.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_192/kks227_1471976782977RS8E6_PNG/16.png?type=w2)\n\nê³„ì† ê°€ë‹¤ ë³´ë©´ ì„¸ ë²ˆì§¸ë¡œ S = 5ì¸ ì§€ì ì„ ë§Œë‚œë‹¤.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_147/kks227_1471976783270H1Bah_PNG/17.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_44/kks227_1471976783607C4F3g_PNG/18.png?type=w2)\n\nê·¸ ì´í›„ ì¡°ê±´ì— ë§ì¶° í¬ì¸í„°ë¥¼ ì¦ê°€ì‹œí‚¤ë‹¤ ë³´ë©´, endê°€ ë°°ì—´ ëì„ ê°€ë¦¬í‚¤ê²Œ ë˜ì–´ ë”ì´ìƒ ì¦ê°€í•  ìˆ˜ ì—†ëŠ” ìƒíƒœê°€ ëœë‹¤.\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_197/kks227_1471976784071FLqRR_PNG/19.png?type=w2)\n\n![img](https://mblogthumb-phinf.pstatic.net/20160824_87/kks227_14719767845214em80_PNG/20.png?type=w2)\n\nê·¸ë ‡ê²Œ ë˜ë©´ ê·¸ëƒ¥ startë§Œ ì¦ê°€ì‹œì¼œ ê°€ë‹¤ê°€ start ì—­ì‹œ ë°°ì—´ ëì— ë‹¤ë‹¤ë¥´ë©´ ì¢…ë£Œí•´ë„ ë˜ê³ , ê·¸ëƒ¥ ê·¸ ìë¦¬ì—ì„œ ë£¨í”„ë¥¼ ëë‚´ë²„ë ¤ë„ ëœë‹¤. ì´ë ‡ê²Œ í•´ì„œ S = 5ì¸ ê²½ìš°ëŠ” 3ê°œ ë°œê²¬ë˜ì—ˆë‹¤.\n\n- ì‹œê°„ ë³µì¡ë„ \n  - ì´ ì•Œê³ ë¦¬ì¦˜ì€ ë§¤ ë£¨í”„ë§ˆë‹¤ í•­ìƒ ë‘ í¬ì¸í„° ì¤‘ í•˜ë‚˜ëŠ” 1ì”© ì¦ê°€í•˜ê³  ìˆê³ , ê° í¬ì¸í„°ê°€ Në²ˆ ëˆ„ì  ì¦ê°€í•´ì•¼ ì•Œê³ ë¦¬ì¦˜ì´ ëë‚œë‹¤. ë”°ë¼ì„œ ê°ê° ë°°ì—´ ëì— ë‹¤ë‹¤ë¥´ëŠ”ë° `O(N)`ì´ë¼ì„œ í•©ì³ë„ `O(N)`ì´ ëœë‹¤.\n\n- **ì¶”ì²œ ë¬¸ì œ(ë°±ì¤€ ê¸°ì¤€)**\n  - 2003 : ìˆ˜ë“¤ì˜ í•©\n  - 1644 : ì†Œìˆ˜ì˜ ì—°ì†í•©\n  - 1806 : ë¶€ë¶„í•©\n  - 2230 : ìˆ˜ ê³ ë¥´ê¸°\n  - 1484 : ë‹¤ì´ì–´íŠ¸\n  - 2038 : ê³¨ë£½ ìˆ˜ì—´\n  - 2531 : íšŒì „ ì´ˆë°¥\n  - 2096 : ë‚´ë ¤ê°€ê¸°\n  - 2293 : ë™ì „1\n\n---\n### Reference\n- https://github.com/WooVictory/Ready-For-Tech-Interview/blob/master/Algorithm/íˆ¬í¬ì¸í„°%20ì•Œê³ ë¦¬ì¦˜.md\n","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/XGB-Modeling":{"title":"XGB Modeling","content":"\n## 1. XGB explained\n\n### XGBoostì˜ íŠ¹ì§•\n\n- ë³‘ë ¬ì²˜ë¦¬ ê°€ëŠ¥\n- GPUì§€ì›ì´ ê°€ëŠ¥\n- ì¶”ê°€ì ìœ¼ë¡œÂ **ì •ê·œí™” ê¸°ëŠ¥, Tree pruning ê¸°ëŠ¥, Early Stopping, ë‚´ì¥ëœ êµì°¨ê²€ì¦ê³¼ ê²°ì¸¡ì¹˜ ì²˜ë¦¬**Â ë“±\n\n### XGBoostì˜ ëŒ€í‘œì ì¸ íŒŒë¼ë¯¸í„°\n\n**ë‹¤ë£° ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ê°€ ë§ê¸° ë•Œë¬¸ì— Customizingì´ ìš©ì´í•˜ë‹¤.**\n\n- n_estimators (int) : ë‚´ë¶€ì—ì„œ ìƒì„±í•  ê²°ì • íŠ¸ë¦¬ì˜ ê°œìˆ˜\n- max_depth (int) : ìƒì„±í•  ê²°ì • íŠ¸ë¦¬ì˜ ë†’ì´\n- learning_rate (float) : í›ˆë ¨ëŸ‰, í•™ìŠµ ì‹œ ëª¨ë¸ì„ ì–¼ë§ˆë‚˜ ì—…ë°ì´íŠ¸í• ì§€ ê²°ì •í•˜ëŠ” ê°’\n- colsample_bytree (float) : ì—´ ìƒ˜í”Œë§ì— ì‚¬ìš©í•˜ëŠ” ë¹„ìœ¨\n- subsample (float) : í–‰ ìƒ˜í”Œë§ì— ì‚¬ìš©í•˜ëŠ” ë¹„ìœ¨\n- reg_alpha (float) : L1 ì •ê·œí™” ê³„ìˆ˜\n- reg_lambda (float) : L2 ì •ê·œí™” ê³„ìˆ˜\n- booster (str) : ë¶€ìŠ¤íŒ… ë°©ë²• (gblinear / gbtree / dart)\n- random_state (int) : ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë‚œìˆ˜ê°’\n- n_jobs (int) : ë³‘ë ¬ì²˜ë¦¬ì— ì‚¬ìš©í•  CPU ìˆ˜\n\n## 2. Usage\n\n### ë¶„ë¥˜ ë¬¸ì œ\n```python\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\n# ì›ë˜ ì—¬ê¸° ë°ì´í„°ì—ëŠ” ê²€ì¦ ë°ì´í„°ë¥¼ ë„£ì–´ì•¼í•¨ Test ë°ì´í„° ë„£ìœ¼ë©´ ì•ˆë¨!\n# ê²€ì¦ ë°ì´í„° ë„£ì–´ì£¼ì–´ì„œ êµì°¨ê²€ì¦ í•´ë³´ë„ë¡í•˜ê¸°\nevals = [(x_test, y_test)]\nxgb_wrapper = XGBClassifier(n_estimators=100, learning_rate=0.1,\n                           max_depth=3)\n                           \n# eval_metricë„£ì–´ì£¼ë©´ì„œ ê²€ì¦ ë°ì´í„°ë¡œ loss ì¸¡ì •í•  ë•Œ ì‚¬ìš©í•  metric ì§€ì •\nxgb_wrapper.fit(x_train, y_train, early_stopping_rounds=200,\n               eval_set=evals, eval_metric='auc')\n# Prediction 1\nprint('Train Score : {}'.format(xgb_wrapper.score(X_train, y_train)))\nprint('Test Score : {}'.format(xgb_wrapper.score(X_test,y_test)))\n\n# Prediction 2\npreds = xgb_wrapper.predict(x_test)\npreds_proba = xgb_wrapper.predict_proba(x_test)[:, 1]\nprint(preds_proba[:10])\n\n# ëª¨ë¸ í‰ê°€\nxgb_roc_score = roc_auc_score(\n    y_test,\n    xgb_wrapper.predict_proba(X_test)[:, 1]\n)\n```\n\n### Missing valueëŠ”?\n\nXGBoostëŠ” ëˆ„ë½ ê°’ì— ëŒ€í•´ì„œ ì–´ë–»ê²Œ ëŒ€ì‘í•  ì§€ ì•Œì•„ì„œ ì •í•˜ë„ë¡ ì„¤ì •ë˜ì–´ ìˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ê°€ í•´ì¤˜ì•¼ ë˜ëŠ” ê°€ê³µ ì‘ì—…ì€ ë‹¨ìˆœíˆ **ëˆ„ë½ê°’ì„ ì „ë¶€ 0ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ê²ƒ**ì´ë‹¤.Â \n\n```python\n# ëˆ„ë½ëœ ê°’ì´ ìˆëŠ” í–‰ì˜ ê°œìˆ˜ = ëˆ„ë½ ë°ì´í„° ê°œìˆ˜\nlen(df.loc[df['Total_Charges'] == ' '])\n\n# ëˆ„ë½ëœ í–‰ ì§ì ‘ë³´ê¸°\ndf.loc[df['Total_Charges'] == ' ']\n\n# ì§ì ‘ ë°”ê¾¸ê¸°\ndf.loc[(df['Total_Charges'] == ' '), 'Total_Charges'] = 0\n```\n\n### Categorical valuesëŠ”?\n\n```python\npd.get_dummies(X, columns = ['Payment_Method'])\n```\n\n### ì„±ëŠ¥ ì•Œì•„ë³´ê¸°\n```python\nval_error = mean_squared_error(y_val, y_pred)\nprint(\"XGB's Validation MSE:\", val_error)\n```\n\n### íšŒê·€ ë¬¸ì œ\n```python\nfrom xgboost import XGBRegressor # XGBRegressor ëª¨ë¸ ì„ ì–¸ í›„ Fitting\nxgbr = XGBRegressor() \nxgbr.fit(x_train, y_train) # Fittingëœ ëª¨ë¸ë¡œx_validë¥¼ í†µí•´ ì˜ˆì¸¡ì„ ì§„í–‰ \n\n# Prediction 1\nprint('Train Score : {}'.format(xgbr.score(X_train, y_train)))\nprint('Test Score : {}'.format(xgbr.score(X_test,y_test)))\n\n# Prediction 2\ny_pred = xgbr.predict(x_valid)\n```\n\nXGBoostëŠ” featureë³„ ì¤‘ìš”ë„ë¥¼ plot í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì œê³µí•œë‹¤. featureë³„ ì¤‘ìš”ë„ë¥¼ ë³´ê¸° ìœ„í•´ì„œ ê°„ë‹¨í•˜ê²Œ ì‹œê°í™”í•˜ëŠ” ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n```python\n# featureë³„ ì¤‘ìš”ë„ ì‹œê°í™”í•˜ê¸°\nfrom xgboost import plot_importance\n\nfig, ax = plt.subplots(figsize=(9,11))\nplot_importance(xgb_wrapper, ax)\n```\n","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/coding-test":{"title":"ğŸ‘©â€ğŸ’» Coding Test","content":"\n## í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ë¬¸ì œ ëª¨ìŒ\n\n### Level 1\n- [ìˆ«ì ë¬¸ìì—´ê³¼ ì˜ë‹¨ì–´](notes/ìˆ«ì%20ë¬¸ìì—´ê³¼%20ì˜ë‹¨ì–´.md) #Kakao \n- [í‚¤íŒ¨ë“œ ëˆ„ë¥´ê¸°](notes/í‚¤íŒ¨ë“œ%20ëˆ„ë¥´ê¸°.md)\n\n### Level 2\n- [ê¸°ëŠ¥ê°œë°œ](notes/ê¸°ëŠ¥ê°œë°œ.md)\n- [ì˜¬ë°”ë¥¸ ê´„í˜¸](notes/ì˜¬ë°”ë¥¸%20ê´„í˜¸.md)\n- [ë‹¤ìŒ í° ìˆ«ì](notes/ë‹¤ìŒ%20í°%20ìˆ«ì.md)\n- [ì˜ì–´ ëë§ì‡ê¸°](notes/ì˜ì–´%20ëë§ì‡ê¸°.md)\n- [êµ¬ëª…ë³´íŠ¸](notes/êµ¬ëª…ë³´íŠ¸.md)\n- [ë©€ë¦¬ë›°ê¸°](notes/ë©€ë¦¬ë›°ê¸°.md)\n- [ë‘ í í•© ê°™ê²Œ ë§Œë“¤ê¸°](notes/ë‘%20í%20í•©%20ê°™ê²Œ%20ë§Œë“¤ê¸°.md) #Kakao \n- [H-index](notes/H-index.md)\n\n### Level 3\n- [ì •ìˆ˜ ì‚¼ê°í˜•](notes/ì •ìˆ˜%20ì‚¼ê°í˜•.md)\n\n\n## HackerRank\n\n### Basic\n- [Picking Numbers](notes/Picking%20Numbers.md)","lastmodified":"2022-11-08T10:07:38.34135262Z","tags":null},"/notes/paper-review":{"title":"ğŸ“‘ Paper Review","content":"\n## Contents\n\n- [ğŸ“„ GLUCOSE](notes/GLUCOSE.md)\n- [ğŸ“„ On Cross-Dataset Generalization in Automatic Detection of Online Abuse](notes/On%20Cross-Dataset%20Generalization%20in%20Automatic%20Detection%20of%20Online%20Abuse.md) \n- [ğŸ“„ Towards generalisable hate speech detection](notes/Towards%20generalisable%20hate%20speech%20detection.md)\n- [ğŸ“„ Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge](notes/Call%20for%20Customized%20Conversation.md)\n- [ğŸ“„ Challenges and frontiers in abusive content detect](notes/Challenges%20and%20frontiers%20in%20abusive%20content%20detect.md)\n- [ğŸ“„ Studying Generalisability Across Abusive Language Detection Datasets](notes/Studying%20Generalisability%20Across%20Abusive%20Language%20Detection%20Datasets.md)\n- [ğŸ“„ Hate speech detection is not as easy as you may think](notes/Hate%20speech%20detection%20is%20not%20as%20easy%20as%20you%20may%20think.md)","lastmodified":"2022-11-08T10:07:38.345352673Z","tags":null}}