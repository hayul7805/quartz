<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="[!note] Goal
이 논문에서 저자들은 GPT-3 언어 모델을 사용하여 가설을 테스트하고 있다. GPT-3는 1,750억 개의 매개 변수를 가지고 있으며 문맥 정보를 사용하여 학습할 수 있다."><meta property="og:title" content="Language Models are Few-Shot Learners"><meta property="og:description" content="[!note] Goal
이 논문에서 저자들은 GPT-3 언어 모델을 사용하여 가설을 테스트하고 있다. GPT-3는 1,750억 개의 매개 변수를 가지고 있으며 문맥 정보를 사용하여 학습할 수 있다."><meta property="og:type" content="website"><meta property="og:image" content="https://hayul7805.github.io/quartz/icon.png"><meta property="og:url" content="https://hayul7805.github.io/quartz/notes/paper-review/Language-Models-are-Few-Shot-Learners/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Language Models are Few-Shot Learners"><meta name=twitter:description content="[!note] Goal
이 논문에서 저자들은 GPT-3 언어 모델을 사용하여 가설을 테스트하고 있다. GPT-3는 1,750억 개의 매개 변수를 가지고 있으며 문맥 정보를 사용하여 학습할 수 있다."><meta name=twitter:image content="https://hayul7805.github.io/quartz/icon.png"><title>Language Models are Few-Shot Learners</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://hayul7805.github.io/quartz//icon.png><link href=https://hayul7805.github.io/quartz/styles.c71fe25d329eef9e3d647988a08ac707.min.css rel=stylesheet><link href=https://hayul7805.github.io/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://hayul7805.github.io/quartz/js/darkmode.f77f63bb01d142b61d2c12fbb4418858.min.js></script>
<script src=https://hayul7805.github.io/quartz/js/util.a0ccf91e1937fe761a74da4946452710.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://hayul7805.github.io/quartz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://hayul7805.github.io/quartz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://hayul7805.github.io/quartz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://hayul7805.github.io/quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://hayul7805.github.io/quartz/",fetchData=Promise.all([fetch("https://hayul7805.github.io/quartz/indices/linkIndex.0fbe818e30c25519787251e4e163d335.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://hayul7805.github.io/quartz/indices/contentIndex.a31a098c17a186c167410e04a5b61a67.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://hayul7805.github.io/quartz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://hayul7805.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/hayul7805.github.io\/quartz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=hayul7805.github.io/quartz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://hayul7805.github.io/quartz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://hayul7805.github.io/quartz/>🪴 Hayul's digital garden</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Language Models are Few-Shot Learners</h1><p class=meta>Last updated
Dec 23, 2022
<a href=https://github.com/hayul7805/notes/paper-review/Language%20Models%20are%20Few-Shot%20Learners.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#term-definition>Term definition</a></li><li><a href=#gpt-in-reading-comprehension>GPT in reading comprehension</a></li><li><a href=#limitations>Limitations</a></li></ol></nav></details></aside><blockquote class=note-callout><p>Goal</p><p>이 논문에서 저자들은 GPT-3 언어 모델을 사용하여 가설을 테스트하고 있다. GPT-3는 1,750억 개의 매개 변수를 가지고 있으며 문맥 정보를 사용하여 학습할 수 있다. 저자들은 이 모델을 사용하여 작업에 빠르게 적응할 수 있는 능력과 few shot 시나리오에서 학습에 대한 숙련도를 평가하고 있다.</p></blockquote><blockquote><p>PLM을 이용한 Fine-tuning이 만능인가?</p></blockquote><p>The approach of pre-trained language models has had significant success and has allowed for more efficient tasks, <strong>but it requires having a task-specific dataset for fine-tuning.</strong> This means that to get strong performance on any specific task, a dataset of a large number of examples related to that task is needed.</p><blockquote><p>아니다, fine-tuning을 위한 데이터셋이 필요하다. 이러한 한계를 없애야하는 이유는 여러가지가 있다.</p></blockquote><ul><li>First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models.</li><li>그리고 무엇보다, 사람은 그렇게 학습하지 않아. 사람에게 무언가를 요구할 때는 간단한 문장이면 충분하다.</li></ul><blockquote><p>그러면, 다른 방향이 있는가?</p></blockquote><ul><li>One potential route towards addressing these issues is <strong>meta-learning</strong>– which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1).</li><li><strong>Meta-learning</strong> is a technique used to train a model in such a way that it can develop a set of skills and be able to recognize patterns quickly, which helps it in adapting to a specific task when it is being used. Basically, it prepares the model to become more efficient at a task by &rsquo;learning&rsquo; from a variety of training experiences first.</li><li>= <strong>In-context learning</strong>.<ul><li>In-context learning is a method of teaching a language model to complete a task from contextual information provided by natural language instructions and/or a few demonstrations of the task. It is possible that this method of learning will become more successful as larger language models are developed; that is, models with more parameters, or units processing the data. This means that with larger language models, the in-context learning abilities would improve in performance.</li></ul></li></ul><a href=#term-definition><h2 id=term-definition><span class=hanchor arialabel=Anchor># </span>Term definition</h2></a><ul><li><strong>Fine-Tuning (FT)</strong> has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task. Typically thousands to hundreds of thousands of labeled examples are used.<ul><li>The main advantage of fine-tuning is strong performance on many benchmarks.</li><li>The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [MPL19 ], and the potential to exploit spurious features of the training data [GSL+18 , NK19 ], potentially resulting in an unfair comparison with human performance.</li></ul></li><li><strong>Few-Shot (FS)</strong> is the term we will use in this work to refer to the setting where <strong>the model is given a few demonstrations of the task at inference time</strong> as conditioning [RWC+19 ], but <strong>no weight updates</strong> are allowed.<ul><li>Basically, the model receives these examples, uses them to make a prediction, and then <strong>doesn&rsquo;t get to adjust its internal parameters</strong> to learn from its mistakes.</li></ul></li><li><strong>One-Shot (1S)</strong> is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that <strong>it most closely matches the way in which some tasks are communicated to humans.</strong></li><li><strong>Zero-Shot (0S)</strong> is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task.</li></ul><blockquote class=note-callout><p>Tip</p><p>When building a machine learning model, the batch size and learning rate are important factors that must be considered. Generally, for larger models, you can use a larger batch size - which means that more data is fed in at once during training - but a smaller learning rate, which dictates the pace at which the model learns.</p></blockquote><a href=#gpt-in-reading-comprehension><h2 id=gpt-in-reading-comprehension><span class=hanchor arialabel=Anchor># </span>GPT in reading comprehension</h2></a><p>On DROP [DWD+19 ], a dataset testing discrete reasoning and numeracy in the context of reading comprehension, <strong>GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline</strong> from the original paper but is still well below both human performance (&mldr;)</p><a href=#limitations><h2 id=limitations><span class=hanchor arialabel=Anchor># </span>Limitations</h2></a><ul><li>First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, <strong>it still has notable weaknesses in text synthesis and several NLP tasks.</strong> On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs.</li><li>A limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that <strong>they are both expensive and inconvenient to perform inference on</strong>, which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is <strong>distillation</strong> [HVD15 ] of large models down to a manageable size for specific tasks.</li><li>Finally, GPT-3 shares some limitations common to most deep learning systems – <strong>its decisions are not easily interpretable</strong>, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on.</li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/quartz/notes/paper-review/ data-ctx="📄 Language Models are Few-Shot Learners" data-src=/notes/paper-review class=internal-link>📑 Paper Review</a></li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://hayul7805.github.io/quartz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Hayul Park using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://hayul7805.github.io/quartz/>Home</a></li><li><a href=https://github.com/hayul7805>Github</a></li><li><a href=https://www.linkedin.com/in/hayulpark/>Linkedin</a></li></ul></footer></div></div></body></html>