<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="[!info] Reference
Yin, W., & Zubiaga, A. (2021). Towards generalisable hate speech detection: a review on obstacles and solutions. PeerJ Computer Science, 7, e598."><meta property="og:title" content="Towards generalisable hate speech detection"><meta property="og:description" content="[!info] Reference
Yin, W., & Zubiaga, A. (2021). Towards generalisable hate speech detection: a review on obstacles and solutions. PeerJ Computer Science, 7, e598."><meta property="og:type" content="website"><meta property="og:image" content="https://hayul7805.github.io/quartz/icon.png"><meta property="og:url" content="https://hayul7805.github.io/quartz/notes/paper-review/Towards-generalisable-hate-speech-detection/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Towards generalisable hate speech detection"><meta name=twitter:description content="[!info] Reference
Yin, W., & Zubiaga, A. (2021). Towards generalisable hate speech detection: a review on obstacles and solutions. PeerJ Computer Science, 7, e598."><meta name=twitter:image content="https://hayul7805.github.io/quartz/icon.png"><title>Towards generalisable hate speech detection</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://hayul7805.github.io/quartz//icon.png><link href=https://hayul7805.github.io/quartz/styles.7a7b335758325e783e6813b791081e23.min.css rel=stylesheet><link href=https://hayul7805.github.io/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://hayul7805.github.io/quartz/js/darkmode.f77f63bb01d142b61d2c12fbb4418858.min.js></script>
<script src=https://hayul7805.github.io/quartz/js/util.a0ccf91e1937fe761a74da4946452710.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://hayul7805.github.io/quartz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://hayul7805.github.io/quartz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://hayul7805.github.io/quartz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://hayul7805.github.io/quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://hayul7805.github.io/quartz/",fetchData=Promise.all([fetch("https://hayul7805.github.io/quartz/indices/linkIndex.2745fa29d7b184f2bbb93b6e140bafd9.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://hayul7805.github.io/quartz/indices/contentIndex.33b7d74fd2f60b1ee1fbda8be72b4033.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://hayul7805.github.io/quartz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://hayul7805.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/hayul7805.github.io\/quartz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=hayul7805.github.io/quartz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://hayul7805.github.io/quartz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://hayul7805.github.io/quartz/>🪴 Hayul's digital garden</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Towards generalisable hate speech detection</h1><p class=meta>Last updated
Dec 12, 2022
<a href=https://github.com/hayul7805/notes/paper-review/Towards%20generalisable%20hate%20speech%20detection.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://hayul7805.github.io/quartz/tags/generalization/>Generalization</a></li><li><a href=https://hayul7805.github.io/quartz/tags/key-observation/>Key observation</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#generalisation>Generalisation</a></li><li><a href=#data>Data</a></li><li><a href=#obstacles-to-generalisable-hate-speech-detection>OBSTACLES TO GENERALISABLE HATE SPEECH DETECTION</a></li></ol></nav></details></aside><blockquote class=info-callout><p>Reference</p><p>Yin, W., & Zubiaga, A. (2021). Towards generalisable hate speech detection: a review on obstacles and solutions. <em>PeerJ Computer Science</em>, <em>7</em>, e598.</p></blockquote><hr><a href=#generalisation><h2 id=generalisation><span class=hanchor arialabel=Anchor># </span>Generalisation</h2></a><blockquote><p>Most if not all proposed hate speech detection models rely on supervised machine learning methods, where the ultimate purpose is for the model to learn the real relationship between features and predictions through training data, which generalises to previously unobserved inputs (Goodfellow, Bengio & Courville, 2016). The generalisation performance of a model measures how well it fulfils this purpose.</p></blockquote><ul><li>제안된 혐오 발언 탐지 모델은 대부분 supervised 기계 학습 방법에 의존하며, 궁극적인 목적은 모델이 이전에 관찰되지 않은 입력으로 일반화하는 훈련 데이터를 통해 기능과 예측 사이의 실제 관계를 학습하는 것이다(Goodfellow, Bengio & Courville, 2016). 모델의 일반화 성과는 이 목적을 얼마나 잘 달성하는지 측정한다.</li></ul><blockquote><p>The ultimate purpose of studying automatic hate speech detection is to facilitate the alleviation of the harms brought by online hate speech. To fulfil this purpose, hate speech detection models need to be able to deal with the constant growth and evolution of hate speech, regardless of its form, target, and speaker.</p></blockquote><ul><li>자동 혐오표현 탐지를 연구하는 궁극적인 목적은 온라인 혐오표현이 가져오는 해악의 완화를 용이하게 하는 것이다. 이러한 목적을 달성하기 위해, 혐오표현 탐지 모델은 형태, 대상 및 화자에 관계없이 혐오 발언의 지속적인 성장과 진화를 처리할 수 있어야 한다.</li></ul><p>#key-observation</p><blockquote><p>Recent research has raised concerns on the generalisability of existing models (Swamy, Jamatia & Gambäck, 2019). Despite their impressive performance on their respective test sets, <strong>the performance significantly dropped when the models are applied to a different hate speech dataset.</strong> This means that the assumption that test data of existing datasets represent the distribution of future cases is not true, and that <strong>the generalisation performance of existing models have been severely overestimated</strong> (Arango, Prez & Poblete, 2020). This lack of generalisability undermines the practical value of these hate speech detection models.</p></blockquote><ul><li>최근 연구는 기존 모델의 일반화 가능성에 대한 우려를 제기했다(Swamy, Jamatia & Gambeck, 2019 :
<a href=/quartz/notes/paper-review/Studying-Generalisability-Across-Abusive-Language-Detection-Datasets/ rel=noopener class=internal-link data-src=/quartz/notes/paper-review/Studying-Generalisability-Across-Abusive-Language-Detection-Datasets/>Studying Generalisability Across Abusive Language Detection Datasets</a>). <strong>각각의 테스트 세트에서 인상적인 성능에도 불구하고 모델이 다른 혐오 음성 데이터 세트에 적용될 때 성능이 크게 떨어졌다.</strong> 이는 기존 데이터 세트의 테스트 데이터가 미래의 사례 분포를 나타낸다는 가정이 사실이 아니며, <strong>기존 모델의 일반화 성능이 심각하게 과대 평가되었다는 것을 의미한다</strong>(Arango, Pres & Poblete, 2020 :
<a href=/quartz/notes/paper-review/Hate-speech-detection-is-not-as-easy-as-you-may-think/ rel=noopener class=internal-link data-src=/quartz/notes/paper-review/Hate-speech-detection-is-not-as-easy-as-you-may-think/>Hate speech detection is not as easy as you may think</a>). 이러한 일반성의 부족은 이러한 혐오표현 탐지 모델의 실질적인 가치를 훼손한다.</li></ul><blockquote class=note-callout><p>Note</p><p>이 부분이 내가 하고 있는 연구의 핵심이다! 모델의 일반화 성능이 과대평가되어 있다는 것. 한국어 데이터셋과 모델로도 비슷한 결과가 나오는지 보는 것.</p></blockquote><a href=#data><h2 id=data><span class=hanchor arialabel=Anchor># </span>Data</h2></a><blockquote><p>[예시 1]
For example, in Wiegand, Ruppenhofer & Kleinbauer (2019)’s study, FastText models (Joulin et al., 2017a) trained on three datasets (Kaggle, Founta, Razavi) achieved F1 scores above 70 when tested on one another, <strong>while models trained or tested on datasets outside this group achieved around 60 or less</strong>.</p></blockquote><ul><li>모델이 훈련된 것과 다른 데이터셋에서는 모델의 성능이 떨어진다는 결과가 있다.</li></ul><p>#key-observation</p><blockquote><p>Founta and OLID produced models that performed well on each other. The source of such differences are usually traced back to search terms (Swamy, Jamatia & Gambäck, 2019), topics covered (Nejadgholi & Kiritchenko, 2020; Pamungkas, Basile & Patti, 2020), label definitions (Pamungkas & Patti, 2019; Pamungkas, Basile & Patti, 2020; Fortuna, Soler-Company & Wanner, 2021), and data source platforms (Glavaš, Karan & Vulić, 2020; Karan & Šnajder, 2018).</p></blockquote><ul><li>서로 테스트 성능이 잘 나오는 데이터셋은 그 근원을 따라가보면 알 수 있는 사실이 있다. 예를 들면,<code> Founta</code>와 <code>OLID</code> 데이터셋은 서로 비슷한 데이터를 공유하고 있다.</li></ul><blockquote><p>Fortuna, Soler & Wanner (2020) used averaged word embeddings (Bojanowski et al., 2017; Mikolov et al., 2018) to compute the representations of classes from different datasets, and compared classes across datasets. <strong>One of their observations is that Davidson’s ‘‘hate speech’’ is very different from Waseem’s ‘‘hate speech’’, ‘‘racism’’, ‘‘sexism’’, while being relatively close to HatEval’s ‘‘hate speech’’ and Kaggle’s ‘‘identity hate’’.</strong> This echoes with experiments that showed poor generalisation of models from Waseem to HatEval (Arango, Prez & Poblete, 2020) and between Davidson and Waseem (Waseem, Thorne & Bingel, 2018; Gröndahl et al., 2018).</p></blockquote><ul><li>혐오표현 데이터셋에서 자주 나오는 단어들인 &lsquo;hate speech&rsquo;, &lsquo;racism&rsquo;, &lsquo;sexism&rsquo; 등도 워드 임베딩을 통해 살펴보니 데이터셋마다 그 의미가 다르다는 관찰이 나왔다. 이는 당연히 모델 성능의 일반화에도 악영향을 끼쳤고 말이다.</li></ul><blockquote><p>In terms of what properties of a dataset lead to more generalisable models, there are frequently mentioned factors (&mldr;)</p></blockquote><blockquote><p>Biases in the samples are also frequently mentioned. <strong>Wiegand, Ruppenhofer & Kleinbauer (2019) hold that less biased sampling approaches produce more generalisable models.</strong> This was later reproduced by Razo & Kübler (2020) and also helps explain their results with the two datasets that have the least positive cases. Similarly, Pamungkas & Patti (2019) mentioned that a wider coverage of phenomena lead to more generalisable models.</p></blockquote><ul><li><code>Wiegand, Ruppenhofer & Kleinbauer (2019)</code> 연구에서 언급한 바와 같이, 조금이라도 더 일반화가 잘 되는 모델을 만드려면 sampling을 덜 치우치게 해주어야 한다. 훈련시 <code>sampler</code>를 잘 만들어야겠다.</li></ul><blockquote><p>Another way of looking at generalisation and similarity is by comparing differences between individual classes across datasets (Nejadgholi & Kiritchenko, 2020; Fortuna, Soler & Wanner, 2020; Fortuna, Soler-Company & Wanner, 2021), as opposed to comparing datasets as a whole.</p></blockquote><ul><li>이 논문에서도 class 개별로 비교하라고 주장하는구나.
<a href=/quartz/notes/paper-review/On-Cross-Dataset-Generalization-in-Automatic-Detection-of-Online-Abuse/ rel=noopener class=internal-link data-src=/quartz/notes/paper-review/On-Cross-Dataset-Generalization-in-Automatic-Detection-of-Online-Abuse/>On Cross-Dataset Generalization in Automatic Detection of Online Abuse</a> 에서 주장하는 것과 맞물린다.</li></ul><a href=#obstacles-to-generalisable-hate-speech-detection><h2 id=obstacles-to-generalisable-hate-speech-detection><span class=hanchor arialabel=Anchor># </span>OBSTACLES TO GENERALISABLE HATE SPEECH DETECTION</h2></a><blockquote><p>Hate speech detection, which is largely focused on social media, shares similar challenges to other social media tasks and has its specific ones, <strong>when it comes to the grammar and vocabulary used.</strong> Such user language style introduces challenges to generalisability at the data source, mainly by making it difficult to utilise common NLP pre-training approaches.</p></blockquote><ul><li>혐오표현 탐지는 문법이나 어휘 관련해서 어려움이 많다는 특징이 있다.</li></ul><blockquote><p>On social media, syntax use is generally more casual, such as the omission of punctuation (Blodgett & O’Connor, 2017). Alternative spelling and expressions are also used in dialects (Blodgett & O’Connor, 2017), to save space, and to provide emotional emphasis (Baziotis, Pelekis & Doulkeridis, 2017). Sanguinetti et al. (2020) provided extensive guidelines for studying such phenomena syntactically.</p></blockquote><ul><li>그래서 혐오표현 탐지 연구를 할 때는 KcELECTRA가 그나마 괜찮겠구나. 이러한 케이스가 많은 데이터로 사전학습된 모델이니까. 실제로 성능도 가장 괜찮고.</li></ul><blockquote><p>Qian et al. (2018) found that rare words and implicit expressions are the two main causes of false negatives; Van Aken et al. (2018) compared several models that used pre-trained word embeddings, and found that rare and unknown words were present in 30% of the false negatives of Wikipedia data and 43% of Twitter data.</p></blockquote><ul><li>또한 rare words, implicit expressions는 false negatives를 증가시킨다. 이는 따라서 하나의 도메인에서만 수집한 데이터셋이 가지는 한계일 수 밖에 없겠다. 이를 극복하려면 여러 도메인에서 데이터를 수집해야겠네.</li></ul><blockquote><p>Indeed, BERT (Devlin et al., 2019) and its variants have demonstrated top performances at hate or abusive speech detection challenges recently (Liu, Li & Zou, 2019; Mishra & Mishra, 2019).</p></blockquote><ul><li>BERT 계열 모델은 혐오표현 탐지에서도 여전히 top 퍼포먼스를 보인다.</li></ul><blockquote><p>It is particularly challenging to acquire labelled data for hate speech detection as knowledge or relevant training is required of the annotators. As a high-level and abstract concept, the judgement of ‘‘hate speech’’ is subjective, needing extra care when processing annotations. Hence, datasets are usually not big in size.</p></blockquote><ul><li>혐오표현 데이터셋은 &lsquo;혐오표현&rsquo;을 정의하는 것 자체가 주관적이기 때문에, 주석 처리에 추가적인 힘이 들고 따라서 큰 사이즈로 만들어지기 어렵다.</li></ul><blockquote><p>Moreover, <strong>different studies are based on varying definitions of ‘‘hate speech’’, as seen in different annotation guidelines</strong> (Table 5). Despite all covering the same two main aspects (directly attack or promote hate towards), datasets vary by their wording, what they consider a target (any group, minority groups, specific minority groups), and their clarifications on edge cases.
Davidson and HatEval both distinguished ‘‘hate speech’’ from ‘‘offensive language’’, while ‘‘uses a sexist or racist slur’’ is in Waseem’s guidelines to mark a case positive of hate, <strong>blurring the boundary of offensive and hateful.</strong>
Additionally, as both HatEval and Waseem specified the types of hate (towards women and immigrants; racism and sexism), hate speech that fell outside of these specific types were not included in the positive classes, while Founta and Davidson included any type of hate speech.</p></blockquote><ul><li>또한, 다른 주석 지침(표 5)에서 볼 수 있듯이, 다양한 연구는 &ldquo;혐오 발언"의 다양한 정의를 기반으로 한다. 모든 것이 동일한 두 가지 주요 측면을 포함함에도 불구하고 데이터 세트는 표현, 대상으로 간주하는 것(모든 그룹, 소수 그룹, 특정 소수 그룹) 및 엣지 사례에 대한 명확화에 따라 다르다.</li><li>Davidson과 HatEval은 모두 &ldquo;hate speech"와 &ldquo;offensive language"를 구분했으며, &ldquo;성차별적 또는 인종차별적 비방 사용"은 Waseem의 가이드라인에 hate로 표시하여 offensive와 hate의 경계를 모호하게 한다.</li><li>또한, HatEval과 Waseem이 혐오의 유형(여성과 이민자에 대한 것; 인종 차별과 성차별)을 명시함에 따라, 이러한 특정 유형에서 벗어난 혐오 발언은 긍정적인 등급에 포함되지 않았고, 반면 Fonta와 Davidson은 모든 유형의 혐오 발언을 포함시켰다.</li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/quartz/notes/paper-review/ data-ctx="📄 Towards generalisable hate speech detection" data-src=/notes/paper-review class=internal-link>📑 Paper Review</a></li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://hayul7805.github.io/quartz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Hayul Park using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://hayul7805.github.io/quartz/>Home</a></li><li><a href=https://github.com/hayul7805>Github</a></li><li><a href=https://www.linkedin.com/in/hayulpark/>Linkedin</a></li></ul></footer></div></div></body></html>